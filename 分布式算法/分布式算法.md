## 分布式互斥
![[Pasted image 20250717190835.png]]

## 分布式选举
![[Pasted image 20250717202444.png]]

## 一致性与共识的区别
一致性是指在分布式系统中，针对同一数据或状态以多个副本形式保存在不同节点上；当对某个数据或状态副本做出修改后，能保证多副本达到对外表现的数据一致性。

共识是指一个或多个进程提议某些修改后，采用一种大家认可的方法，使得系统中所有进程对该修改达成一致意见，该方法称为共识机制。

也就是说，共识重点在于达成一致的过程或方法，一致性问题在于最终对外表现的结果。

## 分布式事务
![[Pasted image 20250718160950.png]]
3PC协议在协调者和参与者均引入了超时机制。即当参与者在预提交阶段向协调者发送Ack消息后，如果长时间没有得到协调者的响应，在默认情况下，参与者会自动将超时的事务进行提交，从而减少整个集群的阻塞时间，在一定程度上减少或减弱2PC中出现的同步阻塞问题。

但三阶段提交仍然存在数据不一致的情况，比如PreCommit阶段，部分参与者已经接受到ACK消息进入执行阶段，但部分参与者与协调者网络不通，导致接收不到ACK消息，此时收到ACK消息的参与者会执行任务，未接收到ACK消息且网路不通的参与者无法执行任务，最终导致数据不一致。

![[Pasted image 20250718161022.png]]

### Base理论
BASE理论包括：基本可用(Basically Avalilable)、柔性状态(Soft State)和最终一致性(Eventual Consistency)。
- 基本可用：分布式系统出现故障的时候，允许损失一部分非核心功能的可用性，保证核心功能可用
- 柔性状态：在柔性事务中，允许系统存在中间状态，且这个中间状态不会影响系统整体可用性。
- 最终一致性：事务在操作过程中可能会由于同步延迟等问题导致不一致，但最终状态下，所有数据都是一致的。

## 分布式锁
![[Pasted image 20250718164553.png]]
![[Pasted image 20250718165044.png]]

## 分布式调度架构之单体调度
![[Pasted image 20250721161505.png]]

![[Pasted image 20250721161329.png]]
## 分布式调度之两层调度
![[Pasted image 20250721171023.png]]
## 分布式调度架构之共享状态调度
![[Pasted image 20250722134524.png]]
![[Pasted image 20250722134535.png]]
## 分布式计算模式之Actor
![[Pasted image 20250723110128.png]]

## 分布式计算模式之流水线
![[Pasted image 20250723135936.png]]
## 分布式通信之远程调用
![[Pasted image 20250723151053.png]]
## 分布式通信之发布订阅
![[Pasted image 20250723162743.png]]
## 分布式通信之消息队列
对于消息队列模式，消息队列中心无需提前获取消费者信息，因此对消费者比较灵活，适合消费者为临时用户的场景；而发布订阅模式，需要消费者提前向消息中心订阅消息，也就是说消息中心需要提前获取消费者信息，比较适合消费者为常驻进程或服务的场景。
![[Pasted image 20250723174203.png]]


## CAP理论
### CP
如果一个分布式场景需要很强的数据一致性，或者场景可以容忍系统长时间无响应的情况下，保CP弃A这个策略就比较合适。这类系统有很多，典型的有Redis、HBase、Zookeeper。
### AP
如果一个分布式场景需要很高的可用性，或者说在网络状况不太好的情况下，该场景允许数据暂时不一致，那这种情况就可以牺牲一定的一致性了。
适合保证AP放弃C的场景有很多，比如，很多查询网站、电商系统中的商品查询等，用户体验非常重要，所以大多会保证系统的可用性，而牺牲一定的数据一致性。
目前，采用AP的系统也有很多，比如CoachDB、Eureka、Cassandra、DynamoDB等。
![[Pasted image 20250724101225.png]]
### CAP和ACID的 "C" "A"的区别
先看下C的区别：
- CAP中的C强调的是数据的一致性，也就是集群中节点之间通过复制技术保证每个节点上的数据在同一时刻是相同的。
- ACID中的C强调的事务执行前后，数据的完整性保证一致或满足完整性约束。也就是不管在什么时候，不管并发事务有多少，事务在分布式系统中的状态始终保持一致。
A的区别：
- CAP中的A指的可用性(Availability)，也就是系统提供的服务一直处于可用状态，即对于用户的请求可即时相应。
- ACID中的A指的是原子性(Atomicity)，强调的是事务要么执行成功，要么执行失败。

![[Pasted image 20250724101912.png]]

## 分布式数据存储系统之三要素
![[Pasted image 20250724113532.png]]
## 数据分布方式之哈希与一致性哈希
![[Pasted image 20250724150330.png]]
## 分布式数据复制技术
![[Pasted image 20250724161927.png]]
## 分布式数据之缓存技术
![[Pasted image 20250724172848.png]]
## 分布式高可靠之负载均衡
![[Pasted image 20250725103459.png]]
![[Pasted image 20250725103506.png]]
## 分布式高可靠之流量控制
**漏斗桶适用于间隔性突发流量且流量不用即时处理的场景**，即可以在流量较小时的"空闲期",处理大流量时流入漏斗桶；不适合流量需要及时处理的场景，即突发流量时可以放入桶中，但缺乏效率，始终以固定速率进行处理。

**令牌桶策略:** 当有突发大流量时，只要令牌桶里有足够多的令牌，请求就会被迅速执行。通常情况下，令牌桶容量的设置，可以接近服务器处理的极限，这样就可以有效利用服务器的资源。因此，这种策略**适用于有突发特性的流量，且流量需要即时处理的场景。**
### 两种策略对比
![[Pasted image 20250725104556.png]]
![[Pasted image 20250725112826.png]]
## 分布式高可用之故障隔离
### 分布式系统中的故障隔离策略有很多，大体上可以从两个维度来划分：
- 一类是可以系统功能模块为粒度进行隔离。根据功能模块或服务由线程执行花式进程执行，通常分为线程级隔离、进程级隔离。
- 另一类是，通过资源隔离来实现。根据资源所属粒度，通常包括进程级隔离(比如采用容器隔离)、虚拟机隔离、服务器隔离和机房隔离等。


![[Pasted image 20250725142650.png]]

![[Pasted image 20250725144629.png]]
## 分布式高可用之故障恢复
### 故障检测
固定心跳检测的核心是，固定周期T秒发送心跳，若连续k次未收到心跳回复(时间T)，则判断心跳超市的时间为`k*T`秒。可以看出，k和T的设置非常重要。

\(φ\)值故障检测是基于心跳间隔符合正态分布的假设，通过对历史心跳数据采样来预测当前心跳是否超时的。

**在网络状态确定且比较稳定的场景下，大多数系统会采用固定心跳检测策略，因为其可以根据网络状况和业务场景自主设定合适的k和T值，简单有效；而当网络状况有所变化，且变化规有规律的场景，则可以使用\(φ\)值故障检测策略。**

![[Pasted image 20250725154532.png]]

## 怎么判断并解决网络分区问题
**关于网络分区的处理方法，其本质就是，在产生分区后，选出一个分区，保证同时最多有一个分区对外提供服务。** 基于次，有四种常见的处理方法，包括：`Static Quorum`、`Keep Majority`、设置仲裁机制和基于共享资源的方式(拿到锁的子集群对外提供服务，没拿到锁的不能对外提供服务)。

其中，基于Static Quorum的方法，因为设计固定票数策略，所以不适用于处理多个分区，以及有动态节点加入的场景；基于Keep Majority的方法，可以解决动态节点场景下分区问题，但因为要求子集群节点数>= 1/2节点数，所以也不适用于处理多个分区的情况；而基于仲裁和共享资源的网络分区处理方法，其实都是依赖一个三方的节点或组件，所以适用于有一个稳定可靠的三方节点或组件的场景。

## 分布式数据基础
### 分区(分片)
- 垂直分区是对表的列进行拆分，列式数据库可以看做已经垂直分区的数据库。
- 水平分区是对表的行进行拆分，将不同的行放入不同的表中。
水平分区一般采用hash分区，hash分区数据的分布相对比较均匀，但是范围查询比较困难。而且新增或者删除节点时，需要修改hash函数，并且有大规模的数据需要重新迁移。

可以采用一致性hash来解决增删节点需要大规模迁移数据和修改hash函数的问题，但是一致性hash如果删除节点，被删节点的数据会全部落到下一个节点上导致数据倾斜，这种情况可以通过新增虚拟节点来使数据更加均衡，迁移数据的时候会把数据分散到各个节点中。
还支持异构的集群，例如其中有些服务器的性能比集群中的其他服务器的性能好一些，可以根据性能相对的来增加虚拟节点的数据量。但是一致性hash还是无法解决范围查询的问题。
![[Pasted image 20250728155432.png]]
### 复制
为了提高可用性，除了分区还需要复制(Replication)。复制是指将同一份数据冗余存储在多个节点上，节点间通过网络来同步数据，使之保持一致。一个存储了复制数据的节点称为副本(Replica)。
### 单主复制
单主复制也叫做主从复制，即指定系统中的一个副本为主节点，客户端的写请求必须发送到主节点；其余的副本称为从节点，从节点只能处理读请求，并从主节点同步最新的数据。
- 如下图所示为**同步复制(Synchronous Replication)**，因为主节点必须等待直到所有副本写入完成，写请求的性能必然会收到影响。
![[Pasted image 20250728160851.png]]
- 下图是**异步复制(Asynchronous Replication)**，异步复制会潜在影响副本数据的一致性和持久性。
![[Pasted image 20250728161005.png]]
- **半同步复制（Semisynchronous Replication）**，主节点只需要等待至少一个从节点同步写操作并返回完成个信息即可。
![[Pasted image 20250728161409.png]]
目前广泛使用的数据库PostgreSQL和MYSQL都支持单主复制，也都支持同步、或半同步复制。单主复制可以说是最常见、最为广泛使用的一种复制方式。

### 多主复制
这种由多个节点充当主节点的数据复制方式称为主复制。
![[Pasted image 20250728170105.png]]
多主复制和单主复制的显著区别是，由于多主复制不止一个节点处理写请求，且网络存在延迟，这就意味着节点可能会对某些请求的正确顺序产生分歧，导致多个节点上的数据不一致，这种现象称为数据冲突。

一些常见的冲突解决方法有：
1. **由客户端解决冲突。** 
2. **最后写入胜利（LWW, Last Write Wins）**。最后写入胜利是让系统中的每个节点为每个请求标记上维轶时间戳或唯一自增ID，当冲突发生时，系统选择具有最新时间戳或最新ID版本的数据，但是由于分布式系统很难有一个同意的全局时间概念，这种技术可能导致一些意想不到的行为。
3. **因果关系跟踪。** 因果关系跟踪的局限性是仍然有一些不存在因果关系的并发写请求，对此系统无法做出决定。
由于多主复制带来的复杂性远超它的好处，因此很少会在单个数据中心使用多主复制来构建分布式系统。多主复制一般用于多个数据中心的存储系统，避免写请求跨月数据中心。例如一个全球服务，在全球多地有多个数据中心，此时可以将请求路由到地理位置更近的数据中心中的主节点，以加快访问速度。

### 无主复制
完全没有主节点的复制技术，称为无主复制，尽管无主复制技术在几十年前就出现了，但直到亚马逊发布了Dynamo架构的论文，并在其中使用无主复制，才让该技术从新引起广泛关注。
无主复制的基本思想是，客户端不仅向一个节点发送写请求，而是将请求发送到多个节点，在某些情况下甚至会发送给所有节点，如下图所示：
![[Pasted image 20250728171721.png]]
Dynamo架构中同时使用了一下两种数据修复方法：
1. 读修复(Read Repair)。
2. 反熵过程(Anti-Entopy Process)。反熵过程会新建一个后台进程来修复数据，该进程找出错误的数据，并从存储最新的数据的节点中将数据复制到错误的节点。反熵过程不保证写操作的顺序，只保证最后结果一样。进行反熵过程修复时，不希望一个个比较数据是否一致，这需要传输很多数据进行对比。Dynamo使用Merkle Tree来验证数据是否产生了不一致，减少了传输的数据量。
### 基于Quorum的数据冗余机制
Quorum（法定人数）机制是分布式系统中用来保证数据冗余和最终一致性的一种算法。Quorum机制就用于确定到底要多少个节点才足够，以及如果我们增加或减少读写请求的节点数量，系统会发生怎样的变化。
基于Quorum的数据冗余机制保证了在一个由N个节点组成的系统中，我们要求至少W个节点写入成功，并且需要同时从R个节点中读取数据，只要W + R > N且W > N / 2，这读取的R个返回值中至少包含一个最新的值。

基于Quorum机制的最小读写副本数据可以作为系统在读写性能方面的可调节参数，将W和R作为可配置参数后，管理员可以根据系统的工作负载来配置具体的参数值。W值越大R值越小，系统的读操作性能就越好。反之写操作的性能越好。

### CAP定理
CAP定理就是一个分布式系统特性的高度抽象，它总结了各个特性之间的冲突。分区和复制等技术既会带来好处，也会带来问题，CAP定理对这类数据系统的特性做了一个重要的总结。
### PACELC定理
PACELC定理的主要论点是，CAP定理忽略分布式系统中的延迟影响是一个重大疏忽，因为延迟在系统运行过程中时刻存在，而网络分区不会一直存在。

PACELC定理之处，在分布式系统存在网络分区(P)的情况下，必须在可用性(A)和一致性(C)之间做出选择；否则(Else，E)系统在没有网络分区且正常运行的情况下，必须在延迟(L)和一致性(C)之间做出选择。
![[Pasted image 20250728180947.png]]
![[Pasted image 20250728181322.png]]

### 一致性模型
![[Pasted image 20250728182622.png]]
如上图所示的一致性模型根据可用性可以分为三类，白底矩形中的模型的可用性为不可用(Unavailable)，灰色矩形中模型的可用性为基本可用(Sticky Available)，椭圆中的模型的可用性为高可用(Total Available)。
### **线性一致性**
线性一致性(Linearizable Consistency) 是最强的一致性模式， CAP定理中的一致性指的就是线性一致性。
线性一致性的严格定义是，**给定一个执行历史，执行历史根据并发操作可以扩展为多个顺序历史（Sequential History），只要从中找到一个合法的顺序历史，那么该执行历史就是线性一致性的。**
总的来说，线性一致性主要有两个约束条件，第一，顺序记录中的的任何一次读必须读到最近一次写入的数据；第二，顺序记录要跟全局时钟下的顺序一直。
### **顺序一致性**
顺序一致性同样允许对并发操作历史进行重新排序，但它的约束比线性一致性要弱，顺序一致性只要求同一个客户端的操作在排序后保持先后顺序不变，但不同客户端之间的先后顺序是可以任意改变的。**顺序一致性和线性一致性的主要区别在于没有全局时间的限制。**
### **因果一致性**
因果一致性(Causal Consistency) 是一种比顺序一致性更弱一些的一致性模型，它与顺序一致性一样不依赖于全局操作的顺序。因果一致性要求，必须以相同的顺序看到因果相关的操作，而没有因果关系的并发操作可以被不同的进程以不同的顺序观察到。
### **最终一致性**
在最终的状态下，只要不再执行写操作，读操作将返回相同的、最新的结果，这就是最终一致性（Eventual Consistency）。

## 以客户端为中心的一致性模型
- **单调读(Monotonic Read)** 一致性模型是一种简单的以客户端为中心的一致性模型，单调读一致性必须满足：如果客户端读到关键字x的值为v，那么该客户端对于x的任何后续的读操作必须返回v或比v更新的值，即保证客户端不会读到旧的值。
- **单调写(Monotonic Write)** 一致性必须满足：同一个客户端的写操作在所有副本上都以同样的顺序执行，即保证客户端的写操作是串行的。
- **读你所写(Read Your Write)** 一致性也称为读己之写(Read My Write)一致性，该一致性要求：当写操作完成后，在同一副本或其他副本上的读操作必须能够读取到写入的值。
- **PRAM(Pipelined RAM)** 一致性也称为FIFO一致性，直译为"流水线随机访问存储器一致性"，它由单调读、单调写和读你所写三分一致性模型组成。PRAM一致性要求：同一个客户端的多个写操作，将被所有的副本按照同样的执行顺序观察到，但不同客户端发出的写操作可以以不同的执行顺序被观察到。
## 一致性和隔离级别的对比
一致性模型和隔离级别的相同点是，它们本质上都是用来描述系统能够容忍哪些行为，不能容忍哪些行为，更严格的一致性模型或隔离级别意味着更少的异常行为，但以降低系统性能和可用性为代价。

一致性模型和隔离级别的一个主要区别就是，一致性模型适用于单个操作对象，比如单个数据项或单个变量的读写，该数据可能存在多个副本；而隔离级别通常涉及多个操作对象，比如在并发事务中修改多个数据。 对于严格一致性模型和隔离级别-----线性一致性和串行化，还有一个重要的区别是，线性一致性提供实时保证，而串行化没有。

一个数据存储系统可以同时保证线性一致性和串行化，这类系统称为严格串行化(Strict Serializable)。这个模型保证了多个事务执行的结果等同于他们的串行执行的结果，同时执行顺序与实时排序一致-----就像单机单线程程序那样。
![[Pasted image 20250729102746.png]]



## 分布式共识
> “共识”、“一致性”并不是同一个概念，二者存在一些细微的差别：共识侧重于研究分布式系统中的节点达成共识的过程和算法，一致性则侧重于研究副本最终的稳定状态。另外，一致性一般不会考虑拜占庭容错问题。

共识算法常用来实现多副本日志，共识算法使得每个副本对日志的值和顺序达成共识，每个节点都存储相同的日志副本，这样整个系统中的每个节点都能有一致的状态和输出。最终，这些节点看起来就像一个单独的、高可用的状态机。

在共识的帮助下，分布式系统不仅可以像单一节点一样工作，还可以具备高可用性、自动容错和高性能。借助共识算法来实现状态机复制，能够解决分布式系统中的大部分问题，所以共识问题是分布式系统最基本、最重要的问题。
### FLP不可能定理
一个分布式共识算法需要具备的两个属性：安全性(Safety)和活性(Liveness)。安全性意味着所有正确的进程都认同同一个值，活性意味着分布式系统最终会认同某一个值。

FLP不可能定义并不是说，共识算法在一般情况下是不可能实现的，相反，这种不可能的结果来自算法流程中最坏的结果：
- 一个完全异步的系统
- 系统发生了故障
- 不可能有一个具备安全性、活性和容错性的公式算法
一般有三种办法来绕过FLP不可能定理：
- 故障屏蔽(Fault Masking)
- 使用故障检测器(Failure Detectors)
- 使用随机性算法(Non-Determinism)

## Paxos算法原理
在分布式环境下，就某个值(提案)达成一致这个问题得复杂度，主要受到下面两个因素得共同影响：
- 系统内部各个节点间得通讯是不可靠得。消息可能延迟，可能丢失，但是不去考虑消息是否有传递错误得情况，一般内网不需要考虑拜占庭容错。
- 系统外部各个用户访问是可并发的。如果系统只有一个用户，那单纯地应用Quorum机制，少数服从多数节点，就已经可以保证值被正确地读写了。

## Paxos算法实现流程
最基本的Paxos算法也叫作Basic Paxos。Basic Paxos算法使系统达成共识并决议出单一的值。
1. 第一阶段
```go
// 提议者收到客户端的请求后，向超过半数的接受者广播Prepare消息
send Prepare(++n)

// 接受者的处理流程的伪代码为：
if (n > max_n)
	max_n = n   //保存见过的最大提案编号
	if (proposal_accepted == true) // 是否已经有提案被接受
		response: PROMISE(n, accepted_N, accepted_VALUE)
	else
		response: PROMISE(n)
else
	do not response (or response with a "fail" message)
```

2. 第二阶段
当提议这收到超过半数的接受者的Pomise()响应后，提议者向多数派的接受者发起Accept(n, value)请求，这次要带上提案编号和提案值。
关于提案的值的选择，需要说明的是，如果之前接受者的Promise()响应有返回已接受的值accepted_VALUE，**那么使用提案编号最大的已接受值作为提案值**。如果没有返回任何accepted_VALUE，那么提议者可以自由决定提案值。这才符合前面说的，Basic Paxos只决议出单一的提案值，并且之后都使用该值继续运行算法。
提议者的伪代码如下：
```go
// 是否收到多数派接受者的响应
did I receive PROMISE responses from a majority of acceptors?
if yes
	do any reponses contain accepted values (from other proposals)?
	if yes
		val = accepted_VALUE // 从PROMISE消息收到提案值
	if no
		val = VALUE // 可以自由决定提案值
	send Accept(ID, val) to at least a majority of acceptors
```

接受者的伪代码如下：
```go
if (n >= max_n) // 提案编号n是否依旧是接受者见过的最大的提案编号
	proposal_accepted = true  // 接受该提案
	max_n = n // 更新承诺的提案编号
	accepted_N = n // 保存提案编号
	accepted_VALUE = VALUE // 保存提案值
	respond: Accepted(N, VALUE) to the proposer and all learners
else
	do not respond (or respond with a "fail" message)
```

可见，接受提案和批准提案是不同的，接受提案是接受者单独决定的，而批准提案需要满足超过半数接受者接受提案。

### 活锁问题
提议者在phase 1a发出Prepare请求消息，还没来得及发送phase 2a的Accept请求消息，紧接着第二个提议者在phase 1a又发出提案编号更大的Prepare请求，如果这样运行，接受者始终停留在决定提案编号的大小这一过程中，那么大家谁也成功不了。
![[Pasted image 20250730150240.png]]
解决活锁问题最简单的方式就是引入随机超时，某个提议者发现提案没有被成功接受，则等待一个随机超时时间，让出几乎，减少一直互相抢占的可能性。

## Multi-Paxos
一个Paxos实例(Instance)用来决议出一个值，多个paxos实例可以并行运行的。
如果每一条日志都通过一个Paxos实例来达成共识，那么每次都要至少两轮通信，这会产生大量的网络开销。所以需要对Basic Paxos做一些优化，以提升性能。这种经过一系列优化后的Paxos被称为Multi-Paxos。Multi-Paxos的目的就是高效地实现状态机日志复制。
**Multi-Paxos对Basic Paxos的核心改进是，增加了"选主"的过程**
具体的优化过程如下：
1. 确定日志索引，日志中包含多个日志条目，如果要通过Paxos不断确认一条条日志的值，那么需要知道当次Paxos实例在写日志的第几位。所以Multi-Paxos做的第一个调整就是添加一个关于日志索引的index参数到Basic Paxos的第一阶段和第二阶段，用来表示某一轮Paxos正在决策哪一个条目。
> 针对冲突和消息轮次过多这两个问题，Multi-Paxos通过一下两个方式优化：
2. 领导者(Leader)选举。从多个提议者中选择一个领导者，任意时刻只有领导者一个节点来提交提案，这样可以避免提案冲突。另外，如果领导者发生故障，则可以从提议者中重新选择一个领导者，所以不存在单点故障问题。
3. 减少第一阶段的请求。有了领导者之后，由于提案都是从领导者这里提出的，实际上可以从发起端保证提案编号的单调递增，因此只需要对整个日志发送一次第一阶段的情趣，后续就可以直接通过第二阶段来发送提案值，使得日志被批准。
4. 副本的完整性，**第一个优化**为了让日志尽可能比复制到每台服务器，领导者在收到超过半数的接受者的回复后，可以继续处理后续请求，同时在后台继续对未回复的接受者进行重试，尝试将提案复制给所有接受者。**第二个优化**为了追踪哪些日志记录是被批准的，增加两个变量：acceptedProposal是一个数组，`acceptedProposal[i]`代表第i条日志的提案编号，如果第i条日志中的命令被批准，则`acceptedProposal[i]`等于无穷大。每个节点都维护一个firstUnChosenIndex变量，表示第一个没有被批准的日子位置。
5. 客户端请求，客户端为每个请求都附加一个唯一id，服务器将该id与请求中的命令一起保存到日志中。状态机在批准一条日志之前，会根据请求的唯一id检查该命令是否被执行过。
6. 配置变更，对于同一个位置的日志，新旧配置中的集群同时组成了两个不同的多数派，批准了不同的值，这会使状态进入异常状态。这是就必须设计一种配置变更方法来避免上述问题。Lamport在Paxos论文中提出的解决方案是使用日志来管理配置变更，即将当前的系统配置当做一条日志记录存储起来，并与提案相关的日志一起复制和同步。Multi-Paxos增加了一个系统参数变量，表示新的配置在多少条记录后才生效，用来平滑过渡配置变更，以及控制什么时候真正应用新配置，如下图所示：
![[Pasted image 20250731181753.png]]

## 其他paxos变体
![[Pasted image 20250731173029.png]]
## Raft
**把共识问题分解为"Leader Election"、"Entity Replication" 和 "Safety"三个问题来思考**
Raft算法和Multi-Paxos算法一样是基于领导者(Leader)的共识算法，因此，Raft算法主要讨论两种情况下的算法流程：领导者正常运行；或者领导者故障，必须选出新的领导者接管和推进算法。
使用基于领导者的算法的优势是，只有一个领导者，逻辑简单。但难点是，当领导者发生改变时，系统可能处于不一致的状态，状态机日志也会堆积一些没有批准的日志，因此，当下一任领导者上任时，必须对状态机日志进行清理。

下面是Raft算法的关键步骤，以及Raft解决方案：
1. 领导者选举。
![[Pasted image 20250801182349.png]]
可以用解决活锁的办法来解决活性问题，即节点随机选择超时时间。
>Raft算法维持了以下两个特性：
>- 如果两个节点的日志在相同的索引位置上的任期号相同，则认为它们具有一样的命令，并且从日志开头到这个索引位置之间的日志也完全相同。
>- 如果给定的记录已提交，那么所有前面的记录也已提交。


3. 算法正常运行。
4. 领导者变更时的安全性和一致性。
5. 处理旧领导者。
6. 客户端交互。
7. 配置变更。
8. 日志压缩。
9. 实现线性一致性。
10. 性能优化。

## 分布式事务
### 原子提交
常见的机械磁盘一般可以保证512字节的原子写。基于硬盘原子写和日志文件来实现事务原子性的方法，在文件系统和数据库中广泛使用。

两阶段提交只满足弱终止条件，如果协调者发生故障，其他没有发生故障的参与者无法决定事务走向，则不满足强终止条件，所以两阶段提交时一种阻塞提交算法。

三阶段提交与两阶段提交最大的不同是，三阶段提交时非阻塞协议，即使协调者发生故障，参与者仍然会选举出新的协调者来推进事务的执行。三阶段提交增加可用性，防止协调者成为一个单点故障。

三阶段提交协议满足了强终止性，但是受网络分区影响并且通信代价较高，出于这个原因，两阶段提交仍然是实现事务原子性的第一选择。

### 两阶段提交和Paxos提交相关名词对比
| 两阶段提交         | Paxos提交                |
| ------------- | ---------------------- |
| 协调者           | 接受者/领导者                |
| 准备(Prepare)消息 | 准备(Prepare)消息          |
| 准备好"是"消息      | Phase 2a 值为Preapared消息 |
| 提交消息          | 提交消息                   |
| 准备失败"否"消息     | Phase 2a 值为Aborted 消息  |
| 中止消息          | 中止消息                   |
Paxos 提交算法是一种基于复制的容错提交算法，Paxos实际上在两阶段提交的基础上引入了多个协调者(接受者)，所以其优点是，只要多数派(f+1个)接受者正常运行，就不会阻塞事务运行。但该算法将Paxos算法和两阶段提交紧密耦合在一起，在工程实现上颇具难度。

### 基于Quorum的提交协议
Quorum提交协议能解决三阶段提交的正确性问题。
基于Quorum的提交协议的一大缺点是，如果系统发生多个连续的、小的网络分区，在这种极端情况下，基于Quorum的提交协议可能会长时间无法做出决议。另一个缺点和三阶段提交一样，基于Quorum的提交协议也存在多轮消息，同样会增加事务的完成时间，不适合对延迟敏感的业务。

### Saga事务
Saga事务在分布式系统中的优点是，既满足业务需求，又保持系统松耦合架构，一些没有提供事务的存储系统可以借用Saga事务的方式由业务方实现分布式事务。

但Saga事务无法保证隔离性，这将导致一些问题。当然，通常来说这种违反隔离性的行为不会产生非常严重的后果；但在某些场景中，这种行为是无法接受的，那么就不能使用Saga事务。

分布式系统的Saga事务只能作为一种折中的选择，并不能提供完整的事务特性。

### 并发控制分为以下三类
- **悲观并发控制(Pessimistic Concurrency Control，PCC)**,这种方法通常涉及锁。在悲观并发控制中，如果有有锁争抢，比如多个事务同时访问同一个数据，就会造成事务等待。所以这里其实是为了正确性而牺牲了性能。该方法主要用来实现串行化的隔离级别。
- **乐观控制(Optimistic Concurrency Control，OCC)**,这种方法乐观地认为并发事务(尤其是并发写操作)是小概率事件，于是到事务结束时再检查事务是否正确。
- **多版本并发控制(Multi-Version Concurrency Control，MVCC或MCC)**，这种方法对每个数据项保存多个版本的数据，并保证每个事务的度操作读取到比该事务早的最后一次提交的数据。
## 两阶段锁
两阶段锁(Two-Phase Locking，2PL)是一种悲观并发控制，它使用锁来防止并发事务对数据的干扰，以实现串行化的隔离级别。

所谓两阶段锁，是根据获得锁和释放锁，将一个事务分为扩张阶段(Expanding Phase) 和 收缩阶段(Shrinking Phase)。在扩张阶段，事务不断上锁，但不允许释放任何锁；在收缩阶段，事务陆续释放锁，但没有新的加锁动作。

两阶段锁还有一些变种，例如：
- 严格两阶段锁(Strict Two-Phase Locking，S2PL)：除了要满足两阶段锁，还要求事务必须在提交之后方可释放它持有的所有排它锁。
- 强严格两阶段锁(Strong Strict Two-Phase Locking，SS2PL)：要求事务只有在结束之后才能释放事务的所有锁。大多数数据库系统选择强严格两阶段锁实现串行化。

两阶段锁协议，会引起死锁，为了让系统正常运行，必须要解决死锁问题。解决死锁有三种手段：
- 死锁避免(Deadlock Avoidance)，即从根本上避免死锁的形成。一种避免死锁的方法是，实现判断事务是否真的需要所有的锁，尽量值获取必要的锁，并且以一种有序的方式去加锁，这种方式适合涉及数据量较少的情况。另一种方式减小锁的粒度，使用行锁，保证只锁住必要的资源。
- 死锁预防(Deadlock Prevention)，即如果发现获取的资源被锁住就尝试终止占有资源的事务，主要有两种方案：
> - 等待-死亡(Wait-Die)
> - 伤害-等待(Wound-Wait)
> 以上两种方案还需要给事务分配一个时间戳，另外有一些死锁预防方案是不需要时间戳的，例如：
> - 无等待算法(No-Waiting Algorithm)。这种方法非常简单，如果一个事务无法获取资源的锁，那么它将立即中止，稍后重试。
> - 谨慎等待(Cautious Waiting)。

- 死锁检测(Deadlock Detection)，如果通过死锁避免和死锁预防仍然没有杜绝死锁，就需要在事务运行过程中检测死锁，并从中干扰并打破死锁。
**两阶段锁为了正确处理数据，增加了系统负载，降低了数据库或存储引擎的并发性能，且存在产生死锁的风险。**

## 乐观并发控制
乐观并发控制算法主要有两类，一类是基于检查的并发控制，另一类是基于时间戳的并发控制。
- 基于检查的并发控制(Validation-Based Concurrency Control)为每个事务涉及的数据创建一个私有的副本，所有的更新操作都在私有副本上执行，再通过检查原来的数据是否有冲突来决定是否能够提交事务。
- 基于时间戳的并发控制(Timestamp-Based Concurrency Control，Time Ordering 或 Basic T/O)通过时间戳来安全地处理并发事务。
![[Pasted image 20250805111010.png]]
基于时间戳的并发控制可以解决常见的并发冲突，事务之间不必相互等待，且不会造成死锁；在单个事务读写的数据不多且事务事务之间涉及的数据基本没有交集的情况下，可以节省大量的额外的成本，提升系统的并发性能。 缺点是，可能产生不可恢复的操作。

虽然基于检查的乐观并发控制提高了系统的并发性，但是校验阶段也需要等待一段检查时间。基于时间戳的并发控制的缺点是，可能产生不可恢复的操作。
## 多版本并发控制
多版本并发控制可以看做在乐观并发控制的基础上增加了多个版本，即每个数据项存储多个版本，每个事务读到的都是某个版本的数据项，写操作不覆盖已有数据，而是新建一个新的版本，直到事务提交后才变为可见。这样一来，正在执行写操作的事务就不会阻塞需要读取相同数据的事务。因此，多版本并发控制有着非常好的读性能，适合实现快照隔离的隔离级别。
通用的元数据：
- txn-id，该字段记录持有当前数据项写锁的事务的开始时间戳Tid。如果没有事务持有当前数据项的写锁，则该字段为0。事务T可以通过CAS操作来修改该字段，从而避免使用锁。
- begin-ts，创建该版本数据项的事务提交时的时间戳，提交时间戳用Tcommit表示。
- end-ts，如果该数据项是最新版本，则end-ts等于无限大(用INF表示)；否则该数据项等于相邻(上一个或上一个) 版本数据项的begin-ts。
主流的三种多版本并发控制分别是：
- **多版本两阶段锁(Multi-Version Two-Phase Locking，MV2PL)**，这里实现的关键是如何处理死锁，在这里无等待算法是可伸缩性最好的策略。Oracle、Postgres和Mysql-InnoDB等都使用多版本两阶段锁来实现多版本并发控制。
- **多版本乐观并发控制(Multi-Version Optimistic Concurrency Control，MVOCC)**，多版本乐观并发控制是在基于检查的并发控制的基础上增加了多个版本，为了兼容多版本并发控制，需要稍微修改基于检查的并发控制，不再维护一个存储私有副本的私有工作空间，因为数据项的元数据已经防止了其他事务访问到其不可见的数据项。MemSQL等数据库使用多版本乐观并发控制来实现多版本并发控制。
- **多版本时间戳排序(Multi-Version Timestamp Ordering，MVTO)**
## 版本存储和垃圾回收

### 主要有以下三种版本存储策略
- 仅追加存储(Append-Only Storage)，这种策略将所有元组存储在同一个表中，新版本数据追缴到现有原则列表末尾，然后修改指针指向新元组。
指针方向既可以从最老版本到最新版本，也可以从最新版本到最老版本，如下图所示：
![[Pasted image 20250804200517.png]]
![[Pasted image 20250804200526.png]]
![[Pasted image 20250804200528.png]]
仅追加存储在Postgres、MemSQL等系统中使用。
- **时间旅行存储(Time-Travel Storage)**，主要思想是单独用一个时间旅行表(Time-Travel Table)来存储历史版本，而最新版本的数据存储在主表中。新增一个新版本的元组时，现在时间旅行表中增加一行，然后将主表的数据复制到这个为止，最后修改主表中的最新版本。
![[Pasted image 20250804200905.png]]
- **增量存储(Delta Storage)**，增量存储每次只讲发生变化的字段信息存储到增量存储中。新增版本时，系统将字段修改信息存储到增量存储中，注意，这里只存储修改的属性，而不是完整的属性。最后，系统直接在原地修改主表信息。增量存储在Mysql和Oracle中被称为回滚段(Rollback Segment)。
![[Pasted image 20250804201101.png]]

增量存储的优势是，对于更新操作频繁的工作负载，可以减少内存分配。但是对于读操作频繁的工作负载，需要访问回滚段才能重新拼接出需要的信息（需要读取更老的数据时），开销会更高。

### 垃圾回收策略
- 第一种策略：元组级别垃圾回收(Tuple-Level Garbage Collection)，又细分为后台清理和协同处理。
- 第二种策略：事务级别垃圾回收(Transaction-Level Garbage Collection)。
无论哪种方案，增加垃圾回线程都可以加速垃圾回收。

## 时钟同步(NTP)
如果NTP发现系统时钟走快了，则NTP客户端要减少当前时间，回退到过去某个时间，如果程序运行的时间比回退的时间还要短，那么计算结果甚至是负数。 针对这个问题，很多语言或操作系统提供单调时钟(Monotonic Clock) 来解决。在Java中System.nanoTime()函数就是一个单调时钟。在Linux系统中可以通过clock_gettime(CLOCK_MONOTONIC)函数来计算单带哦时钟。

单调时钟只能在单机系统中使用，在分布式系统中有着很大的局限性。

->表示happens before，=>表示分布系统的全序关系
逻辑时钟定义，如果a->b，则事件a和事件b满足以下三个条件：
1. 如果事件a、b在同一个进程中，事件a发生于b之前，那么a->b。
2. 如果a是发送一条消息的事件，而b是收到这条消息的事件，那么a->b。
3. 记录a->成立，且b->c成立，那么a->c成立。如果两个事件a、b不满足a->b或者b->a，那么认为a、b这里两个事件是并发的(可以用a || b来表示并发事件)。

定义a=>b满足：
1. `Ci(a)<Cj(b)`
2. 或者`Ci(a)=Cj(b)并且i<j`
逻辑时钟的全序关系其实是在偏序关系的基础上，通过定义一个任意的优先级，最终得到全序关系。

逻辑时钟可以帮助我们在分布式系统中实现多进程状态同步，这需要所有进程都互相进行通信，进程之间互相传输命令。回顾Paxos算法和Raft算法，它们都通过日志复制来传递日志(逻辑时钟)。
逻辑时钟的背后最重要的思想是如何构建分布式系统的状态机。

## 向量时钟(Vector Clock)
向量时钟的主要思想是，让每个进程都能够知道系统中其他所有进程的时钟，这样就无须额外指定进程的优先级。向量时钟依旧只能得出分布式系统中的事件的偏序关系，不过向量时钟包含了其他节点的信息，通常用于数据冲突检测。
## 分布式快照
Chandy-Lamport算法就是朱敏的生成分布式快照的算法，具体流程为以下三个步骤：
1.  初始化快照。假设从进程Pi开始初始化快照，Pi先记录自己的状态，然后将marker消息通过出边管道广播给系统中的其他进程，同时进程Pi记录自己所有的路边Cji(其中j不等于i)收到的消息。
2. 传播快照。任意进程Pj从入边Ckj第一次收到marker消息时，Pj记录自身状态，并将管道Ckj状态记为空，然后将marker消息广播给其他进程，同时开始记录其他路边Clj(其中l不等于j或k)的消息。如果收到的消息不是marker消息，则一直记录这些消息，直到收到marker消息。
3. 终止快照。所有进程从所有的路边都收到了marker消息时，相当于所有进程都记录了自己的快照。之后，某个控制服务器会收集每个进程的局部快照，构建一个全局快照。
![[Pasted image 20250806113159.png]]
Chandy-Lamport算法通过将分布式系统建模为有向图，展示了一种简单、有效的分布式快照算法。分布式流处理框架Flink基于Chandy-Lamport算法，设计了异步生成轻量级分布式快照算法。


## 容错策略
**容错策略，指的是"面对故障，我们该做些什么"；而容错设计模式，指的是"要实现某种容错策略，我们该如何去做"**。
- **第一种容错策略，是故障转移(Failover)** 故障转移是指，如果调用的服务器出现故障，系统不会立即向调用者返回失败结果，而是自动切换到其他服务副本，尝试其他副本能否返回成功调用的结果，从而保证了整体的高可用性。
- **第二种容错策略，是快速失败(Failfast)** 故障转移策略能够实施的前提，是服务具有幂等性。那对于非幂等的服务，重复调用就可能产生脏数据，引起的麻烦远大于单纯的某次服务调用失败。这时候，就应该吧快速失败作为首选的容错策略。
- **第三种容错策略，是安全失败(Failsafe)** 旁路逻辑调用即使失败了，也当作正确来返回，记录error日志备查即可，不影响主路调用逻辑。
- **第四种容错策略，是沉默失败(Failsilent)** 如果大量请求要等到超时才宣告失败，很容易因为某个远程服务的请求堆积而消耗大量的线程、内存、网络等资源，进而影响整个系统的稳定性。面对这种情况，一种合理的失败策略就是当请求失败后，就默认提供者一定时间内无法再对外提供服务，不再向它分配请求流量，并将错误隔离起来，避免对系统其他部分产生影响。
- **第五种容错策略，是故障恢复(Failback)** 故障恢复一般不单独存在，而是作为其他容错策略的补充措施。再微服务管理框架中，如果设置容错策略为故障恢复的话，通常默认采用快速失败加上故障恢复的策略组合。 故障恢复是指，当服务调用出错了以后，将该次调用失败的信息存入一个消息队列中，然后由系统自动开始异步重试调用。(限制重试次数，避免消息堆积)。
> 上面5中绒促策略的英文，都是以"Fail"开头的，它们有一个共同点，都是针对调用失败时如何进行弥补的。下面的两种策略，则是再调用之前就开始考虑如何获得最大的成功概率。
- **第六种容错策略，是并行调用(Forking)** 一开就同时先多个副本发起调用，只要其中任何一个返回成功，那调用便宣告成功。这种策略在一些关键场景中，使用更高的执行成本换取执行时间和成功概率的略。
- **第七种容错策略，是广播调用(Broadcast)** 并行调用是任何一个调用结果返回成功便宣告成功，而广播调用则是任何一个服务提供者出现异常都算调用失败。因此，广播调用通常用于实现"刷新分布式缓存"这类的操作。
![[Pasted image 20250819110911.png]]
## 容错设计模式
### **断路器模式** 
断路器的思路就是通过代理(断路器对象) 来一对一(一个远程服务对应一个断路器对象)地接管服务调用者地远程请求。

断路器会持续监控并统计服务返回地成功、失败、超时、拒绝等各种结果，当出现故障(失败、超时、拒绝)的次数达到断路器的阈值时，它的状态就自动变为"OPEN"。之后这个断路器代理的远程访问都将直接返回失败，而不会发出真正的远程服务请求。

断路器本质上时快速失败策略的一种实现方式。
![[Pasted image 20250819145303.png]]
下图是断路器的状态转换逻辑：
![[Pasted image 20250819145339.png]]
**服务熔断和服务剪辑之间的联系与差别**:
断路器做的事情是自动进行服务熔断，属于一种快速失败的容错策略的实现方法。在快速失败策略明确反馈了故障信息给上有服务以后，上游服务必须能够主动处理调用失败的后果，而不是坐视故障扩散。这里的"处理"，指的就是一种典型的服务降级逻辑，降级逻辑可以是，但不应该只是，把异常信息抛到用户界面去，而应该尽力想办法通过其他路径解决问题，比如把原本要处理的业务记录下来，留待以后重新处理时最低限度的通用降级逻辑。

主动降级，出于应对可预见的峰值流量，或者时系统检修等原因，要关闭系统部分功能或者关闭部分旁路服务。此时服务降级就不一定时处于服务容错的目的了，更可能时属于流量控制的范畴。

## 舱壁隔离模式
对应到分布式系统中，服务隔离，就时避免某一个服务的局部失败影响到全局，而设置的一种止损方案。这种思想，对应的就是容错策略中的失败静默策略。
如下图所示，由于外部服务导致的阻塞：
![[Pasted image 20250819150445.png]]
要解决这类问题，本质上就是要控制单个服务的最大连接数。一种可行的解决方法时为每个服务单独设立线程池，这些线程池不默认不预置活动线程，只用来控制单个服务的最大连接数。
![[Pasted image 20250819150613.png]]
使用局部的线程池来控制服务的最大连接数，有很多好处，比如当服务出问题时能隔离影响，当服务恢复后，还可以通过清理掉局部线程池，瞬间恢复该服务的调用。

但是，局部线程池有一个显著的弱点，那就它额外增加了CPU的开销，每个独立的线程池都要进行排队，调度和上下文切换工作。

为了应对上述情况，还有一种更轻量的控制服务最大连接数的办法，那就是**信号量(Semaphore)**。例如，可以职位每个远程服务维护一个线程安全的计数器。

以上时从微观的的、服务调用的角度应用舱壁隔离设计模式，实际上舱壁隔离模式还可以在更高层、跟宏观的场景中使用，不按调用线程，而是按功能、按子系统、按用户类型等条件来隔离资源都是可以的。
一般来说，我们会选择将服务层面的隔离实现在服务调用端或者边车代理上，将系统层面的隔离实现在 DNS 或者网关处。
## 重试模式
故障转移和故障恢复这两种策略都需要对服务进行重复调用，差别就在于这些重复调用有可能是同步的，也可能是后台异步进行；有可能会重复调用同一个服务，也可能会调用服务的其他副本。但是，无论具体是通过怎样的方式调用、调用的服务实例是否相同，都可以归结为重试设计模式的应用范畴。

**我们判断是否应该且是否能够对一个服务进行重试时，要看是否同时满足下面 4 个条件**。
- 仅在主路逻辑的关键服务上进行同步重试。
- 仅对由于瞬时故障导致的失败进行重试。
- 仅对幂等性的服务进行重试。
- 重试必须有明确的终止条件
	- 超时终止
	- 次数总之：一般3~5次

## 限流的目标
经常用于衡量服务流量压力，但又比较容易混淆的三个指标的定义：
- **每秒事务数(Transactions per Second，TPS)**，TPS是衡量信息系统吞吐量的最终标准。"事务"可以理解为一个逻辑上具备原子性的业务操作。
- **每秒请求数(Hits per Second，HPS)**，HPS是指每秒从客户端发向服务端的请求数。如果只要一个请求就能完成一笔业务，那HPS与TPS是等价的。
- **每秒查询数(Queries per Second，QPS)**，QPS是指一台服务器能够响应的查询次数。如果只有一台服务器来应当请求，那QPS和HPS是等价的，但在分布式系统中，一个请求的响应，往往需要由后台多个服务节点共同协作来完成。
目前来说，**主流系统大多倾向于使用HPS作为首选的限流指标，因为它相对容易观察统计，而且能够在一定成都上反映系统当前以及接下来一段时间的压力。**
但是，像下载、视频、直播等I/O密集型系统，往往会把每次请求和响应报文的大小作为限流指标，而不是调用次数。

##  限流设计模式
### **流量计数器模式**
设置一个计算器，根据当前时刻的流量计数结果是否超过阈值来决定是否限流。
流量计数器模式缺陷的根源在于，它值是针对时间点进行离散的统计。
### **滑动时间窗模式**
滑动时间窗模式的工作过程如下所示：
![[Pasted image 20250820174716.png]]
这样，当频率固定为每秒 1 次的定时器被唤醒时，它应该完成以下几项工作，这也就是滑动时间窗的工作过程：
1. 将数组最后一位的元素丢弃掉，并把所有元素都后移一位，然后在数组第一个插入一个新的空元素。这个步骤即为“滑动窗口”。
2. 将计数器中所有统计信息写入到第一位的空元素中。
3. 对数组中所有元素进行统计，并复位清空计数器数据供下一个统计周期使用。

滑动时间窗口限流模式，它通常只适用于否决式限流，对于超过阈值的流量就必须强制失败或降级，很难进行阻塞等待处理。

**下面两种限流模式适用于阻塞式限流，可以对流量曲线进行整形。**
### **漏桶模式**
可以把"请求"想像成是"水"，谁来了都先放进池子里，水池同时又以额定的速度出水，让请求进入系统中。这样的话，如果一段时间内注水过快的话，水池还能充当缓冲区，让出水口的速度不至于过快。

漏桶在代码实现上也非常简单，它其实就是一个以请求对象作为元素的先入先出队列，队列长度就相当于漏桶的大小，当队列满时就拒绝新的请求进入。漏桶实现比价困难的地方只在于如何确定漏桶的两个参数：**桶的大小和水的流出速率。**

而流出速率在漏桶算法中一般是个固定值，这对于开篇我提到的那个场景应用题中，固定拓扑结构的服务是很合适的；但同时你也应该明白，那是经过最大限度简化的场景，现实世界里系统的处理速度，往往会受到其内部拓扑结构变化和动态伸缩的影响。所以，能够支持变动请求处理速率的令牌桶算法，可能往往会是更受我们青睐的选择。
### **令牌桶模式**
假设我们要限制系统在 X 秒内的最大请求次数不超过 Y，那我们可以每间隔 X/Y 时间，就往桶中放一个令牌，当有请求进来时，首先要从桶中取得一个准入的令牌，然后才能进入系统处理。任何时候，一旦请求进入桶中发现没有令牌可取了，就应该马上失败或进入服务降级逻辑。

总体来说，令牌桶模式的实现看似可能比较复杂，每间隔固定时间，我们就要把新的令牌放到桶中，但其实我们并不需要真的用一个专用线程或者定时器来做这件事情，只要在令牌中增加一个时间戳记录，每次获取令牌前，比较一下时间戳与当前时间，就可以轻易计算出这段时间需要放多少令牌进去，然后一次性放完全部令牌即可，所以真正编码时并不会显得很复杂。
### **分布式限流**
以上限流模式统称为单机限流，而把能够精细控制分布集群中每个服务消耗量的限流算法成为分布式限流。

**一种常见的简单分布式限流方法，是将所有服务的统计结果都存入集中式缓存**（如 Redis）中，以实现在集群内的共享，并通过分布式锁、信号量等机制，解决这些数据在读写访问时的并发控制问题。可是它的代价也显而易见：每次服务调用都必须要额外增加一次网络开销。

**一种可以考虑的办法是在令牌桶限流模式的基础上，进行“货币化改造”改造**。即不把令牌看作是只有准入和不准入的“通行证”，而把它看作是数值形式的“货币额度”。

这种基于额度的限流方案，对限流的精确度会有一定的影响，比如可能存在业务操作已经进行了一部分服务调用，却无法从令牌桶中再获取到新额度，因“资金链断裂”而导致业务操作失败的情况。这种失败的代价是比较高昂的，它白白浪费了部分已经完成了的服务资源，但总体来说，它仍然是一种在并发性能和限流效果上，都相对折衷可行的分布式限流方案。

## 零信任网络安全
为了避免由于单个服务节点出现漏洞被攻击者突破，进而导致整个系统和内网都遭到入侵，我们就必须打破一些传统的安全观念，以此来构筑更加可靠的服务间通讯机制。

### 基于边界的安全模型
长期以来，主流的网络安全观念都比较提倡根据某类与宿主机相关的特征，比如机器所处的位置，或者机器的 IP 地址、子网等等，把网络划分为不同的区域，不同的区域对应不同的风险级别和允许访问的网络资源权限，把安全防护措施集中部署在各个区域的边界之上，重点关注跨区域的网络流量。

边界安全的核心问题在于，边界上的防御措施即使自身能做到永远滴水不漏、牢不可破，也很难保证内网中它所尽力保护的某一台服务器不会成为“猪队友”，一旦“可信的”网络区域中的某台服务器被攻陷，那边界安全措施就成了马其诺防线，攻击者很快就能以一台机器为跳板，侵入到整个内网。

### 零信任安全模型
零信任安全的中心思想是**不应当以某种固有特征来自动信任任何流量，** 除非明确得到了能代表请求来源(不一定是人，更可能是另一台服务)的身份凭证，否则一律不会有默认的信任关系。
其中详细列举了传统的基于边界的网络安全模型，与云原生时代下基于零信任网络的安全模型之间的差异如下所示：
![[Pasted image 20250821134515.png]]
- 零信任网络不等同于放弃在边界上的保护措施
- 身份只来源于服务
- 服务之间也没有固定的信任关系
- 几种、共享的安全策略实施点
- 受信的机器运行来源已知的代码
- 自动化、标准化的变更管理
- 强隔离性的工作负载

## 分布式架构中的可观测
在今天，可观测性一般会被分成事件日志、链路追踪和聚合度量三个主题方向进行探究和研究，我们需要记住一下几个核心要点：
- 事件日志的职责是记录离散事件，通过这些记录事后分析出程序的行为；
- 追踪的主要目的是排查故障，比如分析调用链的哪一部分、哪个方法出现错误或阻塞，输入输出是否符合预期；
- 度量是指对系统中某一类信息的统计聚合，主要目的是监控和预警，当某些度量指标达到风险阈值时就触发事件，以便自动处理或者提醒管理员介入。
![[Pasted image 20250822101329.png]]
## 日志分析的几个核心要点
- 好的日志要能够毫无遗漏地记录信息、格式统一、内容恰当，而"恰当"的真正含义是指日志中不改出现的内容不要有，而该有的不要少。
- 分布式系统处理一个请求要跨越多个服务节点，因此当每个节点输出日志到文件后，就必须要把日志文件统一收集起来，集中存储、索引，而这正是日志收集器需要做的工作。此外，日志收集器还要尽力保证数据的连续性。
- 由于日志是非结构化数据，因此我们需要进行加工，把日志行中的非结构化数据转化为结构化数据，以便针对不同的数据项来建立索引，进行查询、统计、聚合等操作。

## 分布式追踪系统
从广义上讲，一个完整的分布式追踪系统，应该由数据收集、数据存储和数据展示三个相对独立的子系统构成；而从狭义上讲，则就只是特指链路追踪数据的收集部分。比如Spring Cloud Sleuth就属于狭义的追踪系统，通常会搭配Zipkin作为数据展示，搭配ElasticSearch作为数据存储组合使用。 广义的追踪系统通常也被称为"APM系统"(Application Performance Management，应用性能管理)。
### 追踪与跨度
- **追踪(Trace)**：从客户端发起请求抵达系统的边界开始，记录请求流经的每一个服务，直到向客户端返回响应为止，这整个过程就叫做一次"追踪"。
- **跨度(Span)**: 每次开始调用服务前，系统都要先埋入一个调用记录，这个记录就叫做一个"跨度"，Span的数据结构应该足够简单，以便于能放在日志或者网络协议的报文头里；也应该足够完备，起码要包含时间戳、起止时间、Trace的ID、当前Span的ID、父Span的ID等能够满足追踪需要的信息。
下图展示了Trace和Span的关系：
![[Pasted image 20250822164934.png]]
非功能性挑战：
- 低性能损耗
- 对应用透明
- 随应用扩缩
- 持续的监控
## 数据收集的三种主流实现方式
### 基于日志的追踪
- 优点：日志追踪对网络消息完全没有侵入性，对应用程序只有很少量的侵入性，对性能的影响也非常低。
- 缺点：直接依赖于日志归集的过程，日志本身不追求绝对的持续与一致，这就导致了基于日志的追踪，往往不如其他两种追踪实现来的精准。
- ELK在日志、追踪和度量方面都可以发挥作用，这对小型应用确实能起到一定的遍历作用，但是对于大型系统还是要专业的工具，比如日志追踪的代表产品Spring Cloud Sleuth。
### 基于服务的追踪
基于服务的追踪是**目前最为常见的追踪实现方式**，被Zipkin、Skywalking、Pinpoint等主流追踪系统广泛采用。服务追踪的实现思路是通过某种手段给目标应用注入追踪探针(Probe)，比如针对Java应用，一般就是通过Java Agent注入的。
- 优点：追踪更精确
- 缺点：对性能的损耗更大
### 基于边车代理的追踪
- 优点：基于边车代理的追踪是服务网格的专属方案，也是最理想的分布式追踪模型，它对应用完全透明，无论是日志还是服务本身，都不会有任何变化；它与程序语言无关，无论应用是采用什么编程语言来实现的，只要它还是通过网络（HTTP 或者 gRPC）来访问服务，就可以被追踪到；它也有自己独立的数据通道，追踪数据通过控制平面进行上报，避免了追踪对程序通信或者日志归集的依赖和干扰，保证了最佳的精确性。
- 缺点：服务网格现在还不够普及、只能实现服务调用层面的追踪，无法显现本地方法调用级别的追踪诊断。
市场占有率最高的边车代理Envoy就提供了相对完善的追踪功能，但没有提供自己的界面端和存储端，所以 Envoy 和 Sleuth 一样，都属于狭义的追踪系统，需要配合专门的 UI 与存储来使用。SkyWalking、Zipkin、Jaeger、LightStep Tracing等系统，现在都可以接受来自于 Envoy 的追踪数据，充当它的界面端。

### 规范化
可观测性的终极解决方案OpenTelemetry。

## 聚合度量
度量可以分为客户端的指标收集、服务端的存储查询以及终端的监控预警三个相对独立的过程，每个过程在系统中一般也会设置对应的组件来实现。

Prometheus 组件流程图，图中 Prometheus Server 左边的部分都属于客户端过程，而右边的部分就属于终端过程。
![[Pasted image 20250822181652.png]]
### 指标类型
- 计数度量器(Counter)
- 瞬时度量器(Gauge)
- 吞吐率度量器(Meter)
- 直方图度量器(Histogram)
- 采样点分位图度量器(Quantile Summary)
Prometheus就支持了上面提到的五种度量器中的Counter、Gauge、Histogram和Summary四种。

### 如何将这些指标告诉服务端
有两种方式：Pull-Based Metrics Collection、Push-Based Metrics Collection。
Prometeus实际只支持拉模式，但是能够优先度地兼容Push式采集，因为它有Push Gateway的存在。

以 Prometheus 为代表的度量系统就相对强硬，它们不支持任何一种协议，只允许通过 HTTP 访问度量端点这一种访问方式。如果目标提供了 HTTP 的度量端点（如 Kubernetes、Etcd 等本身就带有 Prometheus 的 Client Library）就直接访问，否则就需要一个专门的 Exporter 来充当媒介。
### 存储查询
采用时序数据库来进行存储，这类数据库可以针对指标数据进行相应的存储和查询的优化.

储数据库在写操作时，时序数据通常只是追加，很少删改或者根本不允许删改。因此，针对数据热点只集中在近期数据、多写少读、几乎不删改、数据只顺序追加等特点，时序数据库被允许可以做出很激进的存储、访问和保留策略（Retention Policies）

Prometheus 服务端自己就内置了一个强大的时序数据库实现，我说它“强大”并不是客气，在DB-Engines中近几年它的排名就在不断提升，目前已经跃居时序数据库排行榜的前三。
这个时序数据库提供了一个名为 PromQL 的数据查询语言。

### 监控告警
大多是 Prometheus 配合 Grafana 来进行展示的，这是 Prometheus 官方推荐的组合方案。
Prometheus 提供了专门用于预警的 Alert Manager，我们将 Alert Manager 与 Prometheus 关联后，可以设置某个指标在多长时间内、达到何种条件就会触发预警状态，在触发预警后，Alert Manager 就会根据路由中配置的接收器，比如邮件接收器、Slack 接收器、微信接收器，或者更通用的WebHook接收器等来自动通知我们。