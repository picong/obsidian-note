早期ConcurrentHashMap，其实现是基于：
- 分离锁，也就是将内部进行分段(Segment)，里面则是HashEntry的数组，和HashMap类似，哈希相同的条目也是以链表形式存放。
- HashEntry内部使用volatile的value字段来保证可见性，也利用了不可变对象的机制以改进利用Unsafa提供的底层能力，比如volatile access，去直接完成部分操作，以最优化性能，毕竟Unsafe中的很多操作都是JVM intrinsic优化过的。
**在Java8 和之后的版本中，ConcurrentHashmap
发生了哪些变化呢？**
- 总体结构上，它的内部存储变得和HashMap结构非常相似，同样是打的桶(bucket)数组，然后内部也是一个个所谓的链表结构(bin)，并不的粒度要更细致一些。
- 其内部仍然有Segment定义，但仅仅是为了保证序列化时的兼容性而已，不再有任何结构上的用处。
- 因为不再使用Segment，初始化操作大大简化，修改为lazy-load形式，这样可以有效避免初始开销，解决了版本很多人抱怨的这一点。
- 数据存储利用volatile来保证可见性。
- 使用CAS等操作，在特定场景进行无锁并发操作。
- 使用Unsafe、LongAdder之类底层手段，进行极端情况的优化。

# Exception和Error有什么区别
Exceptin和Error都是继承了Throwable类，在Java中只有Throwable类型的实例才可以被抛出或者捕获，它是异常处理机制的基本组成类型。
![[Pasted image 20240419182635.png]]
我们从性能角度来审视一下Java的异常处理机制，这里有两个可能会相对昂贵的地方：
- try-catch 代码段会产生额外的性能开销，或者换个角度说，它往往会影响JVM对代码进行优化，所以建议仅捕获有必要的代码段，尽量不要一个大的try包住整段代码; 与此同时，利用异常控制代码流程，也不是一个好主意，远比我们通常意义上的条件语句要低效。
- Java每实例化一个Exception，都会对当时的栈进行快照，这是一个相对比较重的操作。如果发生的非常频繁，这个开销可就不能被忽略了。

## 谈谈final、finaly、finalize有什么不同
推荐使用final关键字来明确表示我们代码的语义、逻辑意图，这已经被证明在很多场景下是非常好的时间，比如：
- 我们可以将方法或者类声明为final，这样就可以明确告知别人，这些行为是不许修改的。
- 使用final修饰参数或者变量，也可以清楚地避免意外赋值导致的编程错误，甚至，有人明确推荐将所有方法参数、本地变量、成员变量声明成final。
- final变量产生了某种程度的不可变的效果，所以，可以用于保护只读数据，尤其是在并发编程中，因为明确地不能再赋值final变量，有利于减少额外的同步开销，也可以省去一些防御性拷贝的必要。
某这意义上说，Java语言目前并没有原生的不可变支持，如果要实现immutable的类，我们需要做到：
- 将class自身声明为final，这样别人就不能扩展来绕过限制了。
- 将所有成员变量定义为private和final，并且不要实现setter方法。
- 通常构造对象时，成员变量使用深度拷贝来初始化，而不是直接赋值，这是一种防御措施，因为你无法确定输入对象不被其他人修改。
- 如果确实需要实现getter方法，或者其他可能会返回内部状态的方法，使用copy-on-write原则，创建私有的copy。

finalize真的的那么不堪？
finalize的执行是和垃圾收集关联在一起的，一旦实现了非空的finalize方法，就会导致相应对象回收呈现数量级上的变慢，有人专门做过benchmark，大概是40~50被的下降。
因为，finalize被设计成在对象 **垃圾收集前** 调用，这就意味着实现了finalize方法的对象是个 "特殊公民"，JVM要对它进行额外处理。finalize本质上成为了快速回收的阻碍者，可能导致你的对象经过多个垃圾收集周期才能被回收。
从另一个角度，我们要确回收资源就是因为资源都是有限的，垃圾收集时间的不可预测，可能极大加剧资源占用。这意味着对消耗非常高频的资源，千万不要指望finalize去承担资源释放的主要职责，最多让finalize作为最后的 "守门员"，况且它已经暴露了如此多的问题。这也是为什么推荐，**资源用完即显示释放，或者利用资源池来尽量重用**。
finalize还会掩盖资源回收的出错信息。
## 有什么机制可以替换finalize吗
Java平台目前逐步使用java.lang.ref.Cleaner来替换掉原有的finalize实现。Clear实现利用了幻象引用(PhantomReference)，这是一种常见的所谓post-morterm清理机制。
吸取了finalize里的教训，每个Cleaner的操作都是独立的，它有自己的运行线程，所以可以避免意外死锁等问题。
## 强引用、弱引用、幻象引用有什么区别？
所谓强引用，就是我们最常见的普通对象引用，只要还有强引用指向一个对象，就能表明对象还 "活着"，垃圾收集器不会碰这种对象。
软引用(SoftReference),是一种相对于强引用弱化一些的引用，可以让对象豁免一些垃圾收集，只有当JVM认为内存不足时，才会视图回收软引用指向的对象。JVM会确保在抛出OutOfMemoryError之前，清理软引用指向的对象。软引用通常用来实现内存敏感的缓存，如果还有空闲内存，就可以暂时保留缓存，当内存不足时清理掉，这样就保证了使用缓存的同时，不会耗尽内存。
弱引用(WeakReference)并不能使对象豁免垃圾收集，仅仅是提供一种访问在弱引用状态下对象的途径。这就可以用来构建一种没有特定约束的关系，比如，维护一种非强制性的映射关系，如果视图获取时对象还在，就是用它，否则重新实例化。它同样是很多缓存实现的选择。
对于幻象引用，有时候也翻译为虚引用，你不能通过它访问对象。幻象引用仅仅是提供了一种确保对象被finalize以后，做某些事情的机制，比如，通常用来做所谓的Post-Morterm清理机制。
![[Pasted image 20240422140339.png]]
上图是Java定义的不同可达性级别(reachability level):
- 强可达(Strongly Reachable),就是当一个对象可以有一个或多个线程可以不通过各种引用访问的情况。比如，我们新创建一个对象，那么创建它的线程对它就是强可达。
- 软可达(Softly Reachable)，就是当我们只能通过软引用才能访问到对象的状态。
- 弱可达(Weakly Reachable)，类似前面提到的，就是无法通过强引用或者软引用访问，只能通过弱引用访问时的状态。这是十分临近finalize状态的时机，当弱引用被清理时候，就符合finalize的条件了。
- 幻象可达(Phantom Reachable)，上面流程图已经很直观了，就是没有强、软、弱引用关联，并且finalize过了，只有幻象引用指向这个对象的时候。
- 当然，还有一个最后的状态，就是不可达(unreachable)，意味着对象可以被清楚了。
## 显示地影响软引用垃圾收集
软引用通常会在最后一次引用后，还能保持一段时间，默认值是根据剩余空间计算(以 M bytes为单位)。从Java 1.3.1开始，提供了 `-XX:SoftRefLRUPolicyMSPerMB`参数，我们还可以以毫秒(milliseconds)伪单位设置。比如，下面这个示例就是设置为3秒(3000毫秒)。
```shell
-XX:SoftRefLRUPolicyMSPerMB=3000
```
这个剩余空间，其实会受不同JVM模式影响，对于client模式，剩余空间是计算当前堆里空闲的大小，所以更加倾向于回收；而对于server模式JVM，则是根据-Xmx指定最大值来计算。这个行为还是黑盒，取决于JVM实现，即使是上面提到的参数，在新版的JDK上也未必有效。
## 诊断JVM引用情况
如果你怀疑应用存在引用导致的回收问题，可以有很多工具或者选项可供选择，比如HotSpot JVM自身便提供了明确地选项(PrintReferenceGC)去获取相关信息，我指定了下面选项去使用JDK8运行一个样例应用:
```shell
-XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintReferenceGC
```
## Reachability Fence
除了前面介绍的几种基本引用类型，我们也可以通过底层API来达到强引用的效果，这就是所谓的设置 **reachability fence**。

# String、StringBuffer、StringBuilder有什么区别？
## 字符串设计和实现考量
String是Immutable的典型实现，原生的保证了基础线程安全，因为你无法对它内部数据进行任何修改。
StringBuffer的线程安全是通过把各种修改数据的方法都加上synchronized关键字实现的。为了实现修改字符串序列的目的，StringBuffer和StringBuilder底层都是利用可修改的(char,JDK9以后是byte)数组，二者都继承了AbstractStringBuilder，里面包含了基本操作，区别仅在于最终方法是否加了synchronnized。
如果可以确定字符串的大小，可以初始化时确定这个内部数组的大小，指定合适的大小，避免很多次扩容的开销。扩容会产生多重开销，因为要抛弃原有数组，创建新数组，还要进行arraycopy。
```java
String strByConcat = "aa" + "bb" + "cc" + "dd";
```
在上述拼接操作中，在JDK8中会自动被javac转换为StringBuilder操作；而在JDK9里面，利用InvokeDynamic，将字符串拼接的优化与Javac生成的字节码解耦，假设未来JVM增强相关运行时实现，将不需要依赖Javac的任何修改。
## 字符串缓存
String在java 6以后提供了itern()方法，目的是提示JVM把相应字符串缓存起来，以备重复使用。在我们创建字符串并调用itern()方法的时候，如果已经有缓存的字符串，就会返回缓存里的实例，否则将其缓存起来。一般来说，JVM会将所有的类似 "abc"这样的文本字符串，或者字符串常量之类缓存起来。
但是一般使用Java 6这种历史版本，并不推荐大量使用itern，因为被缓存的字符串是存在所谓PermGen里的，也就是臭名昭著的 "永久代"，这个空间是很有限的，也基本不会被FullGC之外的垃圾收集照顾到，所以，如果使用不到，OOM就会发生。
在后续版本中，这个缓存被放置在了堆中，这样就极大避免了永久代占满的问题，甚至永久代在JDK8中被MetaSpace替代了。而且默认缓存大小也在不断地扩大中，可以通过下面参数试验一下。
```java
-XX:+PrintStringTablestatistics
```
可以通过下面的参数手动调整大小，但是绝大部分情况下并不需要调整，除非你确定它的大小已经影响了操作效率。
```java
-XX:StringTableSize=N
```
Intern是一种显示地排重机制，但是它也有一定的副作用，因为需要开发者写代码时明确调用。
在Oracle JDK 8u20之后，推出了一个新的特性，也就是G1 GC下的字符串排重。它是通过将相同的字符串指向同一份数据来做到的，是JVM底层的改变，并不需要Java类库做什么修改。
注意这个功能目前是默认关闭的，需要使用下面参数启动，并且记得指定使用G1 GC：
```java
-XX:+UseStringDeduplication
```
## String 自身的演化
在Java 9中，引入了Compact Strings的设计，对字符串进行了大刀阔斧的改进。将数据存储方式从char数组，改变为一个byte数组加上一个标识编码所谓的coder，并且将相关字符串操作类都进行了修改。另外，所有相关的Intrinsic之类也都进行了重写，以保证没有任何性能损失。
在极端情况下，字符串也出现了一些能力退化，比如最大字符串的大小。原来char数组的实现，字符串的最大长度就是数组本身的长度限制，但是替换成byte数组，同样数组长度下，存储能力是退化了一倍的！还好这是存在于理论中的极限。
在通用的性能测试和产品实验中，我们能非常明显地看到紧凑字符串带来的优势，即更小的内存占用、更快的操作速度。

# 谈谈Java反射机制，动态代理是基于什么原理？
## 反射机制及其演进
对于Java语言的反射机制本身，可以去看一下java.lang或java.lang.reflect包下的相关抽闲，就会有一个很直观的印象了。Class、Field、Method、Constructor等，这些完全就是我们去操作类和对象的元数据对应的。
反射提供的AccessibleObject.setAccessible(boolean flag)。它的子类也大都重写了这个方法，这里所谓accessible可以理解成修饰成员的public、protected、private，这意味着我们可以在运行时修改成员访问限制！
## 动态代理
首先，它是一个代理机制。代理可以看做是对调用目标的一个包装，这样我们对目标代码的调用不是直接发生的，而是通过代理完成。其实很多动态代理场景，我认为也可以看错是装饰器(Decorator)模式的应用。
通过代理可以让调用者与实现者之间解耦。
从API设计和实现的角度，这种实现仍然有局限性，因为它是以接口为中心的，相当于添加了一种对于被调用者没有太大意义的限制。我们实例化的是Proxy对象，而不是真正的被调用类型，这在实践中还是可能带来各种不便和能力退化。
 cglib动态代理采取的是创建目标类的子类的方式，因为是子类化，我们可以达到近似使用被调用者本身的效果。
# int和Integer有什么区别
## 理解自动装修、拆箱
自动装箱实际上算是一种语法糖。他们发生在编译阶段，也就是生成字节码时一致的。在Java 5中新增了静态工厂方法valueOf，在调用它的时候会利用一个缓存机制，带来了明显的性能改进。按照Javadoc，Integer的这个值默认缓存是-128到127之间。
这种缓存机制并不是只有Integer才有，同样存在于其他的一些包装类，比如：
- Boolean，缓存了true/false对应的实例，确切的说，只会返回两个常来那个实例Boolean.TRUE/FALSE。
- Short，同样是缓存了-128到127之间的数值。
- Byte，数值有限，所以全部都被缓存。
- Character，缓存范围 `\u0000`到 `\u007F`。

## Java原始数据类型和引用类型局限性
- 原始数据类型和Java泛型并不能配合使用：
Java编译器会自动将类型转换为对应的特定类型，这就决定了使用泛型，必须保证相应类型可以转换为Object。
- 无法高效地表达数据，也不便于表达复杂的数据结构，比如vector和tuple。
Java的对象都是引用类型，如果是一个原始数据类型数组，它在内存里是一段连续的内存，而对象数组则不然，数据存储的是引用，对象往往是分散在堆的不同位置。这种设计虽然带来了极大灵活性，但是也导致了数据操作的低效，尤其是无法充分利用现代CPU缓存机制。

理解Java提供的默认排序算法，具体是什么排序方式以及设计思路：
这个问题本身就是有点陷阱的意味，因为需要区分是Arrays.sort()还是Collections.sort() (底层是调用Arrays.sort())；什么数据类型；多大的数据集(太小的数据集，复杂排序是没有必要的，Java会直接进行二分插入排序)等。
- 对于原始数据类型，目前使用的是所谓双轴排序(Dual-Pivot QuickSort)，是一种改进的快速排序算法，早期版本是相对传统的快速排序，你可以阅读源码。
- 而对于对象数据类型，目前则是使用TimSort，思想上也是一种归并和二分插入排序(binarySort)结合的优化排序算法。TimSort并不是Java的独创，简单说它的思路是查找数据集中已经排好序的分区(这里叫run)，然后合并这些分区来达到排序的目的。
![[Pasted image 20240423111027.png]]
## Java提供了哪些IO方式？NIO如何实现多路复用
第一，传统的java.io包，它基于流模型实现，提供了我们最熟知的一些IO功能，比如File抽象、输入输出流等。加护方式是同步、阻塞的方式。java.io包的好处就是代码比较简单、直观，缺点则是IO效率和扩展性存在局限性，容易成为应用性能的瓶颈。
第二，在Java 1.4中引入了NIO框架(java.nio包)，提供了Channel、Selector、Buffer等新的抽象，可以构建多路复用、同步非阻塞IO程序，同时提供了接近操作系统底层的高性能数据操作方式。
第三，在Java 7中，NIO有了进一步的改进，也就是NIO 2，引入了异步非阻塞IO方式。异步IO操作基于时间和回调机制，可以简单理解为，应用操作直接返回，而不会阻塞在那里，当后台处理完成，操作系统会通知相应线程进行后续工作。
![[Pasted image 20240423113149.png]]
## Java NIO概览
NIO的主要组成部分：
- Buffer,高效地数据容器，除了布尔类型，所有原始数据类型都有相应的Buffer实现。
- Channel，类似在Linux之类操作系统上看到的文件描述符，是NIO中被用来支持批量式IO操作的一种抽象。
File或者Socket，通常被认为是比较高层次的抽象，而Channel则是更加操作系统底层的一种抽象，这也使得NIO得以被充分利用现代操作系统底层机制，获得特定场景的性能优化，例如，DMA(Direct Memory Access)等。不同层次的抽象是相互关联的，我们可以通过Socket获取Channel,反之亦然。
- Selector，是NIO实现多路复用的基础，它提供了一种高效地机制，可以检测到注册在Selector上的多个Channel中，是否有Channel处于就绪状态，进而实现了单线程对多Channel的高效管理。Selector同样是基于底层操作系统机制，不同模式、不同版本都存在区别，例如，在最新的代码库里，相关实现如下：
> Linux上依赖于epoll，Windows上NIO2(AIO)模式则是依赖于iocp。

- Charset，提供Unicode字符串定义，NIO也提供了相应的编解码器等，例如通过下面的方式进行字符串到ByteBuffer的转换：
```java
Charset.defaultCharset().encode("Hello world!");
```
NIO利用了单线程轮询事件的机制，通过高效地定位就绪的Channel，俩决定做什么，仅仅select阶段是阻塞的，可以有效避免大量客户端连接时，频繁线程切换带来的问题，应用的扩展能力有了非常大的提高。
![[Pasted image 20240423114758.png]]
## Java有哪几种文件拷贝方式？哪一种最高效？
利用java.io类库，直接为源文件构建一个FileInputStream读取，然后再为目标文件构建一个FileOutputStream，完成写入工作。
```java
public static void copyFileByStream(File source, File dest) throws IOException {
	try (InputStream is = new FileInputStream(source);
		OutputStream os = new FileOutputStream(dest);) {
			byte[] buffer = new byte[1024];
			int length;
			while ((length = is.read(buffer)) > 0) {
				os.write(buffer, 0, length);
			}
		}
}
```
或者，利用java.nio类库提供的transferTo或transferFrom方法实现。
```java
public static void copyFileByChannel(File source, File dest) throws IOException {
	try (FileChannel sourceChannel = new FileInputStream(source).getChannel();
		FileChannel targetChannel = new FileOutputStream(dest).getChannel();) {
			for (long count = sourceChannel.size; count>0;) {
				long transferred = sourceChannel.transferTo(sourceChannel.position, count, targetChannel);
				sourceChannel.position(sourceChannel.position + transferred);
				count -= transferred;
			}
		}
}
```
对于Copy的效率，这个其实与操作系统和配置等情况相关，总体上来说，NIO transferTo/From的方式可能更快，因为它更能利用现代操作系统底层的机制，避免不必要拷贝和上下文切换。
## 1.拷贝实现机制分析
当我们使用输入输出流进行读写时，实际上是进行了多次上下文切换，比如应用读取数据时，现在内核态将数据从磁盘读取到内核缓存，再切换到用户态将数据从内核缓存读取到用户缓存。
![[Pasted image 20240423140618.png]]
而基于NIO transferTo的实现方式，在Linux和Unix上，则会使用到零拷贝技术，数据传输并不需要用户态参与，省去了上下文切换的开销和不必要的内存拷贝，进而可能提高应用拷贝性能。注意，transferTo不仅仅是可以用在文件拷贝，与其内丝的，例如读取磁盘文件，然后惊醒Socket发送，同样可以享受这种机制带来的性能和扩展性提高。

transferTo的传输过程是：
![[Pasted image 20240423140900.png]]
## 2.Java IO/NIO源码结构
Files.copy这个方法其实不是利用transferTo,而是本地技术实现的用户态拷贝。
前面谈了不少机制和源码，总结一下，如何提高类似拷贝等IO操作的性能，有一些宽泛的原则：
- 在程序中，使用缓存等机制，合理减少IO次数(在网络通信中，如TCP传输，window大小也可以看做事类似思路)。
- 使用transferTo等机制，减少上下文切换和额外IO操作。
- 尽量减少不必要的转换工程，比如编解码；对象序列化和反序列化，比如操作文本文件或者网络通信，如果不是过程中需要使用文本信息，可以考虑不要将二进制信息转换成字符串，直接传输二进制。
## 3.掌握NIO Buffer
![[Pasted image 20240423142440.png]]
Buffer有几个基本属性：
- capacity，它反映这个Buffer到底有多大，也就是数组的长度。
- position，要操作的数据其实位置。
- limit，相当于操作的限额。在读取或者写入时，limit的意义很明显是不一样的。比如，读取操作时，很可能将limit设置到锁容纳数据的上限；而在写入时，则会设置容量或容量一下的可写限度。
- mark，记录上一次position的位置，默认是0，算是一个便利性的考虑，往往不是必须得。
前面三个是我们日常使用最频繁的，我简单梳理下Buffer的基本操作：
- 我们创建了一个ByteBuffer，准备放入数据，capacity当然就是缓冲区大小，而position就是0，limit默认就是capacity的大小。
- 当我们写入几个字节的数据时，position就会跟着水涨船高，但是它不可能超过limit的大小。
- 如果我们想把前面写入的数据读出来，需要调用flip方法，将position设置为0，limit设置为以前position哪里。
- 如果还想从头再读一遍，可以调用rewind，让limit不变，position再次设置为0。
## 4.Direct Buffer和垃圾收集
两种特别的Buffer：
- Direct Buffer：如果我们看Buffer的方法定义，你会发现它定义了isDirect()方法，返回当前Buffer是否是Direct类型。这是因为Java提供了堆内和堆外Buffer，我们可以以它的allocate或者allocateDirect方法直接创建。
- MappedByteBuffer：它将文件按照指定大小直接映射为内存区域，当程序访问这个内存区域时将直接操作这块文件数据，省去了将数据从内核空间向用户空间传输的损耗。我们可以使用FileChannel.map创建MappedByteBuffer，它本质上也是种Direct Buffer。
Direct Buffer不在堆上，所以Xmx之类参数，其实并不能影响Direct Buffer等堆外成员所使用的内存额度，我们可以使用下面的参数设置大小：
```java
-XX:MaxDirectMemorySize=512M
```
大多数垃圾收集过程中，都不会主动收集Direct Buffer，它的垃圾收集过程就是基于Cleaner和幻想引用机制，其本身不是public类型，内部实现了一个Deallocator负责销毁逻辑。对它的销毁往往要拖到full GC的时候，所以使用不当很容易导致OutOfMemoryError。
对于Drect Buffer的回收的建议：
- 在应用程序中，显示地调用System.gc()来强制触发。
- 另外一种思路时，在大量使用Direct Buffer的部分框架中，框架会自己在程序中调用释放方法，Netty就是这么做的，有兴趣的可以参考其实现(PlatformDependent0)。
- 重复使用Direct Buffer
可以通过下面参数限制堆外内存大小：
```java
-XX:MaxDirectMemorySize=512M
```
## 跟踪和诊断Direct Buffer内存占用
在JDK 8之后的版本，我们可以方便地使用Native Memory Tracking (NMT)特性来进行诊断，你可以在程序启动时加上下面参数：
```java
-XX:NativeMemoryTracking={summary|detail}
```
注意，激活NMT通常会导致JVM出现5%~10%的性能下降。
运行时，可以采用下面命令进行交互式对比：
```java
// 打印NMT信息
jcmd <pid> VM.native_memory detail
// 进行baseline，以对比分配内存变化
jcmd <pid> VM.native_memory baseline
// 进行baseline，以对比分配内存变化
jcmd <pid> VM.native_memory detail.diff
```
## 谈谈接口和抽象类由什么区别？
面向对象的基本要素：封装、继承、多态
- 封装的目的是隐藏内部实现细节，以便提高安全性和简化编程。
- 继承是代码复用的基础机制。但要注意，继承可以看做是非常紧耦合的一种关系，父类代码修改，子类行为也会变动。
- 多态，你可能立即回想到重写(override)和重载(overload)、向上转型。
进行面向对象编程，掌握基本的设计原则是必须得，S.O.L.I.D原则：
- 单一职责(Single Responsibility)，类或者对象最好是只有单一职责，在程序设计中如果发现某个类承担多种义务，可以考虑进行拆分。
- 开关原则(Open-Close，Open for extension，close for modification)，设计要对扩展开放，对修改关闭。换句话说，程序设计应保证平滑的扩展性，尽量避免因为新增同类功能而修改已有实现，这样可以少产出些回归问题。
- 里氏替换(Liskov Subsitution)，这是面向对象的基本要素之一，进行继承关系抽象时，凡是可以用父类或者基类的地方，都可以用子类替换。
- 接口分离(Interface Segregation)，我们在进行类和接口设计时，如果一个接口里定义了太多方法，其子类很可能面临两难，就是只有部分方法对它是有意义的，这就破坏了程序的内聚性。
对于这种情况，可以通过拆分成功能单一的多个接口，将行为进行解耦。在未来维护中，如果某个接口设计有变，不会对使用其他接口的子类构成影响。
- 依赖反转(Dependency Inversion)，实体应该依赖于抽象而不是实现。也就是说高层次模块不应该依赖于低层次模块，而是应该基于抽象。实践这一原则是保证产品代码之间适当耦合度的法宝。

# synchronized底层如何实现？什么是锁的升级、降级？
synchronized代码块是由一对monitorenter/monitorexit指令实现的，Monitor对象是同步的基本实现单元。
在Java6之前，Monitor的实现完全是依靠操作系统内部的互斥锁，因为需要进行用户态到内核态的切换，所以同步操作是一个无差别的重量级操作。
现代JDK中，JVM对此进行了大刀阔斧的改进，提供了三种不同的Monitor实现，也就是常说的三种不同的锁：偏斜锁(Biased Locking)、轻量级锁和重量级锁，大大改进了其性能。
所谓锁的升级、降级，就是JVM优化synchronized运行的机制，当JVM检测到不同的竞争状况时，会自动切换到适合的锁实现，这种切换就是锁的升级、降级。
当没有竞争出现时，默认会使用偏斜锁。JVM会利用CAS操作，在对象头上的Mark Word部分设置线程ID，以表示这个对象偏向于当前线程，所以并不设计真正的互斥锁。这样做的假设是基于在很多应用场景中，大部分对象生命周期中最多会被一个线程锁定，使用偏斜锁可以境地无竞争开销。
如果有另外的线程视图锁定某个已经被偏斜过的对象，JVM就需要撤销(revoke)偏斜锁，并切换到轻量级锁实现。轻量级锁依赖CAS操作Mark Word来视图获取锁，如果重试成功，就使用普通的轻量级锁；否则，进一步升级为重量级锁。
## ArrayBlockingQueue与LinkedBlockingQueue版本的实现区别：
ArrayBlockingQueue的notEmpty、notFull都是同一个在再入锁的条件变量，而LinkedBlockingQueue则改进了锁操作的粒度，头、尾操作使用不同的锁，所以在通用场景下，LinkedBlockingQueue的吞吐量相对要更好一些。
类似ConcurrentLinkedQueue等，则是基于CAS的无锁技术，不需要在每个操作时使用锁，所以扩展性表现要更加优异。
相对比交另类的SynchronousQueue，在Java 6中，其实现发生了非常大的变化，利用CAS替换掉了原本基于锁的逻辑，同步开销比较小。它是Executors.newCachedThreadPool()的默认队列。
## Java并发类库提供的线程池有哪几种？分别有什么特点？
Executors目前提供了5中不同的线程池配置：
- newCachedThreadPool()，它是一种用来处理大量段时间工作任务的线程池，具有几个鲜明特点：它会试图缓存线程并重用，当无缓存线程可用时，就会创建新的工作线程；如果线程限制时间超过60秒，则被终止并移出缓存；长时间闲置时，这种线程池，不会消耗什么资源。其内部使用SynchronousQueue作为工作队列。
- newFixedThreadPool(int nThreads)，重用指定数目(nThreads)的线程，其背后使用的是无界的工作队列，任何时候最多有nThreads个工作线程是活动的。这意味着，如果任务数量超过了活动队列数目，将在工作队列中等待空闲线程出现；如果有工作线程退出，将会有新的的工作线程被创建，以补足指定的数目nThreads。
- newSingleThreadScheduledExecutor()和newScheduledThreadPool(int corePoolSize)，创建的是个ScheduledExecutorService，可以进行定时或周期性的工作调度，区别在于单一工作线程还是多个工作线程。
- newWorkStealingPool(int parallelism)，这是一个经常被人忽略的线程池，Java 8才加入这个创建方法，其内部会构建ForkJoinPool，利用Work-Stealing算法，并行地处理任务，不保证处理顺序。
下图是Executor框架的基本组成：
![[Pasted image 20240423182538.png]]
- Executor是一个基础的接口，其初衷是将任务提交和任务执行细节结构
```java
void execute(Runnable command);
```
- ExecutorService则更加完善，不仅提供service的管理功能，比如shutdown等方法，也提供了更加全面的提交任务机制，如返回Future而不是void的submit方法。
```java
<T> Future<T> submit(Callable<T> task);
```
- Java 标准类库提供了几种基础实现，比如 `ThreadPoolExecutor`、 `ScheduledThreadPoolExecutor` 、 `ForkJoinPool`。这些线程池的设计特点在于其高度的可调节性和灵活性，以尽量满足复杂多变的实际应用场景。
- Executors则从简化使用的角度，为我们提供了各种方便的静态工厂方法。
![[Pasted image 20240423183749.png]]
线程池的几个基本组成：
- corePoolSize，所谓的核心线程数，可以大致理解为长期驻留的线程数目(除非设置了allowCoreThreadTimeout)。对于不同的线程池，这个值可能会有很大区别，比如newFixedThreadPool会将其设置为nThreads，而对于newCachedThreadPool则是为0.
- maximumPoolSize，顾名思义，就是线程不够时能够创建的最大线程数。同样进行对比，对于newFixedThreadPool，当然就是nThreads，因为其要求是固定大小，而newCachedThreadPool则是Integer.MAX_VALUE。
- keepAliveTime和TimeUnit，这两个参数指定了额外的线程能够闲置多久，显然有些线程池不需要它。
- workQueue，工作队列，必须是BlockingQueue。
- RejectedExecutionHandler，拒绝策略。
线程池既然有生命周期，它的状态是如何表征的呢？
这里有一个非常有意思的设计，ctl变量被赋予了双重角色，通过高低位的不同，既表示线程池状态，又表示工作线程数目，这是一个典型的高效优化。试想，实际系统重，虽然我们可以指定线程极限为Integer.MAX_VALUE，但是因为资源限制，这只是个理论值，所以完全可以进空闲位赋予其他意义。
![[Pasted image 20240423185321.png]]
## 线程池实践
线程池虽然提供了非常强大、方便的功能，但是也不是银弹，使用不当同样会导致问题。我这里介绍典型情况，经过前面的分析，很多方面可以自然的推导出来。
- 避免任务堆积。前面说过newFixedThreadPool是创建指定书目的线程，但是其工作队列是无界的，如果工作线程数目太少，导致处理跟不上入队的速度，这就很有可能占用大量系统内存，甚至出现OOM。诊断时，可以使用jmap之类的工具，查看是否有大量的任务对象入队。
- 避免过度扩展线程。我们通常在处理大量短时任务时，使用缓存的线程池，比如在最新的HTTP/2 client API中，目前的默认实现就是如此。我们在创建线程池的时候，并不能准确预计任务压力有多大、数据特征是什么样子(大部分请求是1K、100K还是1M以上？)，所以很难明确设定一个线程数目。
- 另外如果线程数目不断增长，也需要警惕另外一种可能性，就是线程泄露，这种情况往往是因为任务逻辑有问题，导致工作线程迟迟不能被释放。
- 避免死锁等同步问题。
- 尽量避免在使用线程池时操作ThreadLocal。
## 线程池大小的选择策略
- 如果我们的任务主要是进行计算，那么久就意味着CPU的处理能力是稀缺的资源，我们能够通过大量增加线程数提高计算能力吗？往往不能的，如果线程太多，反倒可能导致大量的上下文切换开销。所以，这种情况下，通常建议按照CPU核的数目N或者N+1.
- 如果是需要较多等待的任务，例如I/O操作比较多，可以参考Brain Goetz推荐算法：
```java
线程数 = CPU核数 x 目标CPU利用率 x (1 + 平均等待时间/平均工作时间)
```
- 上面是仅仅考虑了CPU等限制，实际还可能受各种资源限制影响，。例如端口数受限，当然我们可以通过扩大可用端口范围解决，如果我们不能调整资源的容量，那么就只能限制工作线程的数目了。这里的资源可以使文件句柄、内存等。
## AQS(AbstractQueuedSynchronizer)
从原理上，一种同步往往是可以利用其他的结构实现的，但是对某种同步结构的倾向，会导致复杂、晦涩的实现逻辑，所以，Doug lea选择了将基础的同步相关操作抽象在AbstractQueuedSynchronizer中，利用AQS尾门构建同步结构提供了范本。
AQS内部数据和方法，可以简单拆分为：
- 一个volatile的整数成员表征状态，同时提供了setState和getState方法
- 一个先入先出(FIFO)的等待线程队列，以实现多线程间竞争和等待，这是AQS机制的核心之一。
- 各种基于CAS的基础操作方法，以及各种期望具体同步结构去实现的 acquire/release方法。
利用AQS实现一个同步结构，至少要实现两个基本类型的方法，分别是acquire操作，获取资源的独占权；还有就是release操作，释放对某个资源的独占。

## JVM是怎么解决可见性等问题的呢？
对于一个volatile变量：
- 对该变量的写操作之后，编译器会插入一个写屏障。
- 对该变量的读操作之前，编译器会插入一个读屏障。

Jvm如何根据系统资源(内存、CPU等)情况，在启动时设置默认参数，这就是所谓的Ergonomics机制，例如：
- JVM会大概根据检测到的内存大小，设置最初启动时的堆大小为系统内存的1/64；并将堆最大值，设置为系统内存的1/4。
- 而JVM检测到系统的CPU核数，则直接影响到了Parallel GC的并行线程数目和JIT compiler线程数目，甚至是我们应用中ForkJoinPool等机制的并行等级。
## Java API和工具构成了Java安全基础，可以简单归为三个主要组成部分：
第一，运行时安全机制。可以简单认为，就是限制Java运行时的行为，不要做越权或者不靠谱的事情，具体来看：
- 在类加载过程中，进行字节码验证，以防止不合格的代码影响JVM运行或者载入其他恶意代码。
- 类加载器本身也可以对代码之间进行隔离，例如，应用无法获取启动类加载器(Bootstrap Class-Loader) 对象实例，不同的类加载器也可以起到容器的作用，隔离模块之间不必要的可见性等。目前，Java Applet、RMI等特性已经或逐渐退出历史舞台，类加载等机制总体上反倒在不断简化。
- 利用SecurityManager机制和相关的组件，限制代码的运行时行为能力，其中，你可以定制policy文件和各种粒度的权限定义，限制代码的作用域和权限，例如对文件系统的操作权限，或者监听某个网络端口的权限等。
![[Pasted image 20240430155256.png]]
## 如何写出安全的Java代码？
这个问题有点宽泛，我们可以用特定类型的安全风险为例，如拒绝服务(DoS)攻击，分析Java开发者需要重点考虑的点。
DoS是一种常见的网路攻击，有人称其为"洪水攻击"。最常见的表现是，利用大量机器发送请求，将目标网站的贷款或者其它资源耗尽，导致其无法响应正常用户的请求。
我认为，从Java语言的角度，更加需要重视的是程序级别的攻击，也就是利用Java、JVM或应用程序的瑕疵，进行低成本的DoS攻击，这也是想要写出安全的Java代码所必须考虑的。例如：
- 如果使用的是早期的JDK和Applet等技术，攻击者构建合法但恶劣的程序就想对容易，例如，将其线程优先级设置为最高，做一些看起来无害但空耗资源的事情。幸运的是类似技术已经逐步退出历史舞台，在JDK9以后，相关模块就已经被移除。
- 好戏碰撞就是个典型的例子，对方可以轻易消耗系统优先的CPU和线程资源。从这个角度思考，类似加密、解密、图形处理等计算密集型任务，都要防范被恶意滥用，以免攻击者通过直接调用或者间接触发方式，消耗系统资源。
- 利用Java构建类似上传文件或者其他接受输入的服务，需要对消耗系统内存或存储的上限有所控制，因为我们不能将系统安全依赖于用户的合理使用。其中特别注意的是涉及解压缩功能时，就需要防范Zip bomb等特定攻击。
- 另外，Java程序中需要明确释放的资源有很多种，比如文件描述符、数据库连接，甚至是再入锁，任何情况下都应该保证资源释放成功，否则即使平时能够正常运行，也可能被攻击者利用而耗尽某类资源，这也是可能得DoS攻击。

针对序列化，通常建议：
- 敏感信息不要被序列化！在编码中，建议使用transient关键字将其保护起来。
- 反序列化中，建议在readObject中实现与对象构建过程相同的安全检查和数据检查。

## 后台服务出现明显"变慢",谈谈你的诊断思路？
首先，需要对这个问题进行更加清晰的定义：
- 服务是突然变慢还是长时间运行后观察到变慢？类似问题是否重复出现？
- "慢"的定义是什么，我能够理解是系统对其他方面的请求的反映延时边长吗？
第二，理清问题的症状，这更便于定位具体的原因，有以下一些思路：
- 问题可能来自于Java服务自身，也可能仅仅是受系统里其他服务的影响。初始判断可以先确认是否出现了以外的程序错误，例如检查应用本身的错误日志。对于分布式系统，很多公司都会实现更加系统的日志、性能监控系统。一些Java诊断工具也可以用于这个诊断，例如通过JFR，监控应用是否大量出现了某种类型的异常。
如果有，那么异常可能就是个突破点。
如果没有，可以先检查系统级别的资源等情况，监控CPU、内存等资源是否被其他进程大量占用，并且这种占用是否不符合系统正常运行状况。
- 监控Java服务自身，例如GC日志里面是否观察到Full GC等恶劣情况出现，或者是否Minor GC在边长等；利用jstat等工具，获取内存使用的统计信息也是个常用手段；利用jstack等工具检查是否出现死锁等。
- 如果还不能确定具体问题，对应用进行Profiling也是个办法，但因为它会对系统产生侵入性，如果不是非常必要，大多数情况下并不建议在生产系统进行。
- 定位了程序错误或者JVM配置的问题后，就可以采取相应的补救措施，然后验证是否解决，否则还需要重复上面部分过程。

JWT默认的签名算法 HMAC SHA256是一种带密钥的哈希摘要算法，加密与验证过程都只能由中心化的授权服务来提供，所以**这种方式一般只适合于授权服务与应用服务处于同一个进程中的单体应用。**
而在多方系统，或者是授权服务与资源服务分离的分布式应用当中个，通常会采用非对称加密算法来进行签名，公开方式一般遵循JSON Web Key规范。
这个公钥不能用来签名，但它能被其他服务器用于验证签名是否由私钥所签发的。这样，其他服务器就也能不依赖授权服务器、无需远程通讯，即可独立判断JWT令牌中的信息的真伪了。
## HTTPS 采用混合加密机制
HTTPS采用共享密钥加密和公开密钥加密两者并用的混合加密机制。若密钥能够实现安全交换，那么有可能会考虑仅使用公开密钥加密来通信。但是公开密钥加密与共享密钥加密相比，其处理速度要慢。
在交换密钥环节使用公开密钥加密方式，之后的建立通信交换报文阶段使用共享密钥加密方式。
## 证明公开密钥正确性的证书
为了解决公开密钥是由真实网站发布的，需要使用由数字证书认证机构(CA, Cetificate Authotrity) 和其相关机关颁发的公开密钥证书。
## HTTPS的安全机制
下图是HTTPS的通信步骤：
![[c7163fc9c17df62c20d982a7c455716.png]]

**步骤1：** 客户端发送ClientHello报文开始SSL通信。报文中包含客户端支持的SSL的指定版本、加密组件(Cipher Suite)列表(所使用的加密算法及密钥长度等)。
**步骤2：** 服务器可进行SSL通信时，会以Server Hello报文作为应答。和客户端一样，在报文中包含SSL版本以及加密组件。服务器的加密组件内容是从接收到的客户端加密组件内筛选出来的。
**步骤3：** 之后服务器发送Cerificate报文。报文中包含公开密钥证书。
**步骤4：** 最后服务器发送Server Hello Done报文通知客户端，最初阶段的SSL握手协商部分结束。
**步骤5：** SSL第一次握手结束后，客户端以Client Key Excange报文作为回应。报文中包含通信加密中使用的一种被称为Pre-master secret的随机密码串。该报文已用步骤3中公开密钥进行加密。
**步骤6：** 接着客户端继续发送Change Cipher Spec报文。该报文会提示服务器，在此报文之后的通信会采用Pre-master secret密钥加密。
**步骤7：** 客户端发送Finished报文。该报文包含连接至今报文的整体校验值。这次握手协商是否能够成功，要以服务器是否能正确解密该报文作为判断标准。
**步骤8：** 服务器同样发送Change Cipher Spec报文。
**步骤9：** 服务器同样发送Finished报文。
**步骤10：** 服务器和客户端的Finished报文交换完毕之后，SSL连接就算建立完成。当然，通信会收到SSL的保护。从此处开始进行应用层协议的通信，即发送HTTP请求。
**步骤11：** 应用层协议通信，即发送HTTP响应。
**步骤12：** 最后由客户端断开连接。断开连接时，发送close_notify报文。上图做了一些省略，这步之后再发送TCP FIN报文来关闭与TCP的通信。
以上流程中，应用层发送数据时会附加一种叫做MAC (Message Authentication Code) 的报文摘要。Mac能够差值报文是否遭到篡改，从而保护报文的完整新。
![[Pasted image 20240606142750.png]]

公钥、私钥两个密钥谁加密、谁解密，就构成了两种不同的用途：
- **公钥加密，私钥解密，这种就是加密，** 用于向私钥所有者发送信息，这个信息可能被他人篡改，但是无法被他人知道。
- **私钥加密，公钥解密，这种就是签名，** 用于让所有公钥所有者验证私钥所有者的身份，并能用来防止私钥所有者发布的内容被篡改。但是它不用拿过来保证内容不被他人获得。
	因非堆成加密本身的效率所限,难以支持分组，所以主流的非对称加密算法都只能加密不超过密钥长度的数据，这就决定了非堆成加密不能直接用于大量数据的加密。
	所以 **在加密方面，现在一般会结合对称与非对称加密的优点，通过混合加密来保护信道传输的安全。**

| 类型    | 特点                                                                                                            | 常见实现                                 | 主要用途        | 主要局限              |
| ----- | ------------------------------------------------------------------------------------------------------------- | ------------------------------------ | ----------- | ----------------- |
| 哈希摘要  | 不可逆，即不能解密，所以并不是加密算法，只是一些场景把它当做加密算法使用<br><br>易变性，输入发生1 Bit变动，就可能导致输出结果50%的内容发生改变<br><br>无论输入长度多少，输出长度固定(2的N次幂) | <br><br>MD2/4/5/6、<br>SHA0/1/256/512 | <br><br>摘要  | <br><br>无法解密      |
| 对称加密  | 是指加密和解密是一样的密钥<br><br>设计难度相对较小，执行速度相对较快<br><br>加密明文长度不受限                                                       | <br><br>DES、AES、RC4、IDEA             | <br><br>加密  | 要解决如何把密钥安全地传递给解密者 |
| 非对称加密 | 加密和解密使用的是不同的密钥<br><br>明文长度不能超过公钥长度                                                                            | <br>RSA、BCDSA、EIGamal                | <br>签名、传递密钥 | <br>加密明文长度受限      |

> **额外知识：公开密钥基础设施(Public Key Infrastructure，PKI)**
> 又称公开密钥基础架构、公钥基础建设、公钥基础设施、公开密码钥匙基础建设或公钥基础架构，是一组由硬件、软件、参与者、管理政策与流程组成的基础架构，其目的在于创造、管理、分配、使用、存储以及撤销证书。
> 密码学上，公开密钥基础建设借着数字证书认证中心(Certificate Authority, CA) 将用户的个人身份跟公开密钥链接再一起。对每个证书中心用户的身份