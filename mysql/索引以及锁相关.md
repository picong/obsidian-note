## 索引下推
Mysql5.6引入的索引下推优化 (index condition pushdown)，可以在索引遍历过程中，对索引中包含的字段优先做判断，直接过滤掉不满足条件的记录，减少回表次数。

## 普通索引和唯一索引，应该怎么选择？
**查询过程**
InnoDB的数据是按数据页为单位来读写的。也就是说，当需要读一条记录的时候，并不是将这个记录本身从磁盘读出来，而是以页为单位，将其整体读入内存。在InnoDB中，每个数据页的大小默认是16KB。
因为引擎是按页读写的，所以说，当找到k=5的记录的时候，它所在的数据页就都在内存里了。那么，对于普通索引来说，要多做的那一次 "查找和判断下一条记录"的操作，就只需要一次指针寻找和一次计算。当然，如果k=5这个记录刚好是这个数据页的最后一个记录，那么要取下一个记录，必须读取下一个数据页，这个操作会稍微复杂一些。
计算平均性能差异时，可以认为普通索引和唯一索引之间的性能并几乎差不多。

**更新过程**
当需要更新一个数据页时，如果数据页在内存中就直接更新，而如果这个数据页还没有在内存中的话，在不影响数据一致性的前提下，InnoDB会将这些更新操作缓存在change buffer中，这样就不需要从磁盘读入这个数据页了。在下次查询需要访问这个数据页的时候，将数据页读入内存，然后执行change buffer中与这个页有关的操作。通过这种方式就能保证这个数据逻辑的正确性。
change buffer 在内存中有拷贝，也会被写入到磁盘上。
将change buffer 中的操作应用到原数据页，得到最新结果的过程称为merge。除了访问这个数据页会触发merge外，系统有后台线程会定期merge。在数据库正常关闭的过程中，也会执行merge 操作。
如果能够将更新操作记录在change buffer，减少读磁盘，语句的执行速度会得到明显的提升。而且，数据读入内存是需要占用 buffer pool的，所以这种方式还能够避免占用内存，提高内存利用率。

**那么，什么条件可以使用 change buffer 呢？**
对于唯一索引来说，所有的更新操作都要先判断这个操作是否违反唯一性约束。比如，要往唯一索引上面插入值，就要先判断现在的表中是否已经存在该索引值对应的记录了，而这必须将数据页读入内存才能判断。如果都已经读入到内存了，那直接更新内存会更快，就没有必要使用change buffer了。
因此，唯一索引的更新就不能使用change buffer，实际上也只有普通索引可以使用。
change buffer 用的是buffer pool里的内存，因此不能无限增大。change buffer的大小，可以通过参数 `innodb_change_buffer_max_size`来动态设置。这个参数设置为50的时候，表示change buffer 的大小最多只能占用buffer pool 的 50%。

**change buffer的使用场景**
因为merge的时候是真正进行数据更新的时刻，而 change buffer的主要目的就是将记录的变更动作缓存下来，所以一个数据页做merge之前，change buffer记录的变更越多 (也就是这个页面上要更新的次数越多), 收益越大。
因此，对于写多读少的业务来说，页面在写完以后马上被访问到的概率比较小，此时 change buffer的使用效果最好。这种业务模型常见的就是账单类、日志类的系统。
相反，如果是读到写少，写完之后立马会做查询，那么即使满足了条件，将更新记录先记录在 change buffer，但之后由于马上要访问这个数据页，会立即触发merge过程。这种情况随机访问IO的次数不会减少，反而增加了 change buffer的维护代价。

**change buffer 和 redo log**
这里我们是假设k索引是为普通索引，所以才能用上change buffer。
```sql
mysql> insert into t(id,k) values(id1,k1),(id2,k2);
```
这里，我们假设当前k索引树的状态为，查找到位置后，k1所在的数据页在内存 (InnoDB buffer pool) 中，k2所在的数据页不在内存中。如下图所示是带change buffer 的更新状态图。
![[Pasted image 20240117170826.png]]
这条更新语句做了如下的操作 (按照图中的数字顺序):
1. Page1在内存中，直接更新内存；
2. Page2没有在内存中，就在内存的change buffer区域，记录下 "我要往page2插入一行"这个信息
3. 将上述两个动作记入 redo log 中 (图中3和4)。
做完上面这些，事务就可以完成了。所以，你会看到，执行这条更新语句的成本很低，就是写了两处内存，然后写了一处磁盘 (两次操作合在一起写了一次磁盘),而且还是顺序写的。

```sql
select * from t where k in (k1, k2);
```
如果插入后不久，假设内存中的数据还在，就执行上述sql语句，那么此时的这两个读操作就与系统表空间 (ibdata1) 和 redo log (ib_log_fileX) 无关了。
![[Pasted image 20240117171409.png]]
从图中可以看到：
1. 读page1的时候，直接从内存返回。有人会有疑问，WAL之后如果读数据，是不是一定要读盘，是不是一定要从redo log里面把数据更新以后才可以返回？其实是不用的。你可以看一下图3的这个状态，虽然磁盘上还是之前的数据，但是这里直接从内存返回结果，结果是正确的。
2. 要读page2的时候，需要把page2从磁盘读入内存，然后应用change buffer里面的操作日志，生成一个正确的版本并返回结果。
所以，如果简单地对比change buffer 和 WAL这两个机制在提升性能上的收益的话，**redo log 主要节省的是随机写磁盘的IO消耗 (转成顺序写), 而change buffer 主要节省的则是随机读磁盘的IO消耗。**

## Mysql 为什么有时候会选错索引
**优化器的逻辑**
扫描行数是影响执行代价的因素之一。扫描的行数越少，意味着访问磁盘数据的次数越少，消耗的CPU资源越少。
当然，扫描行数并不是唯一的判断标准，优化器还会结合是否使用临时表、是否排序等因素进行综合判断。

**扫描行数是怎么判断的?**
可以通过 `show index from t`, 中的Cardinality列来查看对应索引在Mysql中统计的区分度基数是多少。
**Mysql 通过采样统计的方法来得到索引的基数**(把整张表取出来一行行统计可以得到精确结果，但是代价太高了，所以只能选择 "采样统计")。
采样统计的时候，InnoDB默认会选择N个数据页，统计这些页面上的不同值，得到一个平均值，然后乘以这个索引的页面数，就得到了这个索引的基数。
而数据表是会持续更新的，索引统计信息也不会固定不变。所以，当变更的数据行数超过 1/M的时候，会自动触发重新做一次索引统计。
在Mysql中，有两种存储索引统计的方式，可以通过设置参数indodb_stats_persistent的值来选择：
- 设置为on的时候，表示统计信息会持久化存储。这时，默认的N是20，M是10。
- 设置为off的时候，表示统计信息只存储在内存中。这时，默认的N是8，M是16.

如果再实践中，发现explain的结果预估的rows值跟实际情况差距比较大，可以采用：
```sql
analyze table t
```
analyze table t 命令，可以用来重新统计索引信息。

**索引选择异常和处理**
- 一种方法是，采用 force index强行选择一个索引。
- 第二种方法是，我们可以修改语句，引导Mysql使用我们期望的sql。
- 第三个方式是，在有些场景下，我们可以新建一个更合适的索引，来提供给优化器做选择，或删掉误用的索引。

## 给字符串字段加索引
假设表中有个字段email，我们可以通过 `alter table xxx add index index_name(email(6))`,来创建字符串的前缀索引，对于该命令创建的索引里面，对于每个记录都只取前6个字节，所以占用的空间更小，这是使用前缀索引的优势。
但，这同时带来的损失是，可能会增加额外的记录扫描次数。前缀索引用不上覆盖索引，不管前缀定义为多长，都无法避免回表的操作，因为系统不确定前缀索引的定义是否截断 了完整信息。
如果要建前缀索引，我们可以通过统计索引上有多少个不同的值来判断要使用多长的前缀。
首先，可以使用下面的语句，算出列上有多少个不同的值：
```sql
select count(distinct email) as L from SUser;
```
然后，一次选取不同长度的前缀来看这个值，比如我们要看一下4~7个字节的前缀索引，可以用这个语句：
```sql
select 
	count(distinct left(email, 4)) as L4,
	count(distinct left(email, 5)) as L5,
	count(distinct left(email, 6)) as L6,
	count(distinct left(email, 7)) as L7
from SUser;
```
当然，使用前缀索引很可能会损失区分度，所以你需要预先设定一个可以接受的损失比例，比如 5%。然后，在返回的L4~L7中，找出小于 `L*95%`的值，假设这里L6、L7都满足，你就可以选择前缀长度6.
**小结：**
1. 直接创建完整索引，这样可能比较占用空间；
2. 创建前缀索引，节省空间，但会增加查询扫描次数，并且不能使用覆盖索引；
3. 倒序存储，再创建前缀索引，用于绕过字符串本身前缀的区分度不够的问题；
4. 创建hash字段索引，查询性能稳定，有额外的存储和计算消耗，跟第三种方式一样，都不支持范围扫描。


## 全局锁
Mysql提供了一个加全局读锁的方法，命令是 `Flush tables with read lock` (FTWRL).
全局锁的典型使用场景是，做全库逻辑备份。也就是把整库每个表都select出来存成文本。
Mysql官方自带的逻辑备份工具是mysqldump。当mysqldump使用参数 --single-transaction 的时候，导数据之前就会启动一个事务，来确保拿到一致性视图。而由于MVCC的支持，这个过程中数据是可以正常更新的。

## 表级锁
Mysql里面表级锁有两种：一种是表锁，一种是元数据锁 (meta data lock, MDL)。
表锁的语法: `lock tables ... read/ write`,需要注意，lock tables语法除了会限制别的线程的读写外，也限定了本线程接下来的操作对象。
栗子：
如果某个线程A中执行lock tables t1 read, t2 write; 这个语句，则其他线程写t1、读写t2的语句都会被阻塞。同时，线程A在执行unlock tables之前，也只能执行读t1、读写t2的操作。连写t1都不允许，自然也不能访问其他表。
**另一类表级锁是MDL (metadata lock)**。在Mysql5.5 版本中引入了MDL，当对一个表做增删改查操作的时候，加MDL读锁；当要对标结构做结构变更操作的时候，加MDL写锁。
alter语句在启动的时候需要获取MDL写锁，但是这个写锁在真正拷贝数据之前就退化成读锁了。
```ad-note
**Online 和 inplace**
我们重建表的这个语句 `alter table t engine=InnoDB`,其实隐含的意思是：
`alter table t engine=innodb,ALGORITHM=inplace;`
与之相对的就是拷贝表的方式了，用法是：
`alter table t engine=innodb,ALGORITHM=copy;`

`alter table t add FULLTEXT(field_name);`
这个过程是inplace的，但会阻塞增删改查，是非Online的。

如果说Online和inplace之间的关系是什么的话，可以概括为：
1.DDL过程如果是ONLINE的，就一定是inplace的；
2.反过来未必，也就是说 inplace的DDL，有可能不是Online的。截止到Mysql 8.0，添加全文索引 (FULLTEXT index) 和 空间索引 (SPATIAL index) 就属于这种情况。
```



## 行锁
在InnoDB事务中，行锁是在需要的时候才加上的，但并不是不需要了就立刻释放，而是要等到事务结束时才释放。这个就是两阶段锁协议。
```ad-note
如果你的事务中需要锁多个行，要把最可能造成锁冲突、最可能影响并发度的锁尽量往后放.
```


## 加锁规则
这个规则又以下两条前提说明：
1. Mysql 后面的版本可能会改变加锁策略，所以这个规则值限于截止到现在的最新版本，即5.x 系列 <=5.7.24，8.0 系列 <= 8.0.13。
2. 如果在验证中有发现 bad case的话，可以纠正过来。
因为间隙锁在可重复读隔离级别下才有效，所以本篇文章接下来的描述，若没有特殊说明，默认是可重复读隔离级别。
**两个 "原则"、两个 "优化" 和一个 "bug" **
1. 原则1：加锁的基本单位是next-key lock。next-key lock是前开后闭区间。
2. 原则2：查找过程中访问到的对象才会加锁。
3. 优化1：索引上的等值查询，给唯一索引加锁的时候，next-key lock退化为行锁。
4. 优化2：索引上的等值查询，向右遍历时且最后一个值不满足等值条件的时候，next-key lock退化为间隙锁。
5. 一个bug：唯一索引上的范围查询会访问到不满足条件的第一个值为止。
**案例一：非唯一索引等值锁**
```sql
CREATE TABLE `t` (
  `id` int(11) NOT NULL,
  `c` int(11) DEFAULT NULL,
  `d` int(11) DEFAULT NULL,
  PRIMARY KEY (`id`),
  KEY `c` (`c`)
) ENGINE=InnoDB;

insert into t values(0,0,0),(5,5,5),
(10,10,10),(15,15,15),(20,20,20),(25,25,25);
```
![[Pasted image 20240126170040.png]]
这里session A 要给索引 c 上 c=5 的这一行加上读锁。
1. 根据原则1，加锁单位是 next-key lock，因此会给 (0,5]加上next-key lock.
2. 要注意c是普通索引，因此仅访问 c=5 这一条记录是不能马上停下来的，需要向右遍历，查到 c=10 才放弃。根据原则2，访问到的都要加锁，因此要给 (5, 10] 加next-key lock。
3. 但是同时这个符合优化2：等值判断，向右遍历，最后一个值不满足 c=5 这个等值条件，因此退化成间隙锁 (5,10)。
4. 根据原则2，**只有访问到的对象才会加锁**, 这个查询使用覆盖索引，并不需要访问主键索引，所以主键索引上没有加任何锁，这就是为什么 session B 的update语句可以执行完成。
但 session C 要插入一个 (7,7,7) 的记录，就会被session A 的间隙锁 (5, 10) 锁住。

需要注意，在这个例子中，lock in share mode 只锁覆盖索引，但是如果是for update 就不一样了。执行 for update 时，系统会认为你接下来要更新数据，因此会顺便给主键索引上满足条件的行加上行锁。

这个例子说明，锁是加在索引上的；同时，它给我们的指导是，如果你要用 lock in share mode 来给行加读锁避免数据被更新的话，就必须得绕过覆盖索引的优化，在查询字段中加入索引中不存在的字段。

## 案例二：主键索引范围锁
对于表t，下面这两条查询语句，加锁范围相同吗？
```sql
mysql> select * from t where id=10 for update;
mysql> select * from t where id>=10 and id<11 for update;
```
第一条sql只加id=10的行锁，第二条sql只加行锁id=10和next-key lock (10, 15]。
![[Pasted image 20240126172929.png]]
分析一下session A 会加什么锁呢？
1. 开始执行的时候，要找到第一个id=10的行，因此本该是 next-key lock(5, 10]。根据优化1，主键id上的等值条件，退化成行锁，只加了id=10 这一行的行锁。
2. 范围查找就往后继续找，找到 id=15 这一行停下来，因此需要加 next-key lock(10, 15]。
所以，session A这时候锁的范围就是主键索引上，行锁 id=10 和next key lock(10,15]。这样，session B 和 session C 的结果你就能理解了。
这里你需要注意一点，首次 session A 定位查找 id =10 的行的时候，是当做等值查询来判断的，而向右扫描到 id=15的时候，用的是范围查询判断。

**案例三：一个死锁的例子**
![[Pasted image 20240126190150.png]]
其实，session B 的 "加next-key lock(5,10]" 操作，实际上分成了两步，显示加 (5,10)的间隙锁，加锁成功；然后加c=10的行锁，这时候才被锁住。
也就是说，我们在分析加锁规则的时候可以用next-key lock来分析。但是要知道，具体执行的时候，是要分成间隙锁和行锁两段来执行的。

**案例四：倒序排序，索引的扫描顺序是倒着来的**
![[Pasted image 20240127161531.png]]
看看session A 的select 语句加了哪些锁：
1. 由于是 order by c desc,第一个要定位的是索引 c 上 "最右边的" c=20的行，所以会加上间隙锁 (20,25) 和 next-key lock (15,20]。
2. 在索引c上向左遍历，要扫描到c=10才停下来，所以 next-key lock会加到 (5,10]，这正是阻塞 session B 的insert 语句的原因。
3. 在扫描过程中，c=20、c=15、c=10这三行都存在值，由于是 `select * `，所以会在主键id上加三个行锁。
因此，session A 的 select 语句锁的范围就是：
1. 索引 c 上 (5,25);
2. 主键索引上 id=15 、20 连个行锁。
这里，在啰嗦一下，每次加锁都会说明是加在 "哪个索引上"的。因为，锁就是加在索引上的，这是InnoDB的一个基础设定，需要我们在分析的时候要一直记得。

## 死锁和死锁检测
当并发系统中不同线程出现循环资源依赖，涉及的线程都在等待别的线程释放资源时，就会导致这几个线程都进入无限等待的状态，称为死锁。
当死锁出现以后，有两种策略：
- 一种策略是，直接进入等待，直到超时。这个超时时间可以通过参数 innodb_lock_wait_timeout来设置，默认是50s。
- 另一种策略是，发起死锁检测，发现死锁后，主动回滚链条中的某一个事务，让其他事务得以继续执行。将参数innodb_deadlock_detect 设置为 on，表示开启这个逻辑。
如果使用让业务等待50s是不可能忍受的，如果并发度很高，死锁检测可能会有很多额外的负担，导致CPU使用率飙升。如果团队内有人对mysql源码熟悉的，可以在Mysql里面做修改，对同一行的更新操作请求进入引擎之前排队。这样在InnoDB内部就不会有大量的死锁检测工作了。

## 事务隔离的实现
在Mysql中，实际上每条记录在更新的时候都会同时记录一条回滚操作。记录上的最新值，通过回滚操作，都可以得到前一个状态的值。
假设一个值从1被按顺序改成了 2 、3 、 4，在回滚日志里面就会有类似下面的记录。
![[Pasted image 20240116164610.png]]
当前值是4，但是在查询这条记录的时候，不同时刻启动的事务会有不同的read-view。如图中看到的，在视图A 、 B 、 B里面，这一个记录的值分别是1 、 2 、 4，同一条记录在系统中可以存在多个版本，就是数据库的多版本并发控制 (MVCC)。对于read-view A，要得到1，就必须将当前值一次执行图中所有的回滚操作得到。
回滚日志不能一直保留，系统会判断，当没有事务再需要用到这些回滚日志时，回滚日志会被删除。
尽量不要使用长事务，长事务意味着系统里面会存在很老的事务视图。由于这些事务随时可能访问数据库里面的任何数据，所以这个事务提交之前，数据里面它可能用到的回滚记录都必须保留，就会导致大量占用存储空间。
在Mysql 5.5及以前的版本，回滚日志是跟数据字典一起放在ibdata文件里的，即使长事务最终提交，回滚段被清理，文件也不会变小。为了清理回滚段，需要重建整个库。
## 事务到底是隔离的还是不隔离的？
begin/start transaction 命令并不是一个事务的起点，在执行到它们之后的第一个操作InnoDB表的语句，事务才真正启动，如果你想要的马上启动一个事务，可以使用start transaction with consistent snapshot 这个命令。

在Mysql里，有两个 "视图"的概念：
- 一个是view。它是一个用查询语句定义的虚拟表，在调用的时候执行查询语句并生成结果。创建驶入的语法是 `create view`，而它的查询方法与表一样。
- 另一个是InnoDB在实现MVCC时用到的一致性读视图，即 `consistent read view`, 用于支持RC (Read Committed, 读提交) 和 RR (Repeatable Read，可重复读) 隔离级别的实现。
## "快照" 在MVCC里是怎么工作的？
在可重复读隔离级别下，事务启动的时候就 "拍了快照"。注意，这快照是基于整库的。
InnoDB里面每个事务有一个唯一的事务ID，叫做 transaction id。它是事务开始的时候向InnoDB的事务系统申请的，是按申请顺序严格递增的。
而每行数据也都是有多个版本的。每次事务更新数据的时候，都会生成一个新的数据版本，并且把transaction id赋值给这个数据版本的事务ID，记为 row trx_id。同时，旧的数据版本要保留，并且在新的数据版本中，能够有信息可以直接拿到它。
如下图所示，就是一个记录被多个事务连续更新后的状态。
![[Pasted image 20240116172048.png]]
图中的三个虚线箭头，就是undo log；而V1、V2、V3并不是物理上真实存在的，而是每次需要的时候根据当前版本的undo log计算出来的。比如，需要V2的时候，就是通过V4依次执行U3、U2算出来。
一个事务只需要在启动的时候声明说， "以我启动的时刻为准，如果一个数据版本是在我启动之前生成的，就认；如果是我启动以后才生成，我就不认，我必须要找到它的上一个版本"。
在实现上，InnoDB为每个事务构造了一个数据，用来保存这个事务启动瞬间，当前正在 "活跃" 的所有事务ID。 "活跃"指的就是，启动了但还没提交。
数组里面事务ID的最小值记为低水位，当前系统里面已经创建过的事务ID的最大值加1记为高水位。
这个视图数组和高水位，就组成了当前事务的一致性视图 (read-view)。
而数据版本的可见性规则，就是基于数据的 row trx_id和这个一致性视图的对比结果得到的。
这个视图数组把所有的row trx_id分成了几种不同的情况。
![[Pasted image 20240116184941.png]]
这样，对于当前事务的启动瞬间来说，一个数据版本的row trx_id，有以下几种可能：
1. 如果落在绿色部分，表示这个版本是已提交的事务或者是当前事务自己生成的，这个数据是可见的；
2. 如果落在红色部分，表示这个版本是由将来启动的事务生成的，是肯定不可见的；
3. 如果落在黄色部分，那就包括两种情况
	a.若row trx_id在数组中，表示这个版本由还没提交的事务生成的，不可见；
	b. 若 row trx_id不在数组中，表示这个版本是已经提交了的事务生成的，可见。

一个数据版本，对于一个事务视图来说，除了自己的更新总是可见以外，有三种情况：
1. 版本未提交，不可见；
2. 版本已提交，但是是在视图创建后提交的，不可见；
3. 版本已提交，而且是在视图创建前提交的，可见。
以上是针对查询逻辑来说，**更新逻辑有所区别：**
**更新数据都是先读后写的，而这个读，只能读当前的值，称为 "当前读" (current read)。**
这里我们提到了一个概念，叫作当前读。其实，除了update语句外，select语句如果加锁，也是当前读。

**可重复读的核心就是一致性读 (consistent read)；而事务更新数据的时候，只能用当前读。如果当前的记录的行锁被其他事务占用的话，就需要进入锁等待。**
而读提交的逻辑和可重复读的逻辑类似，它们最主要的区别是：
- 可重复读隔离级别下，只需要在事务开始的时候创建一致性视图，之后事务里的其他查询都共用这个一致性视图；
- 在读提交隔离级别下，每个语句执行前都会重新算出一个新的视图。

## 幻读是什么，幻读有什么问题？
**RR隔离级别下，为保证binlog记录顺序，非索引更新会锁住全表记录，且事务结束前不会对不符合条件的记录有逐步释放的过程。**

**RC隔离级别下，在语句执行完成后，是只有行锁的。而且语句执行完成后，InnoDB就会把不满足条件的行行锁去掉。当然，where过滤条件中的哪一行，还是会等到commit的时候才释放。**

```sql
CREATE TABLE `t` (
  `id` int(11) NOT NULL,
  `c` int(11) DEFAULT NULL,
  `d` int(11) DEFAULT NULL,
  PRIMARY KEY (`id`),
  KEY `c` (`c`)
) ENGINE=InnoDB;

insert into t values(0,0,0),(5,5,5),
(10,10,10),(15,15,15),(20,20,20),(25,25,25);
```

```sql
begin;
select * from t where d =5 for update;
commit
```
比较好理解的是，这个语句会命中 d=5 的这一行，对应的主键id = 5,因此再select语句执行完成后，id=5这一行会加一个写锁，而且由于两阶段锁协议，这个写锁会在执行 commit 语句的时候释放。
由于字段d上没有索引，因此这条查询语句会做全表扫描。那么其他被扫到的，但是不满足条件5 行记录上，会不会被加锁呢？

