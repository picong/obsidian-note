Memcached和Redis都是采用哈希表作为key-value索引，而RocksDB则采用跳表作为内存中key-value的索引。
Redis采用一些常用的高效索引结构作为某些value类型的底层数据结构，这一技术路线为Redis实现高性能访问提供了良好的支撑。
![[Pasted image 20240913173320.png]]
- Redis主要通过网络框架进行访问，而不再是动态库了，这也使得Redis可以作为一个基础性的网络服务进行访问，扩大了Redis的应用范围。
- Redis数据模型中的value类型很丰富，因此也带来了更多的操作接口，例如面向列表的LPUSH/LPOP，面相集合的SADD/SREM等。
- Redis的持久化模块能支持两种方式：日志(AOF)和快照(RDB)，这两种持久化方式具有不同的优劣势，影响到Redis的访问性能和可靠性。
- Redis支持高可靠集群和高可扩展集群。
## 数据结构
key-value的hash结构，哈希表存在哈希冲突问题和rehash问题，可能带来操作阻塞。
![[Pasted image 20240913180619.png]]

![[Pasted image 20240913181111.png]]
如果哈希冲突越来越多，会导致哈希冲突链过长进而导致这个链上的查找耗时长，效率低下。为了解决这个问题，redis会对hash表做rehash操作，rehash就是增加现有的哈希桶数量，让主键增多的entry元素能在更多的桶之间分散保存，其实redis默认使用了两个全局哈希表：哈希表1和哈希表2。具体rehash步骤：
1. 给哈希表2分配更大的空间，例如是当前哈希表1大小的两倍；
2. 把哈希表1中的数据重新映射并拷贝到哈希表2中；
3. 释放哈希表1的空间。
但是第二步涉及大量的数据拷贝，如果一次性把哈希表1中的数据都迁移玩，会造成Redis线程阻塞，无法服务其他请求。为了避免这个问题，Redis采用了**渐进式rehash**。
简单来说就是在第二部拷贝数据时，Redis仍然正常处理客户端请求，每处理一个请求时，从哈希表1中的第一个索引位置开始，顺带着将这个索引位置上的所有entries拷贝到哈希表2中；等下一个请求时，再顺带拷贝哈希表1的下一个索引位置的entries。
### 底层数据结构
![[Pasted image 20240913182902.png]]
压缩列表和跳表的结构说明：
- **压缩列表**：压缩列表类似于一个数组，数组中的每一个元素都对应保存一个数据。和数组不同的是，压缩列表在表头有三个字段zlbytes、zltail和zllen，分别表示列表长度、列表尾的偏移量和列表中的entry个数；压缩列表在表尾还有一个zlend，表示列表结束。
![[Pasted image 20240913184154.png]]
	在压缩列表中，如果要查找定位第一个元素和最后一个元素，可以通过表头三个字段直接定位，复杂度为O(1)。而查找其他元素时，就没有这么高效了，只能逐个查找，此时的复杂度就是O(N)了。
- **跳表**：有序列表只能逐一查找元素，导致操作起来非常缓慢，于是就出现了跳表。具体来说，跳表在链表的基础上，**增加了多级索引，通过索引位置的几个跳转，实现数据的快速定位。跳表是可以实现二分查找的有序链表**:
![[Pasted image 20240913184537.png]]


![[Pasted image 20240914093808.png]]
### 不同操作的复杂度
- 单元素操作是基础：单元素操作，是指一中集合类型对单个数据实现的增删改查操作。例如，Hash类型的HGET、HSET和HDEL。
- 范围操作非常耗时：范围操作是指集合类型中的遍历操作，可以返回集合中的所欲元素。例如Hash类型的HGETALL和Set类型的SMEMBERS，或者返回一个范围内的部分数据，比如List类型的LRANGE和Zset的ZRANGE。
- 统计操作通常高效：统计操作，是指集合类型对集合中所有元素个数的记录，例如LLEN和SCARD。这类操作复杂度只有O(1)，这是因为当集合类型采用压缩列表、双向链表、整数数组这些数据结构时，这些结构中专门记录了元素的个数统计。
- 例外情况只有几个：某些数据结构的特殊记录，例如压缩列表和双向链表都会记录表头和表位的偏移量。这样依赖，对于List类型的LPOP、RPOP、LPUSH、RPUSH这四个操作来说，它们是在列表的头尾增删元素，就可以通过偏移量直接定位，所以这些头尾的操作复杂度也只有O(1)，因此因地制宜地使用List类型，例如，将它主要用于FIFO队列场景，而不是作为一个可以随机读写的集合。

## 高性能IO模型
我们通常说，Redis是单线程，主要是指Redis的网络IO和键值对读写是由一个线程来完成的，这也是Redis对外提供键值存储服务的主要流程。但Redis的其他功能，比如持久化、异步删除、集群数据同步等，其实都是由额外的线程执行的。
### 单线程Redis为什么那么快
一方面，Redis的大部分操作在内存上完成，再加上它采用了高效的数据结构，例如hash表和跳表，这是它实现高性能的一个重要原因。另一方面，就是Redis采用了多路复用机制，使其在网络IO操作中能并发处理大量的客户端请求，实现高吞吐率。
### 基于多路复用的高性能I/O模型
Linux中的IO多路复用机制是指一个线程处理多个IO流，就是我们经常听到的select/epoll机制。简单来说，在Redis只运行单线程的情况下，该机制允许内核张，同时存在多个监听套接字和已连接套接字。内核会一直监听这些套接字上的链接请求或数据请求。一旦有请求到达，就会交给Redis线程处理，通过回调Redis中注册的对应函数，这就实现了一个Redis线程处理多个IO流的效果。
![[Pasted image 20240914141926.png]]
```ad-note
潜在的性能瓶颈：
Redis单线程处理IO请求性能瓶颈主要包括2个方面： 
1. 任意一个请求在server中一旦发生耗时，都会影响整个server的性能，也就是说后面的请求都要等前面这个耗时请求处理完成，自己才能被处理到。耗时的操作包括以下几种： 
	- 操作bigkey：写入一个bigkey在分配内存时需要消耗更多的时间，同样，删除bigkey释放内存同样会产生耗时； 
	- 使用复杂度过高的命令：例如SORT/SUNION/ZUNIONSTORE，或者O(N)命令，但是N很大，例如lrange key 0 -1一次查询全量数据； 
	- 大量key集中过期：Redis的过期机制也是在主线程中执行的，大量key集中过期会导致处理一个请求时，耗时都在删除过期key，耗时变长； 
	- 淘汰策略：淘汰策略也是在主线程执行的，当内存超过Redis内存上限后，每次写入都需要淘汰一些key，也会造成耗时变长； 
	- AOF刷盘开启always机制：每次写入都需要把这个操作刷到磁盘，写磁盘的速度远比写内存慢，会拖慢Redis的性能； 
	- 主从全量同步生成RDB：虽然采用fork子进程生成数据快照，但fork这一瞬间也是会阻塞整个线程的，实例越大，阻塞时间越久； 
2. 并发量非常大时，单线程读写客户端IO数据存在性能瓶颈，虽然采用IO多路复用机制，但是读写客户端数据依旧是同步IO，只能单线程依次读取客户端的数据，无法利用到CPU多核。 
针对问题1，一方面需要业务人员去规避，一方面Redis在4.0推出了lazy-free机制，把bigkey释放内存的耗时操作放在了异步线程中执行，降低对主线程的影响。 
针对问题2，Redis在6.0推出了多线程，可以在高并发场景下利用CPU多核多线程读写客户端数据，进一步提升server性能，当然，只是针对客户端的读写是并行的，每个命令的真正操作依旧是单线程的。
```
## AOF日志
### 三种写回策略，也就是AOF配置项appendfsync的三个可选值：
- **Always**，同步写回：每个写命令执行完，立马同步地将日志写回磁盘；
- **Everysec**，每秒写回：每个写命令执行完，只是先把日志写到AOF文件的内存缓冲区，每隔一秒把缓冲区中的内容写入磁盘；
- **No**，操作系统控制的写回：每个写命令执行完，只是先把日志写到AOF文件的内存缓冲区，由操作系统决定何时将缓冲区内容写回磁盘。
![[Pasted image 20240914151413.png]]
### AOF重写机制
简单来说，AOF重写机制就是在重写时，Redis根据数据库的现状创建一个新的AOF文件，也就是说，读取数据库中的所有键值对，然后对每一个键值对用一条命令记录它的写入。（压缩命令的条数, 这样就可以大大减小AOF文件的大小)
重写过程是由后台子进程bgrewriteof来完成的，这也是为了避免阻塞主线程，导致数据库性能下降。
这个重写的过程总结为**一个拷贝，两处日志**：
- 一个拷贝是指，每次执行重写时，主线程fork出后台的bgrewriteaof子进程。此时，fork会把主线程的内存拷贝一份给bgrewriteaof子进程(这里的拷贝是指拷贝的内存的页表，实际是共享同一份内存的物理存储)，这里面就包含了数据库的最新数据。
- 两处日志是指，此时主线程未阻塞，仍然可以处理新来的操作。此时，如果有写操作，第一处日志就是指正在使用的AOF日志，Redis会把这个操作写到它的缓冲区。而第二处日志，就是指新的AOF重写日志。这个操作也会被写到重写日志的缓冲区。等到bgrewriteaof的拷贝数据的所有操作记录重写完成后，刚刚写入重写日志缓冲区中的最新操作也会被写入新的AOF文件，此时重写AOF日志中的状态就是最新的。此时，我们就可以用新的AOF文件替代旧文件了。
![[Pasted image 20240914155051.png]]
### 小问题
1. AOF 日志重写的时候，是由 bgrewriteaof 子进程来完成的，不用主线程参与，我们今天说的非阻塞也是指子进程的执行不阻塞主线程。但是，你觉得，这个重写过程有没有其他潜在的阻塞风险呢？如果有的话，会在哪里阻塞？

2. AOF 重写也有一个重写日志，为什么它不共享使用 AOF 本身的日志呢？
```ad-info
问题1，Redis采用fork子进程重写AOF文件时，潜在的阻塞风险包括：fork子进程 和 AOF重写过程中父进程产生写入的场景，下面依次介绍。 
- fork子进程，fork这个瞬间一定是会阻塞主线程的（注意，fork时并不会一次性拷贝所有内存数据给子进程，老师文章写的是拷贝所有内存数据给子进程，我个人认为是有歧义的），fork采用操作系统提供的写实复制(Copy On Write)机制，就是为了避免一次性拷贝大量内存数据给子进程造成的长时间阻塞问题，但fork子进程需要拷贝进程必要的数据结构，其中有一项就是拷贝内存页表（虚拟内存和物理内存的映射索引表），这个拷贝过程会消耗大量CPU资源，拷贝完成之前整个进程是会阻塞的，阻塞时间取决于整个实例的内存大小，实例越大，内存页表越大，fork阻塞时间越久。拷贝内存页表完成后，子进程与父进程指向相同的内存地址空间，也就是说此时虽然产生了子进程，但是并没有申请与父进程相同的内存大小。那什么时候父子进程才会真正内存分离呢？“写实复制”顾名思义，就是在写发生时，才真正拷贝内存真正的数据，这个过程中，父进程也可能会产生阻塞的风险，就是下面介绍的场景。
- fork出的子进程指向与父进程相同的内存地址空间，此时子进程就可以执行AOF重写，把内存中的所有数据写入到AOF文件中。但是此时父进程依旧是会有流量写入的，如果父进程操作的是一个已经存在的key，那么这个时候父进程就会真正拷贝这个key对应的内存数据，申请新的内存空间，这样逐渐地，父子进程内存数据开始分离，父子进程逐渐拥有各自独立的内存空间。因为内存分配是以页为单位进行分配的，默认4k，如果父进程此时操作的是一个bigkey，重新申请大块内存耗时会变长，可能会产阻塞风险。另外，如果操作系统开启了内存大页机制(Huge Page，页面大小2M)，那么父进程申请内存时阻塞的概率将会大大提高，所以在Redis机器上需要关闭Huge Page机制。Redis每次fork生成RDB或AOF重写完成后，都可以在Redis log中看到父进程重新申请了多大的内存空间。

问题2，AOF重写不复用AOF本身的日志，一个原因是父子进程写同一个文件必然会产生竞争问题，控制竞争就意味着会影响父进程的性能。二是如果AOF重写过程中失败了，那么原本的AOF文件相当于被污染了，无法做恢复使用。所以Redis AOF重写一个新文件，重写失败的话，直接删除这个文件就好了，不会对原先的AOF文件产生影响。等重写完成之后，直接替换旧文件即可。
```
## 内存快照
### 给哪些内存数据做快照
Redis的数据都在内存中，为了提供所有数据的可靠性保证，它执行的是**全量快照**，也就是内存中所有数据都记录到磁盘中。
Redis提供了两个命令来生成RDB文件，分别是save和bgsave：
- save：在主线程中执行，会导致阻塞；
- bgsave：创建一个子进程，专门用于写入RDB文件，避免主线程的阻塞，这也是Redis RDB文件生成的默认配置。
### 快照时数据能修改吗
因为bgsave是fork了一个子进程进行RDB的生成，所以这个时候Redis会借助操作系统的写时复制(Copy-On-Write，COW)，在执行快照的同时，正常处理写操作。
![[Pasted image 20240914181138.png]]
### 多久进行一次快照
虽然bgsave不足蛇主线程，但是，如果频繁地执行全量快照，也会带来两方面的开销：
- 一方面，频繁将全量数据写入磁盘，会给磁盘带来很大压力。
- 另一方面，bgsave子进程需要通过fork操作从主线程创建出来。这fork操作本身会阻塞主线程，而且主线程的内存越大，阻塞时间越长。
此时有一种方案就是，后续的快照值对修改的数据进行快照记录，这样可以避免每次全量快照的开销。但是假如我们对每一个键值对的修改，都做个记录，那么有一万个被修改的键值对，我们就需要有一万条额外的记录。而且有的时候，键值对非常小，比如只有32字节，而记录它被修改的元数据信息，可能就需要8字节，这样的话，为了"记录"修改，引入额外空间开销比较大。
**Redis 4.0提出了一个混合使用AOF日志和内存快照的方法，简单来说，内存快照以一定的频率执行，在两次快照之间，使用AOF日志记录这期间的所有命令操作。** 这样一来，快照不用很频繁地执行，这就避免了频繁fork对主线程的影响。
### 关于AOF和RDB的选择问题的三点建议：
- 数据不能丢失时，内存快照和AOF的混合使用是一个很好的选择；
- 如果允许分钟级别的数据丢失，可以只使用RDB；
- 如果只用AOF，优先使用everysec的配置选项，因为它在可靠性和性能之间取了一个平衡。
```ad-info
我曾碰到过这么一个场景：我们使用一个 2 核 CPU、4GB 内存、500GB 磁盘的云主机运行 Redis，Redis 数据库的数据量大小差不多是 2GB，我们使用了 RDB 做持久化保证。当时 Redis 的运行负载以修改操作为主，写读比例差不多在 8:2 左右，也就是说，如果有 100 个请求，80 个请求执行的是修改操作。你觉得，在这个场景下，用 RDB 做持久化有什么风险吗？你能帮着一起分析分析吗？
答：
2核CPU、4GB内存、500G磁盘，Redis实例占用2GB，写读比例为8:2，此时做RDB持久化，产生的风险主要在于 CPU资源 和 内存资源 这2方面： a、内存资源风险：Redis fork子进程做RDB持久化，由于写的比例为80%，那么在持久化过程中，“写实复制”会重新分配整个实例80%的内存副本，大约需要重新分配1.6GB内存空间，这样整个系统的内存使用接近饱和，如果此时父进程又有大量新key写入，很快机器内存就会被吃光，如果机器开启了Swap机制，那么Redis会有一部分数据被换到磁盘上，当Redis访问这部分在磁盘上的数据时，性能会急剧下降，已经达不到高性能的标准（可以理解为武功被废）。如果机器没有开启Swap，会直接触发OOM，父子进程会面临被系统kill掉的风险。 b、CPU资源风险：虽然子进程在做RDB持久化，但生成RDB快照过程会消耗大量的CPU资源，虽然Redis处理处理请求是单线程的，但Redis Server还有其他线程在后台工作，例如AOF每秒刷盘、异步关闭文件描述符这些操作。由于机器只有2核CPU，这也就意味着父进程占用了超过一半的CPU资源，此时子进程做RDB持久化，可能会产生CPU竞争，导致的结果就是父进程处理请求延迟增大，子进程生成RDB快照的时间也会变长，整个Redis Server性能下降。 c、另外，可以再延伸一下，老师的问题没有提到Redis进程是否绑定了CPU，如果绑定了CPU，那么子进程会继承父进程的CPU亲和性属性，子进程必然会与父进程争夺同一个CPU资源，整个Redis Server的性能必然会受到影响！所以如果Redis需要开启定时RDB和AOF重写，进程一定不要绑定CPU。
```
## 数据同步：主从库如何实现数据一致
Redis提供了主从库模式，以保证数据副本的一致，主从库之间采用的是读写分离的方式。
- 读操作：主库、从库都可以接收；
- 写操作：首先到主库执行，然后，主库将写操作同步给从库。
### 主从库间进行第一次同步的流程如下所示：
![[Pasted image 20240918134226.png]]
第一阶段，psync命令包含了主库的**主库的runID**和**复制进度offset**两个参数：
- runID，是每个Redis实力启动时都会自动生成的一个随机ID，用来唯一标记这个实例。当从库和主库第一次复制时，因为不知道主库的runID，所以将runID设为"?"。
- offset，此时设为-1，表示第一次复制。
FULLRESYNC响应表示第一次复制采用全量复制，也就是说，主库会把当前所有的数据都复制给从库。

第二阶段，主库会执行bgsave命令，生成RDB文件，然后将文件发给从库。从库收到RDB文件后，会先清空当前数据库，然后加载RDB文件。在主库将数据同步给从库的过程中，主库不会被阻塞，仍然可以正常接收请求。为了保证主从库的数据一致性，主库会在内存中用专门的repliation buffer，记录RDB文件生成后收到的所有写操作。

第三阶段，主库会把第二阶段执行过程中新收到的写命令，再发送给从库。具体的操作是，当主库完成RDB文件发送后，就会把此时replication buffer中的修改操作发给从库，从库再重新执行这些操作。
### 主从级联模式分担全量复制时的主库压力
对于主库来说，在第一次全量复制中，需要完成两个耗时的操作：生成RDB文件和传输RDB文件。如果从库数量很多，而且都要和主库进行全量复制的话，就会导致主库忙于fork子进程生成RDB文件，进行全量同步。fork这个操作会阻塞主线程处理正常请求，从而导致主库响应应用程序的请求速度变慢。
**可以通过"主-从-从"模式将主库生成RDB和传输RDB的压力，以级联的方式分散到从库上。**
简单来说，我们在部署主从集群的时候，可以手动选择一个从库，用于级联其他的从库。然后，我们可以再选择一些从库，在这些从库上执行如下命令，让它们和刚才所选的从库，建立起主从关系：
```shell
replicaof 所选从库的IP 6379
```

![[Pasted image 20240918140449.png]]

### 主从库间网络断了怎么办？
在Redis2.8之前，如果主从在命令传播时出现了网络闪断，那么从库就会和主库重新进行一次全量复制，开销非常大。从2.8开始，网络断开之后，主从库会采用增量复制的方式继续同步。
当主从库断连后，主库会把断连期间收到的写操作命令，写入repl_backlog_buffer这个缓冲区。repl_baklog_buffer是一个环形缓冲区，**主库会记录自己写到的位置，从库则会记录自己已经读取到的位置。** 主库的写入偏移量叫做master_repl_offset，从库的读取偏移量为slave_repl_offset。
![[Pasted image 20240918141134.png]]
主从库的连接恢复之后，从库首先给主库发送psync命令，并把自己当前的slave_repl_offset发送给主库，主库会判断自己的master_repl_offset和slave_repl_offset之间的差距。
一般情况，主库只需要将master_repl_offset和slave_repl_offset之间的命令操作同步给从库就行了。
![[Pasted image 20240918141528.png]]
因为repl_backlog_buffer是一个环形缓冲区，所以缓冲区写满后，主库会继续写入，此时，就会覆盖掉之前写入的操作。**如果从库的读取速度比较慢，就有可能导致从库还未读取的操作被主库新写的操作覆盖了，这会导致主从库间的数据不一致。**
一般我们可以通过调整**repl_backlg_size**控制环形缓冲区的大小。缓冲空间的计算公式是：缓冲空间大小 = 出库写入速度 `*` 操作大小 - 主库网路传输命令速度 `*` 操作大小。在实际应用中，考虑到可能存在一些突发的请求压力，我们统统需要把这个缓冲空间扩大一倍。
### 小结
Redis的主从同步的基本原理，总结来说，有三种模式：全量复制、基于长连接的命令传播，以及增量复制。

全量复制虽然耗时，但是对于从库来说，如果是第一次同步，全量复制是无法避免的，所以，我给你一个小建议：**一个 Redis 实例的数据库不要太大**，一个实例大小在几 GB 级别比较合适，这样可以减少 RDB 文件生成、传输和重新加载的开销。另外，为了避免多个从库同时和主库进行全量复制，给主库过大的同步压力，我们也可以采用“主 - 从 - 从”这一级联模式，来缓解主库的压力。

长连接复制是主从库正常运行后的常规同步阶段。在这个阶段中，主从库之间通过命令传播实现同步。不过，这期间如果遇到了网络断连，增量复制就派上用场了。我特别建议你留意一下 repl_backlog_size 这个配置参数。如果它配置得过小，在增量复制阶段，可能会导致从库的复制进度赶不上主库，进而导致从库重新进行全量复制。所以，通过调大这个参数，可以减少从库在网络断连时全量复制的风险。
## 哨兵机制
### 哨兵机制的基本流程
![[Pasted image 20240918174706.png]]
在监控和选主这两个任务中，哨兵需要做出两个决策：
- 在监任务中，哨兵需要判断主库是否处于下线状态；
- 在选主任务重，哨兵也要决定选择哪个从库实力作为主库。

主观下线和客观下线
哨兵进程会使用PING命令检查它自己和主、从库的网络连接情况，用来判断实例的状态。如果哨兵发现主库或从库对PING命令相应超时了，那么，哨兵就会把它标记为"主观下线"。如果检测的是从库，标记为"主观下线"就行了，如果检测的是主库，将标记为"主观下线"后，不能立马启动主从切换。因为可能存在误判的情况，一旦启动了主从切换，后续的选主和通知操作都会带来额外的计算和通信开销。
哨兵机制，通常会采用多实例组成的集群模式进行部署，这也被称为哨兵集群。引入多个哨兵实例一起来判断，就可以避免单个哨兵因自身网络状况不好，而误判主库下线的情况。
![[Pasted image 20240918175828.png]]
简单来说，"客观下线"的标准就是，当有N个哨兵实例时，最好要有N/2 + 1个实例判断主库为"主观下线"，才能最终判定主库为"客观下线"。
### 如何选定新主库？
哨兵在多个从库中，先按照**一定的筛选条件**，把不符合条件的从库去掉，然后，再按照**一定的规则**，给剩下的从库逐个打分，将得分最高的从库选为新主库。
![[Pasted image 20240918180207.png]]
筛选条件：**除了检查从库的当前在线状态，还要判断它之前的网络连接状态。** 
打分规则：**按照从库优先级、从库复制进度以及从库ID号依次打分，按照文字中提及的顺序，哪一个阶段有从库得分最高，那么就选该从库作为主库，否则进行下一轮打分。**
- 第一轮：优先级最高的从库得分高，用户可以通过slave-priority配置项给不同的从库设置不同的优先级。
- 第二轮：和旧主库同步程度最接近的从库得分高，根据从库的slave_repl_offset跟master_repl_offset差值，距离最小的当选。
- 第三轮：ID号小的从库得分高，在优先级和复制进度都相同的情况下，ID号最小的从库得分最高，会被选为新主库。
## 哨兵集群：哨兵挂了，主从库还能切换吗？
- **基于 pub/sub 机制的哨兵集群组成**，哨兵只要和主库建立起了连接，就可以在主库上发布消息了，可以发布自己的连接信息(IP和端口)。同时，哨兵也可以从主库上订阅消息，获取其他哨兵发布的连接信息。当多个哨兵实例都在主库上做了发布和订阅操作后，它们之间就能知道彼此的IP地址和端口。Redis会以频道的形式，对这些消息进行分门别类的管理。不同哨兵哨兵是通过主库上的同一个名为"`__sentinel__:hello`"的频道来进行互相轧线，实现互相通信的。然后通过这些地址，哨兵之间可以建立相互的连接，形成集群。
![[Pasted image 20240919094317.png]]
- **哨兵向主库发送INFO命令来获取从库的IP地址和端口，从而和每个从库建立连接，并对从库进行监控。**
![[Pasted image 20240919094735.png]]
- **基于 pub/sub 机制的客户端事件通知**，从本质上说，哨兵就是一个运行在特定模式下的Redis实例，虽然它不服务请求操作，但是每个哨兵实例也提供pub/sub机制，客户端可以从哨兵订阅消息。哨兵提供的消息订阅频道很多，不同频道包含了主从切换过程中的不同关键事件。以下是几个重要的频道汇总：
![[Pasted image 20240919095125.png]]
### 由哪个哨兵执行主从切换？
确定由哪个哨兵执行主从切换的过程，和主库"客观下线"的过程类似，也是一个"投票仲裁"的过程。
任何一个哨兵实例只要自身判断主库"主观下线"后，就会给其他实例发送is-master-down-by-addr命令。接着，其他实例会根据自己和主库的连接情况，做出Y或N的响应，Y赞成，N反对。
![[Pasted image 20240919095548.png]]
一个哨兵获得了仲裁所需的赞成票数后，就可以标记主库为"客观下线"。这个所需的赞成票数是通过哨兵配置文件中的quorum配置项设定的。
投票的简单规则：
1. 如果当前哨兵自己发起了投票后，别的哨兵的投票请求会被当前哨兵投为N。
2. 如果当前哨兵没有发起投票请求，第一次收到其他哨兵的投票请求会给这个哨兵投Y，后续其他哨兵的投票请求会被当前哨兵投为N。
## 切片集群
### Redis数据两增多的两种扩展方案：
- **纵向扩展**：升级单个Redis实例的资源配置，包括增加内存容量、增加磁盘容量、使用更高配置的CPU。
	- 优点：实施起来简单、直接。
	- 缺点：第一个问题是，当使用RDB对数据进行持久化时，如果数据量增加，需要的内存也会增加，主线程fork子进程时可能会阻塞。不过，如果不要求Redis持久化保存数据，那么纵向扩展会是一个不错的选择。面临的第二个问题是，纵向扩展会受到硬件和成本的限制。
- **横向扩展**：横向增加当前Redis实例的个数。
	- 横向扩展的扩展性比纵向扩展更好，想要保存更多的数据，采用这种方案的话，只需要增加Redis的实例个数就行了。
![[Pasted image 20240919140922.png]]
### 数据切片和实例的对应关系
从Redis 3.0开始，官方提供了一个名为Redis Cluster的方案，用于实现切片集群。Redis Cluster方案中规定了数据和实例的对应规则。
Redis Cluster采用哈希槽(Hash Slot)，来处理数据和实例之间的映射关系。一个切片汲取一共有16348个哈希槽，这些槽类似于数据分区，每个键值对都会根据它的key，被映射到一个哈希槽中。这个过程分为下面两大步：
```ad-note
- 首先根据键值对的key，按照CRC16算法计算一个16bit的值；然后再用这个16bit值对16384取模，得到0~16383范围内的模数，每个模数代表一个相应编号的哈希槽。
- 可以使用`cluster create`命令创建集群，那么Redis会自动把这些槽平均分布在集群实例上；也可以使用`cluster meet`命令手动建实例间的连接，形成集群，再使用`cluster addslots`命令，指定每个实例上的哈希槽个数，需要注意手动分配哈希槽时，我们需要将16384个哈希槽全部分配完成，否则集群无法启动。
![[Pasted image 20240919142454.png]]
命令类似下面：
```shell
redis-cli -h 172.16.19.3 –p 6379 cluster addslots 0,1

redis-cli -h 172.16.19.4 –p 6379 cluster addslots 2,3

redis-cli -h 172.16.19.5 –p 6379 cluster addslots 4
...
```
### 客户端如何定位数据？
客户端通过key可以计算出该key处于哪一个哈希槽。一般来说，客户端和集群实例建立连接后，实例就会把哈希槽的分配信息发给客户端。集群刚刚创建的时候，Redis实例会把自己的哈希槽信息发给和它相连接的其他实例，来完成哈希槽分配信息的扩散。当所有实例之间相互连接后，每个实例就有所有哈希槽的映射关系了。
在急群中实例和哈希槽的关系是会变化的，最常见的变化有两个：
- 在集群中，实例有新增或删除，Redis需要重新分配哈希槽；
- 为了负载均衡，Redis需要把哈希槽在所有实例上重新分布一遍。
Redis Cluster方案提供了一种**重定向机制**，客户端给一个实例发送数据读写操作时，这个实例上并没有相应的数据，客户端要再给一个新实例发送操作命令。而这个新实例的访问地址会如下通过MOVED命令响应给客户端。
```shell
GET hello:key
(error) MOVED 13320 172.16.19.5:6379
```
客户端接收到MOVED响应后会再次向响应中的地址进行操作请求，同时还会更新本地缓存，把该哈希槽跟实例的对应关系更新过来。
![[Pasted image 20240919144849.png]]

如果客户请求操作的哈希槽只迁移了一部分数据到别redis实例，而不是全部迁移过去了的话，客户端会收到一条ASK报错信息，如下：
```shell
GET hello:key
(error) ASK 13320 172.16.19.5:6379
```
这个结果中的ASK命令就表示，客户端请求的键值对所在的哈希槽13320，在172.16.19.5这个实例上，但是这个哈希槽正在迁移。此时，客户端需要先给172.16.19.5这个实例发送一个ASKING命令。这个命令的意思是，让这个实例允许执行客户端接下来的命令。然后客户端再向这个实例发送GET命令，以读取数据。
![[Pasted image 20240919145514.png]]
**ASK命令并不会更新客户端缓存的哈希槽分配信息。**
## String的数据结构类型
当你保存64位有符号整数时，String类型会把它保存为一个8字节的Long类型整数，这种保存方式通常也叫做 int 编码方式。
但是，当你保存的数据中包含字符时，String类型就会用用简单动态字符串(Simple Dynamic String，SDS)结构体来保存：
![[Pasted image 20240920150431.png]]
Redis中会用一个RedisObject结构体来统一记录一些元数据，同时指向实际数据。
![[Pasted image 20240920155458.png]]
为了节省内存空间，Redis还对Long类型整数和SDS的内存布局做了专门的设计。
当保存的是Long类型整数时，RedisObject中的指针就直接赋值为整数数据了，节省了指针的空间开销。（int编码)
当保存的是字符串数据，并且字符串小于等于44字节时，RedisObject中元数据、指针和SDS是一块连续的内存区域，这样可以避免内存碎片化。（embstr编码）
当字符串大于44字节时，SDS的数据量就开始变多了，Redis就会给SDS分配独立的空间，并用指针指向SDS结构。（raw编码）
![[Pasted image 20240920155924.png]]
Redis会使用全局哈希表保存所有键值对，哈希表的每一项是一个dicEntry的结构体。dictEntry结构体中有三个8字节的指针，分别指向key、value以及下一个dicEntry。
![[Pasted image 20240920160226.png]]
这3个指针供占用24字节，但是jemalloc在分配内存时，会根据我们申请的字节数N，如果N不是2的幂次数，就需要找一个比N大，但是最近新N的2的幂次数作为分配的空间，这样可以减少频繁分配的次数。所以，这个dicEntry会被分配为32字节。
### 用什么数据结构可以节省这种需要分配大量单key-value字符串类型的内存空间
Redis有一种底层数据结构，叫压缩列表(ziplist)，这是一种非常节省内存的结构。
![[Pasted image 20240920161614.png]]
每个entry的元数据包括下面几部分：
- **prev_len**：表示前一个entry的长度。pre_len有两种取值情况：1字节或者5字节。取值1字节时，表示上一个entry的长度小于254字节。虽然1字节的值能表示的数值范围是0~255，但是压缩列表中zlend的取值默认是255，因此就默认用255表示整个压缩列表的结束，其他表示长度的地方就不能再用255这个值了。所以，当一个entry长度小于254字节时，pre_len取值为1字节，否则，就取值为5字节。
- **len**：表示自身长度，4字节；
- **encoding**：表示编码方式，1字节；
- **content**：保存实际数据。
这些entry会挨个放置在内存中，不需要再用额外的指针进行连接，这样就可以节省指针所占用的空间。Redis基于压缩列表实现了List、Hash和Sorted Set这样的集合类型，这样做的最大好处就是可以节省dictEntry的开销。当使用String类型时，一个键值对就有一个dictEntry，要用32字节空间。
### 用Hash类型来存储
Redis有两个配置项，这个两个配置项设置了Hash类型底层使用压缩列表的是的阈值，一旦超过了阈值，Hash类型就会用哈希表来保存数据了，这两个配置分别为：
- hash-max-ziplist-entries：表示压缩列表保存时哈希结合中的最大元素个数。
- hash-max-ziplist-value：表示用压缩列表保存时哈希集合找那个单个元素的最大长度。
如果我们往Hash集合中写入的元素个数超过了hash-max-ziplist-entries，或者写入的单个元素大小超过hash-max-ziplist-value，Redis就会自动把Hash类型的实现结构由压缩列表转为哈希表。
一旦压缩列表转为了哈希表，Hash类型就会一直用哈希表进行保存了，再也不会转回城压缩列表了。
## Redis中高效的统计数据结构
![[Pasted image 20240920183856.png]]
可以看到，Set 和 Sorted Set 都支持多种聚合统计，不过，对于差集计算来说，只有 Set 支持。Bitmap 也能做多个 Bitmap 间的聚合计算，包括与、或和异或操作。

当需要进行排序统计时，List 中的元素虽然有序，但是一旦有新元素插入，原来的元素在 List 中的位置就会移动，那么，按位置读取的排序结果可能就不准确了。而 Sorted Set 本身是按照集合元素的权重排序，可以准确地按序获取结果，所以建议你优先使用它。

如果我们记录的数据只有 0 和 1 两个值的状态，Bitmap 会是一个很好的选择，这主要归功于 Bitmap 对于一个数据只用 1 个 bit 记录，可以节省内存。

对于基数统计来说，如果集合元素量达到亿级别而且不需要精确统计时，我建议你使用 HyperLogLog。
## 如何在Redis中保存时间序列数据
### 同时使用Hash和Sorted Set保存时间序列数据
Hash类型的一个特点是，可以实现对单键的快速查询(O(1)时间复杂度)。但是，**Hash类型又个短板：它并不支持对数据进行范围查找。** 
为了能同时支持按时间范围的查询，可以用Sorted Set来保存时间序列数据，因为它能够根据元素的权重分数来排序，我们可以把时间戳作为Sorted Set集合的元素分数，把时间点上记录的数据作为元素本身。使用Sorted Set保存数据后，我们就可以使用`ZRANGEBYSCORE`命令，按照输入的最大时间戳和最小时间戳来查询这个时间范围内的温度值了。
### 如何保证写入Hash和Sorted Set是一个原子性的操作呢
Redis用MULTI/EXEC命令来实现简单的事务。
- MULTI命令：表示一系列原子性操作的开始。收到这个命令后，Redis就知道，接下来再收到的命令要放入到一个内部队列中，后续一起执行，保证原子性。
- EXEC命令：表示一系列原子性操作的结束。一旦Redis收到这个命令，就表示所有要保证原子性的操作都已经发送完成了。此时，Redis开始执行刚才放到内部队列中的的所有命令操作。这些命令会被一起执行，保证了原子性。
![[Pasted image 20240923150513.png]]
但是基于Hash和Sorted Set来保存的时间序列，如果需要进行聚合操作，只能将Redis中的数据加载到应用内存中来去进行聚合操作。
### 基于RedisTimeSeries模块保存时间序列数据
RedisTimeSeries是Redis的扩展模块，在使用前，需要先把它的源码独立编译成动态链接redistimeseries.so，再使用loadmodule命令进行加载：
```shell
loadmodule redistimeseries.so
```
RedisTimeSeries的5个主要操作：
- 用TS.CREATE 命令创建时间序列数据集合；
- 用TS.ADD 命令插入数据；
- 用TS.GET命令读取最新数据；
- 用TS.MGET 命令按标签过滤查询数据集合；
- 用TS.RANGE 支持聚合计算的范围查询。
### 小结
时间序列数据的写入特点是要能快速写入，而查询的特点有三个：
- 点查询，根据一个时间戳，查询相应时间的数据；
- 范围查询，查询起止时间戳范围内的数据；
- 聚合计算，针对起始和截止时间戳范围内的所有数据进行计算，例如求最大/最小值，求平均值等。
关于快速写入，Redis的高性能写特性足以应对了；而针对多样化的查询需求，Redis提供了两种方案：
- 组合使用Redis内置的Hash和Sorted类型。这种方案可以利用Hash类型实现单键的快速查询，还能利用Sorted Set实现对范围查询的高效支持。不过这种方案有两个不足，一个是在执行聚合计算时，我们需要把数据读取到客户端再进行聚合，当有大量数据要聚合时，数据传输开销大；另一个是，内存开销大。
- 使用RedisTimeSeries模块，这是专门为存取时间序列数据而设计的扩展模块。能支持在Redis实例上进行多种数据聚合计算，避免了大数据在实例和客户端间传输。不过，RedisTimeSeries的底层数据结构使用了链表，它的范围查询的复杂度是O(N)级别的，同时，他的TS.GET只能回最新的数据。
如果你的部署环境中网络带宽高、Redis实例内存大，可以优先考虑第一种方案；否则优先考虑第二种方案。
## Redis有哪些承担消息队列职责的方案
### 分布式系统组件使用消息队列时的三大需求：
消息保序、重复消息处理和消息可靠性保证，这三大需求可以进一步转换为对消息队列的三大要求：消息数据有序存取，消息数据具有全局唯一编号，以及消息数据在消费完成后被删除。
![[Pasted image 20240927154334.png]]
### 基于List的消息队列解决方案：
```shell
LPUSH mq "101030001:stock5" # List往一个方向push能保证有序，在value中拼接一个全局唯一的id，然后让消费端保证幂等以确保重复消息对业务无影响
BRPOPLPUSH mq mqback 5000 # 从list找那个弹出最右边的元素，同时将该元素push到另外的list中，保障消息的可靠性，如果消费者处理中宕机下次想重新获取消息可以去mqback中获取，可以在最后面加一个超时时间，阻塞pop，防止在mq中没有数据时，业务线程一直重复调POP命令消耗cpu
```
### 基于Streams的消息队列解决方案
- XADD：插入消息，保证有序，可以自动生成全局唯一ID；
- XREAD：用于读取消息，可以按ID读取数据；
- XREADGROUP：按消费组形式读取消息；
- XPENDING：可以用来查询每个消费组内所有消费者已读取但尚未确认的消息
- XACK：用于向消息队列确认消息处理已完成。
```shell
XADD mqstream `*` repo 5 #消息队列名称mqstream后面的`*`，表示让Redis为插入的数据自动生成一个全局唯一的ID
"1599203861727-0"

XREAD BLOCK 100 STREAMS mqstream 1599203861727-0 # 读取从ID号1599203861727-0之后的所有消息

XREAD BLOCK 10000 streams mqstream $ # 表示读取最新的的消息，BLOCK 10000表示当前命令执行后等待10s，如果队列mq中一直没有最新的消息，xread在10s后返回空值(nil)

XGROUP create mqstream group1 0 # 为队列mqstream创建一个名为group1的消费组
XREADGROUP group group1 conusumer1 streams mqstream > # 让group1消费组里的消费者consumer1从mqstream中读取所有消息，其中，命令最后的参数">"，表示从第一条尚未被消费的消息开始读取。
XPENDING mqstream group1 # 查看group2中各个消费者已读取，单尚未确认的消息个数。
XPENDING mqstream group1 - + 10 consumer2 # 查看某个消费者具体读取了哪些数据
XACK mqstream group1 1599274912765-0 # 一旦1599274912765-0被消费者处理了，就可以使用XACK命令通知Sreams，然后这条消息就会从消费者的pending列表中被删除，通过XPENDING命令查不到该记录了
```
## 如何避免单线程的阻塞模型
### 不同交互产生的不同操作：
- **客户端**：网络IO，键值对增删改查操作，数据库操作；
- **磁盘**：生成RDB快照，记录AOF日志，AOF日志重写；
- **主从节点**：主库生成、传输RDB文件，从库接收RDB文件、清空数据库、加载RDB文件；
- **切片集群实例**：向其他实例传输哈希槽信息，数据迁移。
![[Pasted image 20240927180540.png]]
### 阻塞点：
- **集合全量查询和聚合操作。**
- **bigkey删除操作**。（可异步)
- **清空数据库。** (可异步)
- **AOF日志同步写。** (可异步)
- **从库加载RDB文件。**
### 异步的子线程机制
Redis主线程启动后，会使用操作系统提供的pthread_create函数创建3个子线程，分别由它们负责AOF日志写操作、键值对删除以及文件关闭的异步执行。
AOF写入策略配置成everysec选项后，主线程会把AOF写日志操作封装成一个任务，也放入到任务队列中。后台子线程读取任务后，开始自行写入AOF日志，这样主线程不用一直等待AOF日志写完了。
![[Pasted image 20240927181745.png]]
异步键值对删除和数据库清空操作是Redis 4.0后提供的功能，Redis也提供了新的命令来执行这两个操作:
- 键值对删除：当你的集合类型中有大量元素(例如百万级别或者千万级别元素)需要删除时，建议使用UNLINK命令(异步删除)。
- 清空数据库：可以在FLUSHDB和FLUSHALL命令后加上ASYNC选项，这样可以让后台子线程异步地清空数据库：
```shell
FLUSHDB ASYNC
FLUSHALL ASYNC
```
### 小结
异步删除操作是Redis4.0以后才有的功能，如果使用的是4.0之前的版本，当遇到bigkey删除时，可以先试用集合类型提供的SCAN命令读取一部分数据然后再进行删除，这样可以避免一次性删除大量key给主线程带来阻塞。例如hash类型的bigkey删除，可以使用hscan命令。

集合全量查询和聚合操作、从库加载RDB文件在关键路径上，无法使用异步操作来完成，建议如下：
- 集合全量查询和聚合操作：可以使用SCAN命令，分批读取数据，再在客户端进行聚合计算；
- 从库加载RDB文件：把主库的数据量大小控制在2~4GB左右，以保证RDB文件能以较快的速度加载。
```ad-note
1、lazy-free是4.0新增的功能，但是默认是关闭的，需要手动开启。 
2、手动开启lazy-free时，有4个选项可以控制，分别对应不同场景下，要不要开启异步释放内存机制： 
a) lazyfree-lazy-expire：key在过期删除时尝试异步释放内存 
b) lazyfree-lazy-eviction：内存达到maxmemory并设置了淘汰策略时尝试异步释放内存 
c) lazyfree-lazy-server-del：执行RENAME/MOVE等命令或需要覆盖一个key时，删除旧key尝试异步释放内存 
d) replica-lazy-flush：主从全量同步，从库清空数据库时异步释放内存 
3、即使开启了lazy-free，如果直接使用DEL命令还是会同步删除key，只有使用UNLINK命令才会可能异步删除key。 
4、这也是最关键的一点，上面提到开启lazy-free的场景，除了replica-lazy-flush之外，其他情况都只是*可能*去异步释放key的内存，并不是每次必定异步释放内存的。 开启lazy-free后，Redis在释放一个key的内存时，首先会评估代价，如果释放内存的代价很小，那么就直接在主线程中操作了，没必要放到异步线程中执行（不同线程传递数据也会有性能消耗）。 什么情况才会真正异步释放内存？这和key的类型、编码方式、元素数量都有关系（详细可参考源码中的lazyfreeGetFreeEffort函数）： 
a) 当Hash/Set底层采用哈希表存储（非ziplist/int编码存储）时，并且元素数量超过64个 
b) 当ZSet底层采用跳表存储（非ziplist编码存储）时，并且元素数量超过64个 
c) 当List链表节点数量超过64个（注意，不是元素数量，而是链表节点的数量，List的实现是在每个节点包含了若干个元素的数据，这些元素采用ziplist存储） 只有以上这些情况，在删除key释放内存时，才会真正放到异步线程中执行，其他情况一律还是在主线程操作。 也就是说String（不管内存占用多大）、List（少量元素）、Set（int编码存储）、Hash/ZSet（ziplist编码存储）这些情况下的key在释放内存时，依旧在主线程中操作。 可见，即使开启了lazy-free，String类型的bigkey，在删除时依旧有阻塞主线程的风险。所以，即便Redis提供了lazy-free，我建议还是尽量不要在Redis中存储bigkey。 个人理解Redis在设计评估释放内存的代价时，不是看key的内存占用有多少，而是关注释放内存时的工作量有多大。从上面分析基本能看出，如果需要释放的内存是连续的，Redis作者认为释放内存的代价比较低，就放在主线程做。如果释放的内存不连续（大量指针类型的数据），这个代价就比较高，所以才会放在异步线程中去执行。
```
## CPU架构对Redis性能的影响

### CPU架构对应用程序运行的影响
- L1、L2缓存中的指令和数据的访问速度很快，所以，充分利用L1、L2缓存，可以有效缩短应用程序的执行时间；
![[Pasted image 20240929113636.png|650"物理核核逻辑核的关系"]]
- 在NUMA架构下，如果应用程序从一个Socket上调度到另一个Socket上，就可能会出现远端内存访问的情况，这会直接增加应用程序的执行时间。
![[Pasted image 20240929112502.png|650"NUMA架构"]]
### CPU多核对Redis性能的影响
Redis运行时发生context switch后，主线程的运行信息需要被重新加载到另一个CPU核上，而且，此时，另一个CPU核上的L1、L2缓存中，并没有Redis实例之前运行频繁访问的指令和数据，所以，这些指令和数据都需要重新从L3缓存，甚至是内存中加载。Redis实例需要等待这个重新加载的过程完成后，才能开始处理请求，会导致一些请求的处理时间增加。
如果在CPU多核场景下，Redis实例被频繁调度到不同CPU核上运行的话，那么对Redis实例的请求处理时间影响就比较大。**每调度一次，一些请求就会受到运行时信息、指令和数据重新加载过程的影响，这就会导致某些请求的延迟明显高于其他请求。** 
为了避免Redis总是在不同的CPU核上来回调度执行，可以使用`taskset`命令把Redis程序绑定在一个核上运行。
比如说，下面的命令，就把Redis实例绑在了0号核上，其中，"-c"选项用于设置要绑定的核编号。
```shell
taskset -c 0 ./redis-server
```
### CPU的NUMA架构对Redis性能的影响
Redis实例和网络中断程序的数据交互过程：网络中断处理程序从网卡硬件中读取数据，并把数据写入到操作系统内核维护的一块内存缓冲区。内核会通过epoll机制触发事件，通知Redis实例，Redis实例再把数据从内核的内存缓冲区拷贝到自己的内存空间。
![[Pasted image 20240929134554.png]]
在CPU的NUMA架构下，当网络中断处理程序、Redis实例分别和CPU核绑定后，就会有一个潜在的风险：**如果网络中断处理程序和Redis实例各自所绑的CPU核不在同一个CPU Socket上，那么，Redis实例读取网络数据时，就需要跨CPU Socket访问内存，这个过程会话费较多时间。**
![[Pasted image 20240929134807.png]]
所以，为了避免Redis跨CPU Socket访问网络数据，我们最好把网络中断程序和Redis实例绑在一个CPU Socket上，从而让Redis实例可以直接从本地内存读取网络数据了。
![[Pasted image 20240929134935.png]]
但是，**在CPU的NUMA架构下，对CPU核的编号规则，并不是先把一个CPU Socket中的所有逻辑核编完，再对下一个CPU Socket中逻辑核编码，而是先给每个CPU Socket中每个物理核的第一个逻辑核一次编号，再给每个CPU Socket中的物理核的第二个逻辑核依次编号。**
```shell
lscpu

Architecture: x86_64
...
NUMA node0 CPU(s): 0-5,12-17
NUMA node1 CPU(s): 6-11,18-23
...
```
### 绑核的风险和解决方案
- 风险：当我们把Redis实例帮到一个CPU逻辑核上时，就会导致子进程、后台线程和Redis主线程竞争CPU资源，一旦子进程或后台线程占用CPU时，主线程就会被阻塞，导致Redis请求延迟增加。
- 两种解决方案
	- **方案一：一个Redis实例对应绑一个物理核**，以上面的两核NUMA架构为例，NUMA node0的CPU编号是0-5、12-17，其中编号0和12、1和13 、2和14等都表示一个物理核的2个逻辑核。所以绑核时，我们使用属于同一个物理核的2个逻辑核进行绑核操作：`taskset -c 0,12 ./redis-server`
	- **方案二：优化Redis源码**，这个方案就是通过修改Redis源码，把子进程和后台线程绑定到不同的CPU核上。如下所示为绑核的流程
		- 第一步：创建一个cpu_set_t结构的位图变量：
		- 第二步：使用CPU_ZERO函数，把cpu_set_t结构的位图所有的位都设置为0；
		- 第三步：根据要绑定的逻辑核编号，使用CPU_SET函数，把cpu_set_t结构的位图相应设置为1；
		- 第四步：使用sched_setaffinity函数，把程序绑定在cpu_set_t结构位图中为1的逻辑核上。
```c
// 现成绑核操作
//线程函数
void worker(int bind_cpu){
cpu_set_t cpuset; //创建位图变量
CPU_ZERO(&cpu_set); //位图变量所有位设置0
CPU_SET(bind_cpu, &cpuset); //根据输入的bind_cpu编号，把位图对应为设置为1
sched_setaffinity(0, sizeof(cpuset), &cpuset); //把程序绑定在cpu_set_t结构位图中为1的逻辑核
//实际线程函数工作
}

int main(){
pthread_t pthread1
//把创建的pthread1绑在编号为3的逻辑核上
pthread_create(&pthread1, NULL, (void *)worker, 3);
}
```

```c
//子进程绑核
int main(){
//用fork创建一个子进程
pid_t p = fork();
if(p < 0){
printf(" fork error\n");
}
//子进程代码部分
else if(!p){
cpu_set_t cpuset; //创建位图变量
CPU_ZERO(&cpu_set); //位图变量所有位设置0
CPU_SET(3, &cpuset); //把位图的第3位设置为1
sched_setaffinity(0, sizeof(cpuset), &cpuset); //把程序绑定在3号逻辑核
//实际子进程工作
exit(0);
}
...
}
```
对于Redis来说，生成RDB和AOF日志重写的子进程分别是下面两个文件的函数中实现的。
- rdb.c文件：rdbSaveBackgroud函数；
- aof.c文件：rewriteAppendOnlyFileBackgroud函数。
这两个函数中都调用了fork创建子进程，所以，我们可以在子进程代码部分加上绑核的四步操作。**不过，Redis6.0出来后，可以支持CPU核绑定的配置操作了。** 使用源码优化方案，我们既可以实现Redis实例绑核，避免切换核带来的性能影响，还可以让子进程、后台线程和主线程不在同一个核上运行，避免了它们之间的CPU资源竞争。相比使用taskset绑核来说，这个方案可以进一步降低绑核的风险。
## Redis变慢的应对方法
### 查看Redis的相应延迟
通过查看响应延迟，突起的一般是响应变慢。
基于当前环境下的Redis基线性能做判断。所谓基线性能，就是一个系统在低压力、无干扰下的基本性能，这个性能只由当前的软硬件配置决定。
通过--intrinsic-latency选项的参数来指定多长时间内检测到的最大延迟，如下命令是检测120秒内最大的延迟是119微秒，也就是基线性能为119微秒。一般运行120秒就足够检测到最大延迟了。
```shell
./redis-cli --intrinsic-latency 120
Max latency so far: 17 microseconds.
Max latency so far: 44 microseconds.
Max latency so far: 94 microseconds.
Max latency so far: 110 microseconds.
Max latency so far: 119 microseconds.

36481658 total runs (avg latency: 3.2893 microseconds / 3289.32 nanoseconds per run).
Worst run took 36x longer than the average latency.
```
一般来说，你要把运行时延迟和基线性能进行对比，如果观察到的Redis运行时延迟是其基线性能的2倍及以上，就可以认定Redis变慢了。
### 如何应对Redis变慢？
如下图所示，红色模块，也就是Redis自身的操作特性、文件系统和操作系统，它们是影响Redis性能的三大要素。
![[Pasted image 20240929160409.png]]
### Redis自身操作特性的影响
**1. 慢查询命令**
我们必须要知道Redis的不同命令的复杂度，可以通过查询Redis官方文档找那个对每个命令的复杂度的介绍。
当发现Redis性能变慢时，可以通过Redis日志，或者是latency monitor工具，查询变慢的请求，根据请求对应的具体命令以及官方文档，确认下是否采用了高复杂度的慢查询。
如果的确有大量的慢查询命令，有两种处理方式：
1. **用其他高效命令代替。** 比如说，如果你需要返回一个SET中的所有成员时，不要使用SMEMBERS命令，而是使用SSCAN多次迭代返回，避免一次返回大量数据，造成线程阻塞。
2. **当需要执行排序、交集、并集操作时，可以在客户端完成，而不要用SORT、SUNION、SINTER这些命令，以免拖慢Redis实例。**
### 过期key操作
Redis键值对的key可以设置过期时间。默认情况下，Redis每100毫秒会删除一些过期key，具体的算法如下：
1. 采用ACTIVE_EXPIRE_CYCLE_LOOKUPS_PER_LOOP个数的key，并将其中过期的key全部删除。
2. 如果超过25%的key过期了，则重复删除的过程，知道过期的key的比例降至25%。
ACTIVE_EXPIRE_CYCLE_LOOKUPS_PER_LOOP是Redis的一个参数，默认是20，那么一秒内基本有200个过期key会被删除。这一策略对清除过期key、释放内存空间很有帮助。如果每秒钟删除200个过期key，并不会对Redis造成太大影响。
但是，如果触发了上面算法的第2条，Redis就会一直删除以释放内存空间。注意，**删除操作时阻塞的**（Redis 4.0后可以用异步线程机制来减少阻塞影响)。
算法第二条触发的重要来源是，**频繁使用带有相同时间参数的EXPIREAT命令设置过期key**、**使用expire命令给批量的key设置相同的过期秒数**，从而导致，在同一秒内有大量的key同时过期。
解决方案：
根据实际业务的使用需求，决定EXPIREAT和EPIRE的过期时间参数。可以在过期时间参数上，加上一个一定大小范围内的随机数，这样，既可以减少同时过期造成的压力，又保证了key在一个邻近时间范围内被删除。
```ad-note
在 Redis 中，还有哪些其他命令可以代替 KEYS 命令，实现同样的功能呢？这些命令的复杂度会导致 Redis 变慢吗？ 
如果想要获取整个实例的所有key，建议使用SCAN命令代替。客户端通过执行`SCAN $cursor COUNT $count`可以得到一批key以及下一个游标`$cursor`，然后把这个`$cursor`当作SCAN的参数，再次执行，以此往复，直到返回的`$cursor`为0时，就把整个实例中的所有key遍历出来了。 关于SCAN讨论最多的问题就是，Redis在做Rehash时，会不会漏key或返回重复的key。 在使用SCAN命令时，不会漏key，但可能会得到重复的key，这主要和Redis的Rehash机制有关。Redis的所有key存在一个全局的哈希表中，如果存入的key慢慢变多，在达到一定阈值后，为了避免哈希冲突导致查询效率降低，这个哈希表会进行扩容。与之对应的，key数量逐渐变少时，这个哈希表会缩容以节省空间。 
1、为什么不会漏key？Redis在SCAN遍历全局哈希表时，采用*高位进位法*的方式遍历哈希桶（可网上查询图例，一看就明白），当哈希表扩容后，通过这种算法遍历，旧哈希表中的数据映射到新哈希表，依旧会保留原来的先后顺序，这样就可以保证遍历时不会遗漏也不会重复。 
2、为什么SCAN会得到重复的key？这个情况主要发生在哈希表缩容。已经遍历过的哈希桶在缩容时，会映射到新哈希表没有遍历到的位置，所以继续遍历就会对同一个key返回多次。 SCAN是遍历整个实例的所有key，另外Redis针对Hash/Set/Sorted Set也提供了HSCAN/SSCAN/ZSCAN命令，用于遍历一个key中的所有元素，建议在获取一个bigkey的所有数据时使用，避免发生阻塞风险。 但是使用HSCAN/SSCAN/ZSCAN命令，返回的元素数量与执行SCAN逻辑可能不同。执行`SCAN $cursor COUNT $count`时一次最多返回count个数的key，数量不会超过count。 但Hash/Set/Sorted Set元素数量比较少时，底层会采用intset/ziplist方式存储，如果以这种方式存储，在执行HSCAN/SSCAN/ZSCAN命令时，会无视count参数，直接把所有元素一次性返回，也就是说，得到的元素数量是会大于count参数的。当底层转为哈希表或跳表存储时，才会真正使用发count参数，最多返回count个元素。

补充：在redis-cluster中，不能使用一次scan在整个集群中获取所有的key，只能通过在每个实例上单独执行scan才可以，再到客户端进行合并
```
### 文件系统和操作系统对Redis性能的影响及解决方案
**1. 文件系统：AOF模式**
AOF日志提供了三种日志写回策略，如下图所示：
![[Pasted image 20240929175754.png]]
当写回策略配置为everysec时，Redis会使用后台的子线程异步完成fsync的操作。而always策略是直接使用主线程写回磁盘。

为了避免日志文件不断增大，Redis会执行AOF重写。但是日志重写存在一个潜在的风险点：AOF会对磁盘进行大量IO操作，同时，fsync有需要等到数据写到磁盘后才能返回，所以，当AOF压力比较大时，就会导致fsync被阻塞。虽然fsync是又后台子线程负责执行的，但是，主线程会监控fsync的执行进度。

当主线程使用后台子线程执行了一次fsync，需要再次把新接收的操作记录写回磁盘时，如果主线程发现上一次的fsync还没有执行完，那么它就会阻塞。
![[Pasted image 20240929180418.png]]
如果业务应用对延迟非常敏感，但同时允许一定量的数据丢失，那么可以进行如下配置的修改：
```properties
no-appendfsync-on-rewrite yes
```
这个配置设置为yes时，表示在AOF重写时，不进行fsync操作。也就是说，Redis实例把命令写到内存后，不调用后台线程进行fsync操作，就可以直接返回了。
如果的确需要高性能，同时也需要高可靠数据保证，建议考虑采用高速的固态硬盘作为AOF日志的写入设备。

**2.操作系统：swap**
内存swap是操作系统里将内存数据在内存和磁盘间来回换入和换出的机制，涉及到磁盘的读写，所以，一旦触发swap，无论被换入数据的进程，还是被换出数据的进程，其性能都会受到慢速磁盘读写的影响。
通常，触发swap的原因主要是物理机器内存不足，对于Redis而言，有两种常见的情况：
- Redis实例自身使用了大量的内存，导致物理机器的可用内存不足；
- 和Redis实例在同一台机器上运行的其他进程，在进行大量的文件读写操作。文件读写本身会占用系统内存，这会导致分配给Redis实例的内存量变少，进而触发Redis发生swap。
针对这个问题的一个解决思路：**增加机器内存或者使用Redis集群。**
```shell
redis-cli info | grep process_id # 查看Redis进程号
process_id: 5332

cd /proc/5332
cat smaps | egrep '^(Swap|Size)' #查看Redis进程的使用情况
Size: 584 kB
Swap: 0 kB
Size: 4 kB
Swap: 4 kB
Size: 4 kB
Swap: 0 kB
Size: 462044 kB
Swap: 462008 kB
Size: 21392 kB
Swap: 0 kB
```
每一行Size表示的是Redis实例所用的一块内存大小，而Size下方的Swap和它相对应，表示这块Size大小的内存区域有多少一斤被换出到磁盘上了。

**3. 操作系统：内存大页(Transparent Huge Page)**
Linux内核从2.6.38开始支持内存大页机制，该机制支持2MB大小的内存页分配，而常规的内存页分配是按4KB的粒度来执行的。
这是一种取舍问题，虽然内存大页可以给Redis带来内存分配方面的收益，但是，Redis为了提供数据可靠性保证，需要将数据做持久化保存。利用写时复制来处理后续新来的对数据修改的请求，也就是这时会将这些需要修改的数据先拷贝一份，然后再进行修改。采用内存大页，即使客户端请求只修改100B的数据，Redis也需要拷贝2MB的大页。相反，如果是常规的内存页机制，只用拷贝4KB。
这种情况的解决方案是：关闭内存大页。
```shell 
cat /sys/kernel/mm/transparent_hugepage/enabled #返回always表示开启了内存大页
echo never > /sys/kernel/mm/transparent_hugepage/enabled #关闭内存大页
```

```ad-note
### Checklsit
1. 获取Redis实例在当前环境下的基线性能。
2. 是否用了慢查询命令？如果是的话，就是用其他命令替代慢查询命令，或者把聚合计算命令放在客户端做。
3. 是否对过期key设置了相同的过期时间？对于批量删除的key，可以在每个key的过期时间上加一个随机数，避免同时删除。
4. 是否存在bigkey？对于bigkey的删除操作，如果你的Redis是4.0及以上的版本，可以直接利用异步线程机制减少主线程阻塞；如果是Redis4.0以前的版本，可以使用SCAN命令迭代删除；对于bigkey的集合查询和聚合操作，可以使用SCAN命令在客户端完成。
5. Redis AOF配置级别是什么？业务层面是否的确需要这一可靠级别？如果我们需要高性能，同时也允许数据丢失，可以将配置项no-appendfsync-on-rewrite设置为yes，避免AOF重写和fsync竞争磁盘IO资源，导致Redis延迟增加。当然，如果既需要高性能有需要高可靠性，最好使用高速固态盘作为AOF日志的写入盘。
6. Redis 实例的内存使用是否过大？发生 swap 了吗？如果是的话，就增加机器内存，或者是使用 Redis 集群，分摊单机 Redis 的键值对数量和内存压力。同时，要避免出现 Redis 和其他内存需求大的应用共享机器的情况。
7. 在 Redis 实例的运行环境中，是否启用了透明大页机制？如果是的话，直接关闭内存大页机制就行了。
8. 是否运行了 Redis 主从集群？如果是的话，把主库实例的数据量大小控制在 2~4GB，以免主从复制时，从库因加载大的 RDB 文件而阻塞。
9. 是否使用了多核 CPU 或 NUMA 架构的机器运行 Redis 实例？使用多核 CPU 时，可以给 Redis 实例绑定物理核；使用 NUMA 架构时，注意把 Redis 实例和网络中断处理程序运行在同一个 CPU Socket 上。

给redis一个安静的环境，给Redis充足的计算、内存和IO资源。
```

## 删除数据后，内存占用率还是很高的原因及处理
### 内存碎片的成因
1. **内因：内存分配器的分配策略**，Redis默认使用jemalloc进行内存分配，而jemalloc的分配策略之久，就是一系列固定的大小划分内存空间，例如8字节、16字节、32字节、48字节，...，2KB、4KB、8KB等。当程序申请的内存最接近某个固定值时，jemalloc会给它分配相应大小的空间。
2. **外因：键值对大小不一样和删改操作**，Redis的负载属于外因，包括了大小不一的键值对和键值对修改删除带来的内存空间变化。
### 如何判断是否有内存碎片化？
```shell
INFO memory
# Memory
used_memory:1073741736
used_memory_human:1024.00M
used_memory_rss:1997159792
used_memory_rss_human:1.
…
mem_fragmentation_ratio:1.86
```
这里有一个mem_fragmentation_ratio的指标，它表示的就是Redis当前的内存碎片率。mem_fragmentation_ration的计算公式如下：
```shell
mem_fragmentation_ratio = used_memory_rss/ used_memory
```
used_memory_rss是操作系统实际分配给Redis的物理内存空间，里面就包含了碎片；而used_memory是Redis为了保存数据实际申请使用的空间。
经验阈值：
- **mem_fragmentation_ratio大于1但小于1.5**。这种情况是合理的，内存分配器是一定要使用的，分配策略都是通用的，不会轻易修改；而外因由Redis负载决定，也无法限制。
- **mem_fragmentation_ratio大于1.5**.这表明内存碎片率已经超过50%。这时候，需要采取一些措施来降低内存碎片率。
### 如何清理内存碎片
一个"简单粗暴"的方法就是**重启Redis实例**。
从Redis 4.0-RC3版本以后，Redis自身提供了一种内存碎片自动清理的方法。
需要注意的是：**碎片清理是有代价的**，操作系统要把多份数据拷贝到新为止，把原有空间释放出来，这会带来时间开销。因为Redis是单线程，在数据拷贝时，Redis只能等待。而且数据拷贝还需要注意顺序，这会进一步增加Redis的等待时间，导致性能降低。
```shell
config set activedefrag yes #开始自动内存碎片清理
```
只有当下面两个条件同时满足时，才开始清理，在清理过程中，只要有一个条件不满足，就停止自动清理。
- **active-defrag-ignore-bytes 100mb**：表示内存碎片的字节数达到100 MB时，开始清理；
- **active-defrag-ignore-lower 10**：表示内存碎片空间占操作系统分配给Redis的总空间比例达到10%时，开始清理。
为了避免降低Redis性能，可以对下面两个参数进行调试：
- **active-defrag-cycle-min 25**：表示自动清理过程所用CPU时间的比例不低于25%，保证清理能正常开展；
- **active-defrag-cycle-max 75**：表示自动清理过程所用CPU时间的比例不高于75%，一旦超过，就停止清理，从而避免在清理时，大量的内存拷贝阻塞Redis，导致响应延迟升高。
## 缓冲区
### 客户端输入和输出缓冲区
![[Pasted image 20241008110159.png]]
### 如何应对输入缓冲区溢出？
导致溢出的两个主要原因：
- 写入了bigkey
- 服务端处理请求的速度过慢
使用`CLIENT LIST`命令查看每个客户端对输入缓冲区的使用情况：
```shell
id=5 addr=127.0.0.1:50487 fd=9 name= age=4 idle=0 flags=N db=0 sub=0 psub=0 multi=-1 qbuf=26 qbuf-free=32742 obl=0 oll=0 omem=0 events=r cmd=client
```
与输入缓冲区相关的三个参数：
- cmd，表示客户端最新执行的命令。
- qbuf，表示输入缓冲区已经使用的大小。
- qbuf-free，表示输入缓冲区尚未使用的大小。
Redis的客户端输入缓冲区大小的上限阈值，在代码中就设定了1GB。没有办法通过参数进行设置。
如果要避免输入缓冲区溢出，可以通过避免客户端写入bigkey，以及避免Redis主线程阻塞。
### 如何应对输出缓冲区溢出？
造成输出缓存区溢出的原因：
- 服务端返回bigkey的大量结果；(无法避免)
- 执行了MONITOR命令；(线上禁用该命令)
- 缓冲区大小设置得不合理。
通过`client-output-buffer-limit`参数来设置输出缓冲区的大小：
```shell
# normal表示普通客户端，一般普通客户端不需要进行限制所以都是0
client-output-buffer-limit normal 0 0 0
```

```shell
# pubsub参数表示当前是对订阅客户端进行设置；8mb表示输出缓冲区的大小上限为8mb,一旦占用的缓冲区大小要超过8mb，服务器端就会直接关闭客户端的连接；2mb和60表示，如果连续60秒内对输出缓冲区的写入量超过2mb的话，服务端也会关闭客户端连接
client-output-buffer-limit pubsub 8mb 2mb 60
```
### 主从集群中的和缓冲区
1. 复制缓冲区的溢出问题
![[Pasted image 20241008112152.png]]
如何避免复制缓冲区发生溢出？
一方面，我们可以控制主节点保存的数据量大小。按通常的使用经验，我们会把主节点的数据量控制在2~4GB，这样可以让全量同步执行得更快些，避免复制缓冲区积累过多命令。
另一方面，我们可以使用client-output-buffer-limit配置项，来设置合理的复制缓冲区大小。
```shell
config set client-output-buffer-limit slave 512mb 128mb 60
```
我们再继续看看这个设置对我们有啥用。假设一条写命令数据是 1KB，那么，复制缓冲区可以累积 512K 条（512MB/1KB = 512K）写命令。同时，主节点在全量复制期间，可以承受的写命令速率上限是 2000 条 /s（128MB/1KB/60 约等于 2000）。
### 复制积压缓冲区的溢出问题
![[Pasted image 20241008112716.png]]复制积压缓冲区(repl_backlog_buffer)是一个大小有限的环形缓冲区。当主节点把复制积压缓冲区写满后，会覆盖缓冲区中的旧命令数据。如果从节点还没有同步这些旧命令数据，就会造成主从节点重新开始全量复制。
**可以使用rdb-tools来对redis的内存占用情况进行分析，不影响redis的运行性能。**
## Redis作为旁路缓存
### 缓存的类型
按照Redis缓存是否接受写请求，我们可以把它分成只读缓存和读写缓存。
- **只读缓存**
![[Pasted image 20241008161933.png]]
- **读写缓存**：又存在异步写回策略和同步写回策略
![[Pasted image 20241008162054.png]]

关于是选择只读缓存，还是选择读写缓存，主要看我们对写请求是否有加速的需求。
- 如果需要对写请求加速，我们选择读写缓存；
- 如果写请求很少，或者是只需要提升读请求的响应速度的话，我们选择只读缓存。
```ad-note
Redis只读缓存和使用直写策略的读写缓存，这两种缓存都会把数据同步写到后端数据库中，它们的区别在于： 
1、使用只读缓存时，是先把修改写到后端数据库中，再把缓存中的数据删除。当下次访问这个数据时，会以后端数据库中的值为准，重新加载到缓存中。这样做的优点是，数据库和缓存可以保证完全一致，并且缓存中永远保留的是经常访问的热点数据。缺点是每次修改操作都会把缓存中的数据删除，之后访问时都会先触发一次缓存缺失，然后从后端数据库加载数据到缓存中，这个过程访问延迟会变大。 
2、使用读写缓存时，是同时修改数据库和缓存中的值。这样做的优点是，被修改后的数据永远在缓存中存在，下次访问时，能够直接命中缓存，不用再从后端数据库中查询，这个过程拥有比较好的性能，比较适合先修改又立即访问的业务场景。但缺点是在高并发场景下，如果存在多个操作同时修改同一个值的情况，可能会导致缓存和数据库的不一致。 
3、当使用只读缓存时，如果修改数据库失败了，那么缓存中的数据也不会被删除，此时数据库和缓存中的数据依旧保持一致。而使用读写缓存时，如果是先修改缓存，后修改数据库，如果缓存修改成功，而数据库修改失败了，那么此时数据库和缓存数据就不一致了。如果先修改数据库，再修改缓存，也会产生上面所说的并发场景下的不一致。 我个人总结，只读缓存是牺牲了一定的性能，优先保证数据库和缓存的一致性，它更适合对于一致性要求比较要高的业务场景。而如果对于数据库和缓存一致性要求不高，或者不存在并发修改同一个值的情况，那么使用读写缓存就比较合适，它可以保证更好的访问性能。
```

## 缓存满了的替换策略
### 怎么设置合适缓存容量
容量规划不能一概而论，需要结合**应用数据实际访问特征和成本开销**来综合考虑。
大容量缓存是能带来性能加速的收益，但是成本也会更高，而小容量缓存不一定就起不到加速访问的效果。一般来说，**建议把缓存容量设置为总数据量的15%到30%，兼顾访问性能和内存空间开销。**
```shell
# 使用该命令设置缓存容量为4GB
CONFIG SET maxmemory 4gb
```
### Redis缓存有哪些淘汰策略
![[Pasted image 20241008172414.png]]
redis3.0后的默认策略是noeviction。
7种进行数据淘汰的策略：
- volatile-ttl：在筛选时，会针对设置了过期时间的键值对，根据过期时间的先后进行删除，越早过期的越先被删除。
- volatile-random：针对设置了过期时间的键值对，进行随机删除。
- volatile-lru：使用lru算法筛选设置了过期时间的键值对。
- volatile-lfu：使用LFU算法选择设置了过期时间的键值对。
- allkeys-random：从所有键值对中随机选择并删除数据。
- allkeys-lru：使用LRU算法在所有数据中进行筛选。
- allkeys-lfu：使用LFU算法在所有数据中进行筛选。
### LRU算法
![[Pasted image 20241008173657.png]]
LRU算法在实际实现时，需要用链表管理所有的缓存数据，这会带来额外的空间开销。而且当有数据被访问时，需要在链表上把该数据移动到MRU端，如果有大量数据被访问，就会带来很多链表移动操作，会很耗时，进而会降低Redis缓存性能。
所以，Redis对LRU算法进行了简化，以减轻数据淘汰对缓存性能的影响。
```shell
# 待淘汰的数据采样100个，能进入候选集合的数据的lru字段值必须小于候选集合中最小的lru值。(小顶堆)
config set maxmemory-samples 100
```

```shell
# 设置allkeys-lru淘汰策略
config set maxmemory-policy allkeys-lru
```
三个建议：
- **优先使用allkeys-lru策略。**
- 如果应用中的数据访问频率相差不大，没有明显冷热数据区分，建议使用allkeys-random策略。
- 如果业务中有置顶的需求，可以使用volatile-lru策略，同时不给这些置顶数据设置过期时间。
### 如何处理被淘汰的数据
![[Pasted image 20241008175518.png]]
不过，对于Redis来说，它决定了被淘汰的数据后，会把它们删除。即使淘汰的数据是脏数据，Redis也不会把它们写回数据库。所以，我们在使用Redis缓存时，如果数据被修改了，需要在数据修改时就将它写回数据库。
## 如何解决缓存和数据库的数据不一致问题？
缓存和数据库的数据不一致一般是由两个原因导致的：
- 删除缓存值或更新数据库失败而导致数据不一致，你可以使用重试机制确保删除或更新操作成功。
- 在删除缓存值、更新数据库的这两步操作中，有其他线程的并发读操作，导致其他线程读取到旧值，应对方案是延迟双删。
![[Pasted image 20241009100309.png]]
建议优先使用先更新数据库再删除缓存的方法，原因有两个：
1. 先删除缓存值再更新数据库，有可能导致请求因缓存缺失而访问数据库，给数据库带来压力；
2. 如果业务应用中读取数据库和写缓存的时间不估算，那么，延迟双删的等待时间不好设置。
不过，当使用先更新数据库再删除缓存时，也有个地方需要注意，如果业务层要求必须读取一致的数据，那么，我们就需要在更新数据库时，现在Redis缓存客户端暂存并发读请求，等数据库更新完、缓存值删除后，再读取数据，从而保证数据一致性。

## 缓存雪崩、击穿、穿透问题
![[Pasted image 20241009112107.png]]
服务熔断、服务降级、请求限流这些方法都是属于"有损"方案，会影响用户的体验。所以建议，尽量使用预防式方案：
- 针对缓存雪崩，合理地设置数据过期时间，以及搭建高可靠缓存集群；
- 针对缓存击穿，在缓存访问非常频繁地热点数据时，不要设置过期时间；
- 针对缓存穿透，提前在入口前端实现恶意请求检测，或者规范数据库的数据删除操作，避免误删除。
## 缓存污染问题的解决方法
### 什么事缓存污染？
在一些场景下，有些数据被访问的次数非常少，甚至只会被访问一次。当这些数据服务完访问请求后，如果还继续留存在缓存中的话，就只会白白占用缓存空间，这种情况，就是缓存污染。
volatile-random和allkeys-random策略无法解决缓存污染的问题，volatile-ttl在明确知道数据被再次访问的情况下，可以有效避免缓存污染。
**因为只看数据的访问时间，使用LRU策略在处理扫描式单词查询操作时，无法解决缓存污染。** 扫描单次查询操作，就是指应用对大量的数据进行一次全体读取，每个数据都会被读取，而且只会被读取一次。
### LFU
Redis 4.0版本开始增加了LFU淘汰策略，与LRU策略相比，LFU策略中会从两个维度来筛选并淘汰数据：一是，数据访问的时效性(访问时间离当前时间的远近)；而是，数据的被访问次数。
为了避免操作链表的开销，Redis在实现LRU策略时使用了两个近似方法：
- Redis是用RedisObject结构来保存数据的，RedisObject结构中设置了一个lru字段，用来记录数据的访问时间戳；
- Redis并没有为所有的数据维护一个全局的链表，而是通过随机采样方式，选取一定数量(例如10个)的数据放入候选集合，后续在候选集合中根据lru字段值的带下进行筛选。
在次基础上，**Redis在实现LFU策略的时候，只是把原来24bit大小的lru字段，有进一步拆分成了两部分。**
1. ldt值：lru字段的前16bit，表示数据的访问时间戳；
2. counter值：lru字段的后8bit，表示数据的访问次数。
8为最大能表示的值为255，**Redis在实现LFU策略时，并没有采用数据每被访问一次，就给对应的counter值加1的计数规则，而是采用了一个更优化的计数规则。**
简单来说，LFU策略实现的计数规则是：每当数据被访问一次时，首先，用计数器当前的值乘以配置项`lfu_log_factor`再加1，再取其倒数，得到一个p值；然后，把这个p值和一个取值范围在(0,1)间的随机数r值比大小，只有p值大于r值时，计数器采加1。
技数规则如下代码所示：
```c
double r = (double)rand()/RAND_MAX;
...
double p = 1.0/(baseval*server.lfu_log_factor+1);
if (r < p) counter++;
```
其中baseval是计数器当前的值。计数器的初始值默认为5，而不是0，这样可以避免数据刚被写入缓存，就因为访问次数少而被立即淘汰。
下图，记录了当`lfu_log_factor`取不同值时，在不同的实际访问次数情况下，计数器的值是如何变化的。
![[Pasted image 20241009150519.png]]
一般应用取值为10就差不多可以区分访问次数了。

在一些场景下，有些数据在短时间内被大量访问后就不会再被访问了。那么按照访问次数来筛选的话，这些数据会被留存在缓冲中，但不会提升缓存命中率。为此，Redis在实现LFU策略时，还设计了一个counter值的衰减机制。

简单来说，LFU策略使用衰减因子配置项`lfu_decay_time`来控制访问次数的衰减。LFU策略会计算当前时间和数据最近一次访问时间的差值，并把这个差值转换为分钟为单位。然后，LFU策略再把这个差值除以`lfu_decay_time`值，所得的结果就是数据counter要衰减的值。
### 小结
在实际业务应用中，LRU和LFU两个策略都有应用。LRU和LFU连个策略关注的数据访问特征各有侧重，LRU策略更加关注数据的时效性，而LFU策略更加关注数据的访问频次。通常情况下，实际应用的负载具有较好的时间局部性，所以LRU策略的引用会更加广泛。但是，在扫描式查询的应用场景中，LFU策略就可以更好地应对缓存污染问题。
```ad-note
使用了 LFU 策略后，缓存还会被污染吗？ 我觉得还是有被污染的可能性，被污染的概率取决于LFU的配置，也就是lfu-log-factor和lfu-decay-time参数。 
1、根据LRU counter计数规则可以得出，counter递增的概率取决于2个因素： 
a) counter值越大，递增概率越低 
b) lfu-log-factor设置越大，递增概率越低 所以当访问次数counter越来越大时，或者lfu-log-factor参数配置过大时，counter递增的概率都会越来越低，这种情况下可能会导致一些key虽然访问次数较高，但是counter值却递增困难，进而导致这些访问频次较高的key却优先被淘汰掉了。 另外由于counter在递增时，有随机数比较的逻辑，这也会存在一定概率导致访问频次低的key的counter反而大于访问频次高的key的counter情况出现。 
2、如果lfu-decay-time配置过大，则counter衰减会变慢，也会导致数据淘汰发生推迟的情况。 
3、另外，由于LRU的ldt字段只采用了16位存储，其精度是分钟级别的，在counter衰减时可能会产生同一分钟内，后访问的key比先访问的key的counter值优先衰减，进而先被淘汰掉的情况。 可见，Redis实现的LFU策略，也是近似的LFU算法。Redis在实现时，权衡了内存使用、性能开销、LFU的正确性，通过复用并拆分lru字段的方式，配合算法策略来实现近似的结果，虽然会有一定概率的偏差，但在内存数据库这种场景下，已经做得足够好了。
```

## Pika
### 大内存Redis实例的潜在问题
- 内存快照RDB生成和恢复效率低：RDB文件生成时的fork时长就会增加，这就会导致Redis实例阻塞。而且RDB文件增大后，使用RDB进行恢复的时长也会增加，会导致Redis较长时间无法对外提供服务。
- 主从节点全量同步时长增加、缓冲区易溢出：如果RDB文件很大，肯定会导致全量同步时长增加，效率不高，而且还可能会导致复制缓冲区溢出。一旦缓冲区溢出了，主从节点间就会又开始全量同步，影响业务应用的正常使用。
### Pika的整体架构
![[Pasted image 20241010102624.png]]
Pika的网路框架是对操作系统底层的网络函数进行了封装。Pika在进行网络通信时，可以直接调用网络框架封装好的函数。
如下图所示，为Pika的多线程模型：
![[Pasted image 20241010102906.png]]
### Pika基于RocksDB的数据写入和读取的基本流程
![[Pasted image 20241010103121.png]]
![[Pasted image 20241010103126.png]]
Pika使用了binlog机制实现增量命令同步，既节省了内存，还避免了缓冲区溢出的问题。
### Pika如何实现Redis数据类型兼容？
为了保持和Redis的兼容性，Pika的Nemo模块就负责把Redis的集合类型转成单值的键值对。简单来说，我们可以把Redis的集合类型分成两类：
- 一类是List和Set类型，它们的集合中也只有单值；
- 另一类是Hash和Sorted Set类型，它们的集合中的元素都是成对的，其中，Hash集合元素使field-value类型，而Sorted Set集合元素使member-score类型。
![[Pasted image 20241010103708.png]]
![[Pasted image 20241010103713.png]]
![[Pasted image 20241010103717.png]]
![[Pasted image 20241010103721.png]]
### 小结
Pika的优点：既支持Redis操作接口，又能支持保存大容量的数据。
Pika的缺点：Pika毕竟是把数据保存到了SSD上，数据访问要读写SSD，所以，读写性能要弱于Redis。针对这点，提供两个降低读写SSD对Pika性能影响的小建议：
1. 利用Pika的多线程模型，增加线程数量，提升Pika的并发处理能力；
2. 为Pika配置高配的SSD，提升SSD自身的访问性能。
```ad-note
是否可以使用机械硬盘作为Redis的内存容量的扩展？ 
我觉得也是可以的。机械硬盘相较于固态硬盘的优点是，成本更低、容量更大、寿命更长。 
1、成本：机械硬盘是电磁存储，固态硬盘是半导体电容颗粒组成，相同容量下机械硬盘成本是固态硬盘的1/3。 
2、容量：相同成本下，机械硬盘可使用的容量更大。 
3、寿命：固态硬盘的电容颗粒擦写次数有限，超过一定次数后会不可用。相同ops情况下，机械硬盘的寿命要比固态硬盘的寿命更长。 但机械硬盘相较于固态硬盘的缺点也很明显，就是速度慢。 机械硬盘在读写数据时，需要通过转动磁盘和磁头等机械方式完成，而固态硬盘是直接通过电信号保存和控制数据的读写，速度非常快。 如果对于访问延迟要求不高，对容量和成本比较关注的场景，可以把Pika部署在机械硬盘上使用。 
另外，关于Pika的使用场景，它并不能代替Redis，而是作为Redis的补充，在需要大容量存储（50G数据量以上）、访问延迟要求不苛刻的业务场景下使用。在使用之前，最好是根据自己的业务情况，先做好调研和性能测试，评估后决定是否使用。
```

## Redis中两种原子操作方法
1. 把多个操作在Redis中实现成一个操作，也就是单命令操作；
2. 把多个操作写到一个Lua脚本中，以原子性方式执行单个Lua脚本。
因为Redis的主线程是单线程运行的，所有操作都是顺序原子的执行的，比如INCR/DECR命令在Redis中就是原子的对数据进行增值/减值操作。
可以通过一下命令来执行lua脚本，Redis的单线程模型会保证lua脚本的执行是原子操作：
```shell
EVAL script numkeys key [key ...] arg [arg ...]
```

```ad-note
是否需要把读取客户端 ip 的访问次数 GET(ip)，以及判断访问次数是否超过 20 的判断逻辑，也加到 Lua 脚本中？ 我觉得不需要，理由主要有2个。 1、这2个逻辑都是读操作，不会对资源临界区产生修改，所以不需要做并发控制。 
2、减少 lua 脚本中的命令，可以降低Redis执行脚本的时间，避免阻塞 Redis。 另外使用lua脚本时，还有一些注意点： 
	1、lua 脚本尽量只编写通用的逻辑代码，避免直接写死变量。变量通过外部调用方传递进来，这样 lua 脚本的可复用度更高。 
	2、建议先使用SCRIPT LOAD命令把 lua 脚本加载到 Redis 中，然后得到一个脚本唯一摘要值，再通过EVALSHA命令 + 脚本摘要值来执行脚本，这样可以避免每次发送脚本内容到 Redis，减少网络开销。
```
## Redis实现分布式锁
### 基于单个Redis节点实现分布式锁
通过`set`命令来加锁，并且设置过期时间，以及判断锁是否存在，可以保证这三个操作是原子执行的。
```shell
SET key value [EX seconds | PX milliseconds] [NX]
```
有了SET命令的NX和EX/PX选项后，我们就可以用下面的命令来实现加锁操作了。
```shell
## 加锁，unique_value作为客户端唯一性的标识，避免其他客户端可以直接del掉当前客户端加的锁。
SET lock_key unique_value NX PX 10000
```
因为在加锁操作中，每个客户端都使用了一个唯一标识，所以在释放锁操作时，我们需要判断变量的值，是否等于执行执行释放锁操作的客户端的唯一标识：
```lua
// 释放锁 比较unique_value是否相等，避免误释放
if redis.call("get",KEYS[1] == ARGV[1]) then
	return redis.call("del",KEYS[1])
else
	return 0
end
```
我们通过`script load lua.script`将释放锁的脚本预加载到缓存中，该命令会返回一个SHA值，通过`[EVALSHA sha1 numkeys key [key ...] arg [arg ...]]`命令可以原子的执行该脚本。
### 基于多个Redis节点实现高可靠的分布式锁--Redlock
Redlock算法的基本思路，是让客户端和多个独立的Redis实例一次请求加锁，如果客户端能够和半数以上的实例成功完成加锁操作，那么我们就认为，客户端成功地获得分布式锁了，否则加锁失败。这样一来，即使单个Redis实例发生故障，因为锁变量在其它实例上也有保存，所以，客户端仍然可以正常地进行锁操作，锁变量并不会丢失。

Redlock算法的执行步骤，假设有N个独立的Redis实例。
- **第一步，客户端获取当前时间**。
- **第二步，客户端按顺序依次向N个Redis实例执行加锁操作。**（使用SET命令，带上NX，EX/PX选项，以及带上客户端的唯一标识。如果某个Redis实例发生故障了，为了保证在这种情况下，Redlock算法能够继续运行，我们需要给加锁操作设置一个超时时间。）
- **第三步，一旦客户端完成了和所有Redis实例的加锁操作，客户端就要计算整个加锁过程的耗时。** 客户端只有在满足下面的这两个条件时，才能认为是加锁成功。
	- 条件一：客户端超过半数的Redis实例上成功获取到了锁；
	- 条件而：客户端获取锁的总耗时没有超过锁的有效时间。

关于Redlock的争论文章：[链接](http://zhangtielei.com/posts/blog-redlock-reasoning.html)
## Redis事务机制
Redis提供了MULTI、EXEC命令来提供事务支持。
### 原子性的保证
- 命令入队时就报错，会放弃事务执行，保证原子性；
- 命令入队时没报错，实际执行时报错，不保证原子性；
- EXEC命令执行实例故障，如果开启了AOF日志，可以保证原子性。
Redis中没有提供回滚机制，只提供了DISCARD命令来主动放弃事务执行，把暂存的命令队列清空，起不到回滚的效果。
**在执行事务的EXEC命令时，Redis实例发生了故障，导致事务执行失败。**
这种情况下，如果Redis开启了AOF日志，那么，只会有部分的事务操作被记录到AOF日志中。我们需要使用redis-check-aof工具检查AOF日志文件，这个工具可以把未完成事务操作从AOF文件中取出。这样一来，我们使用AOF回复实例后，事务操作不会再被执行，从而保证原子性。
### 一致性保障
- 情况一：命令入队时就报错，事务本身就会被放弃执行，所以可以保证数据库的一致性。
- 情况二：命令入队时没有报错，实际执行时报错，保证数据的一致性。
- 情况三：EXEC命令执行时实例发生故障，没有开启RDB或AOF，那么实例故障重启后，数据都没有了，数据库是一致的。如果开启了RDB快照，因为RDB快照不会再事务执行时执行，所以，事务命令不会被保存到RDB快照中，所以可以保证一致性。如果开启的是AOF，需要使用redis-check-aof工具清楚事务中已经完成的操作，数据库回复之后也是一致的。
### 隔离性
1. 并发操作在EXEC命令前执行，此时，隔离性的保证要使用WATCH机制来实现，否则隔离性无法保证。
2. 并发操作在EXEC命令后执行，此时，隔离性可以保证。
![[Pasted image 20241010172201.png]]
![[Pasted image 20241010172206.png]]
![[Pasted image 20241010172219.png]]
### 持久性
不管Redis采用什么持久模式，事务的持久性属性是得不到保证的。
### 命令总结
![[Pasted image 20241010172322.png]]
```ad-note
在执行事务时，如果 Redis 实例发生故障，而 Redis 使用的 RDB 机制，事务的原子性还能否得到保证？ 
我觉得是可以保证原子性的。 如果一个事务只执行了一半，然后 Redis 实例故障宕机了，由于 RDB 不会在事务执行时执行，所以 RDB 文件中不会记录只执行了一部分的结果数据。之后用 RDB 恢复实例数据，恢复的还是事务之前的数据。但 RDB 本身是快照持久化，所以会存在数据丢失，丢失的是距离上一次 RDB 之间的所有更改操作。 关于 Redis 事务的使用
，有几个细节我觉得有必要补充下，关于 Pipeline 和 WATCH 命令的使用。 1、在使用事务时，建议配合 Pipeline 使用。 
a) 如果不使用 Pipeline，客户端是先发一个 MULTI 命令到服务端，客户端收到 OK，然后客户端再发送一个个操作命令，客户端依次收到 QUEUED，最后客户端发送 EXEC 执行整个事务（文章例子就是这样演示的），这样消息每次都是一来一回，效率比较低，而且在这多次操作之间，别的客户端可能就把原本准备修改的值给修改了，所以无法保证隔离性。 
b) 而使用 Pipeline 是一次性把所有命令打包好全部发送到服务端，服务端全部处理完成后返回。这么做好的好处，一是减少了来回网络 IO 次数，提高操作性能。二是一次性发送所有命令到服务端，服务端在处理过程中，是不会被别的请求打断的（Redis单线程特性，此时别的请求进不来），这本身就保证了隔离性。我们平时使用的 Redis SDK 在使用开启事务时，一般都会默认开启 Pipeline 的，可以留意观察一下。 
2、关于 WATCH 命令的使用场景。 
a) 在上面 1-a 场景中，也就是使用了事务命令，但没有配合 Pipeline 使用，如果想要保证隔离性，需要使用 WATCH 命令保证，也就是文章中讲 WATCH 的例子。但如果是 1-b 场景，使用了 Pipeline 一次发送所有命令到服务端，那么就不需要使用 WATCH 了，因为服务端本身就保证了隔离性。 
b) 如果事务 + Pipeline 就可以保证隔离性，那 WATCH 还有没有使用的必要？答案是有的。对于一个资源操作为读取、修改、写回这种场景，如果需要保证事物的原子性，此时就需要用到 WATCH 了。例如想要修改某个资源，但需要事先读取它的值，再基于这个值进行计算后写回，如果在这期间担心这个资源被其他客户端修改了，那么可以先 WATCH 这个资源，再读取、修改、写回，如果写回成功，说明其他客户端在这期间没有修改这个资源。如果其他客户端修改了这个资源，那么这个事务操作会返回失败，不会执行，从而保证了原子性。 细节比较多，如果不太好理解，最好亲自动手试一下。
```
## Redis主从同步与故障切换存在的坑及解决方法
![[Pasted image 20241011140129.png]]
### 监控主从库间复制进度的思路
Redis的INFO replication命令可以查看主库接收写命令的进度信息(master_repl_offset)和从库复制写命令的进度信息(slave_repl_offset)，所以，我们就可以开发一个监控程序，查到主从库的进度，然后就可以用master_repl_offset减去slave_repl_offset，这样就能得到从库和主库间的复制进度差值了。
![[Pasted image 20241011140537.png]]
### 哨兵protected-mode的配置建议
```shell
protected-mode no
bind 192.168.10.3 192.168.10.4 192.168.10.5
```
这样配置之后，只有bind中设置得ip地址的哨兵，才可以访问当前实例，既保证了实例间能够通信进行主从切换，也保证了哨兵的安全性。
### cluster-node-timeout配置项
如果执行主从切换的实例超过半数，而主从切换时间又过长的话，就可能有半数以上的实力心跳超时，从而可能导致整个集群挂掉。所以，建议将cluster-node-timeout调大。
### slave-serve-stale-data与slave-read-only的解释
slave-serve-stale-data:当一个slave与master失去联系，或者复制正在进行的时候，slave应对请求的行为: 1)如果为yes(默认值)，slave仍然会应答客户端请求，但返回的数据可能是过时，或者数据可能是空的在第一次同步的时候；2) 如果是no，在你执行除了info和slaveof之外的其他命令时，slave都将返回一个"SYNC with master in progress"的错误。
slave-read-only:设置slave是否是只读。从2.6版起，slave默认是只读的。
## Redis中的脑裂现象，以及应对方法
Redis丢失数据的原因：
- 主库的数据还没有同步到从库，结果主库发生了故障，等从库升级为主库后，未同步的数据就丢失了，这种情况可以通过比对主从库上的复制进度差值来进行判断，也就是计算master_repl_offset和slave_repl_offset的差值。
![[Pasted image 20241011172751.png]]
- 脑裂引发数据丢失。
![[Pasted image 20241011172744.png]]
### 如何应对脑裂问题？
Redis提供了两个配置项来限制主库的请求处理：
- `min-slaves-to-write`：这个配置项设置了主库能进行同步的最小从库数量；
- `min-slaves-max-lag`：这个配置项设置了主从库间进行数据复制时，从库给主库发送ACK消息的最大延迟(以秒为单位)。
我们可以把min-salves-to-write 和 min-slaves-max-lag这两个配置项搭配起来使用，分别给它们设置一定的阈值，假设为N和T。这两个配置项组合后的要求是，主库连接的从库中至少有N个从库，和主库进行数据复制时的ACK消息延迟不能超过T秒，否则，主库就不会再接收客户端的请求了。

在实际应用中，可能会存在网路短暂的抖动导致从库暂时和主库的ACK消息超时。所以建议：
假设从库有K个，可以将min-slaves-to-write设置为K/2+1(如果K等于1，就设为1)，将min-slaves-max-lag设置为十几秒，在这个配置下，如果有一半以上的从库和主库进行的ACK消息延迟超过十几秒，我们就禁止主库接收客户端写请求。
```ad-note
假设我们将 min-slaves-to-write 设置为 1，min-slaves-max-lag 设置为 15s，哨兵的 down-after-milliseconds 设置为 10s，哨兵主从切换需要 5s。主库因为某些原因卡住了 12s，此时，还会发生脑裂吗？
主从切换完成后，数据会丢失吗？ 主库卡住 12s，达到了哨兵设定的切换阈值，所以哨兵会触发主从切换。但哨兵切换的时间是 5s，也就是说哨兵还未切换完成，主库就会从阻塞状态中恢复回来，而且也没有触发 min-slaves-max-lag 阈值，所以主库在哨兵切换剩下的 3s 内，依旧可以接收客户端的写操作，如果这些写操作还未同步到从库，哨兵就把从库提升为主库了，那么此时也会出现脑裂的情况，之后旧主库降级为从库，重新同步新主库的数据，新主库也会发生数据丢失。 由此也可以看出，即使 Redis 配置了 min-slaves-to-write 和 min-slaves-max-lag，当脑裂发生时，还是无法严格保证数据不丢失，它只能是尽量减少数据的丢失。 其实在这种情况下，新主库之所以会发生数据丢失，是因为旧主库从阻塞中恢复过来后，收到的写请求还没同步到从库，从库就被哨兵提升为主库了。如果哨兵在提升从库为新主库前，主库及时把数据同步到从库了，那么从库提升为主库后，也不会发生数据丢失。但这种临界点的情况还是有发生的可能性，因为 Redis 本身不保证主从同步的强一致。 

还有一种发生脑裂的情况，就是网络分区：主库和客户端、哨兵和从库被分割成了 2 个网络，主库和客户端处在一个网络中，从库和哨兵在另一个网络中，此时哨兵也会发起主从切换，出现 2 个主库的情况，而且客户端依旧可以向旧主库写入数据。等网络恢复后，主库降级为从库，新主库丢失了这期间写操作的数据。 脑裂产生问题的本质原因是，Redis 主从集群内部没有通过共识算法，来维护多个节点数据的强一致性。它不像 Zookeeper 那样，每次写请求必须大多数节点写成功后才认为成功。当脑裂发生时，Zookeeper 主节点被孤立，此时无法写入大多数节点，写请求会直接返回失败，因此它可以保证集群数据的一致性。 另外关于 min-slaves-to-write，有一点也需要注意：如果只有 1 个从库，当把 min-slaves-to-write 设置为 1 时，在运维时需要小心一些，当日常对从库做维护时，例如更换从库的实例，需要先添加新的从库，再移除旧的从库才可以，或者使用 config set 修改 min-slaves-to-write 为 0 再做操作，否则会导致主库拒绝写，影响到业务。
```

## Codis VS Redis Cluster
Codis的整体架构和基本流程
![[Pasted image 20241012143142.png]]
首先使用codis dashboard设置codis server和codis proxy的访问地址，设置完成后，codis server和codis proxy才会开始接收连接。

客户端直接和codis proxy建立连接，因为codis proxy本身支持Redis的RESP交互协议，所以，客户端访问codis proxy和访问原生的Redis实例没有什么区别。
![[Pasted image 20241012143604.png]]
## Codis进群数据如何分布
![[Pasted image 20241012143650.png]]
我们把Slot和codis server的映射关系称为数据路由表。在dashboard上分配好路由表后，dashboard会把路由表发送给codis proxy，同时会在Zookeeper中保存一份。
![[Pasted image 20241012143841.png]]
在数据分布的实现上，Codis和Redis Cluster很相似，都采用了key映射到Slot、Slot再分配到实例上的机制。
Codis中路由表保存在Zookeeper集群中，一旦数据位置发生变化(例如有实例增减)，路由表被修改了，codis dashboard就会把修改后的路由表发送给codis proxy，proxy就可以根据最新的路由信息转发请求了。
在Redis Cluster中，数据路由表是通过每个实例相互间的通信传递的，最后会在每个实例上保存一份。当数据路由信息发生变化时，就需要在所有实例之间通过网络消息进行传递。所以，实例数量较多的话，就会消耗较多的集群网络资源。
### 集群扩容和数据迁移
Codis集群扩容包括两方面：增加 codis server 和 增加 codis proxy。
增加codis server的步骤：
1. 启动codis server，将它加入集群；
2. 把部分数据迁移到新的server。
Codis集群按照Slot的粒度进行数据迁移，支持同步迁移和异步迁移，同步迁移流程如下：
![[Pasted image 20241012150022.png]]
而一部迁移有两个特点：
1. 当源server把数据发送给目的server后，就可以处理其他请求操作了，不用等到目的server的命令执行完。而目的server会在收到数据并反序列不啊保存到本地后，给源server发送一个ACK消息，表明迁移完成。此时，源server在本地把刚才迁移的数据删除。在这个过程成，迁移的数据会被设置为只读，所以，源server上的数据不会被修改，保证数据一致性。
2. 对于bigkey，异步迁移采用了拆分指令的方式进行迁移。避免了bigkey迁移时，因为序列化大量数据而阻塞源server的问题。**你还可以通过异步迁移命令 SLOTSMGRTTAGSLOT-ASYNC 的参数 numkeys 设置每次迁移的 key 数量。**
codis proxy 也可以进行扩容。
![[Pasted image 20241012150157.png]]
### 客户端兼容性 如果是使用Codis，不需要升级客户端，但是如果从但实例redis改为Redis Cluster需要升级客户端才能支持一集群中的命令。
### codis 集群可靠性保证
![[Pasted image 20241012150916.png]]
### 切片集群方案选择建议
![[Pasted image 20241012150947.png]]

  
1. 从稳定性和成熟度来看，Codis 应用得比较早，在业界已经有了成熟的生产部署。虽然 Codis 引入了 proxy 和 Zookeeper，增加了集群复杂度，但是，proxy 的无状态设计和 Zookeeper 自身的稳定性，也给 Codis 的稳定使用提供了保证。而 Redis Cluster 的推出时间晚于 Codis，相对来说，成熟度要弱于 Codis，如果你想选择一个成熟稳定的方案，Codis 更加合适些。

2. 从业务应用客户端兼容性来看，连接单实例的客户端可以直接连接 codis proxy，而原本连接单实例的客户端要想连接 Redis Cluster 的话，就需要开发新功能。所以，如果你的业务应用中大量使用了单实例的客户端，而现在想应用切片集群的话，建议你选择 Codis，这样可以避免修改业务应用中的客户端。

3. 从使用 Redis 新命令和新特性来看，Codis server 是基于开源的 Redis 3.2.8 开发的，所以，Codis 并不支持 Redis 后续的开源版本中的新增命令和数据类型。另外，Codis 并没有实现开源 Redis 版本的所有命令，比如 BITOP、BLPOP、BRPOP，以及和与事务相关的 MUTLI、EXEC 等命令。Codis 官网上列出了不被支持的命令列表，你在使用时记得去核查一下。所以，如果你想使用开源 Redis 版本的新特性，Redis Cluster 是一个合适的选择。

4. 从数据迁移性能维度来看，Codis 能支持异步迁移，异步迁移对集群处理正常请求的性能影响要比使用同步迁移的小。所以，如果你在应用集群时，数据迁移比较频繁的话，Codis 是个更合适的选择。
```ad-note
假设 Codis 集群中保存的 80% 的键值对都是 Hash 类型，每个 Hash 集合的元素数量在 10 万 ～ 20 万个，每个集合元素的大小是 2KB。迁移一个这样的 Hash 集合数据，是否会对 Codis 的性能造成影响？ 不会有性能影响。 Codis 在迁移数据时，设计的方案可以保证迁移性能不受影响。 1、异步迁移：源节点把迁移的数据发送给目标节点后就返回，之后接着处理客户端请求，这个阶段不会长时间阻塞源节点。目标节点加载迁移的数据成功后，向源节点发送 ACK 命令，告知其迁移成功。 2、源节点异步释放 key：源节点收到目标节点 ACK 后，在源实例删除这个 key，释放 key 内存的操作，会放到后台线程中执行，不会阻塞源实例。（没错，Codis 比 Redis 更早地支持了 lazy-free，只不过只用在了数据迁移中）。 3、小对象序列化传输：小对象依旧采用序列化方式迁移，节省网络流量。 4、bigkey 分批迁移：bigkey 拆分成一条条命令，打包分批迁移（利用了 Pipeline 的优势），提升迁移速度。 5、一次迁移多个 key：一次发送多个 key 进行迁移，提升迁移效率。 6、迁移流量控制：迁移时会控制缓冲区大小，避免占满网络带宽。 7、bigkey 迁移原子性保证（兼容迁移失败情况）：迁移前先发一个 DEL 命令到目标节点（重试可保证幂等性），然后把 bigkey 拆分成一条条命令，并设置一个临时过期时间（防止迁移失败在目标节点遗留垃圾数据），迁移成功后在目标节点设置真实的过期时间。 Codis 在数据迁移方面要比 Redis Cluster 做得更优秀，而且 Codis 还带了一个非常友好的运维界面，方便 DBA 执行增删节点、主从切换、数据迁移等操作。 我当时在对 Codis 开发新的组件时，被 Codis 的优秀设计深深折服。当然，它的缺点也很明显，组件比较多，部署略复杂。另外，因为是基于 Redis 3.2.8 做的二次开发，所以升级 Redis Server 比较困难，新特性也就自然无法使用。 现在 Codis 已经不再维护，但是作为国人开发的 Redis 集群解决方案，其设计思想还是非常值得学习的。也推荐 Go 开发者，读一读 Codis 源码，质量非常高，对于 Go 语言的进阶也会有很大收获！
```

## Redis在秒杀系统中的作用
### 秒杀场景的负载特征对支撑系统的要求，秒杀场景的特征
**第一个特征是瞬时并发量非常高。**
所以当有大量并发请求涌入秒杀系统时，我们就需要使用Redis先拦截大部分请求，避免大量请求直接发送给数据库，把数据库压垮。
**第二个特征是，读多写少，而且读操作是简单的查询操作。**
### Redis可以在秒杀场景的哪些环节发挥作用？
一般可以把秒杀活动分成三个阶段：
- 第一阶段是秒杀互动前。这个阶段，用户会不断刷新伤情详情页，这个节点一般尽量把上平详情页面元素静态化，然后使用CDN或是浏览器把这些静态化的元素缓存起来。所以这个阶段不需要Redis。
- 第二阶段是秒杀活动开始。这个阶段的操作主要有三个：库存查验、库存扣减和订单处理。库存查验和库存扣减需要用Redis来支持，订单处理会涉及到支付，商品出库，物流等多个关联操作，一般涉及多张表，需要用数据库来处理事务。
- 第三阶段是秒杀活动结束后。
下图显示了再秒杀场景中需要Redis参与的两个环节：
![[Pasted image 20241014114158.png]]
### Redis的哪些方法可以支撑秒杀场景？
1. 支持高并发。Redis本身高速处理请求的特性就可以支持高并发。如果有多个商品，我们可以使用切片集群，不过需要将商品-库存信息分布到不同redis实例的slot上，需要使用crc算法计算不同秒杀商品的key。
2. 保证库存查验和库存扣减原子性执行。这对这条要求，我们可以使用Redis的原子操作或是分布式锁这两个功能特性来支撑了。
如果我们使用分布式锁来保证原子性，我们可以使用切片集群中的不同实例来分别保存分布式锁和商品信息。如果客户端没有拿到锁，这些客户端就不会查询商品库存，这就可以减轻保存库存信息的实力的压力了。
### 秒杀系统是一个系统性工程，Redis实现了对库存查验和扣减这个环节的支撑，除此之外，还有4个环节需要处理好：
1. **前端页面的设计。** 尽量静态化，充分利用CDN或浏览器缓存服务秒杀开始前的请求。
2. **请求拦截和流控**。 拦截恶意IP，限流。
3. **库存信息过期时间处理。** 不设置过期时间。
4. **数据库订单异常处理。** 重试。

## 数据分布优化：如何应对数据倾斜？
![[Pasted image 20241014183338.png]]
Hash Tag是指在键值对key中的一对花括号{}。这对括号会把key的一部分括起来，客户端在计算key的CRC16值时，只对Hash Tag花括号中的key内容进行计算。

建议一：如果已经发生了数据倾斜，我们可以通过数据迁移来缓解数据倾斜的影响。Redis Cluster和Codis集群都提供了查看Slot分配和手动迁移Slot的命令。

Redis Cluster的迁移步骤：
第1步，我们先从目标实例5上执行下面的命令，将Slot 300的源实例设置为实例3，表示要从实例3上迁移Slot 300.
![[Pasted image 20241014183713.png]]
```shell
cluster setslot 300 importing 3
```

第2步，在源实例3上，我们把Slot 300的目标实例设置为5，这表示，Slot 300要迁移到实例5上：
```shell
cluster setslot 300 migrating 5
```

第3步，从Slot300中获取100个key。因为Slot中的key数量可能很多，所以我们需要在客户端上多次执行下面的命令：
```shell
cluster getkeysinslot 300 100
```

第4步，我们把刚才获取的100个key中的key1迁移到目标实例5上，同时把要嵌入的数据库设置为0号数据库，把迁移的超时时间设置为timeout。我们重复执行MIGRATE命令，把100个key都迁移完。
```shell
migrate 192.168.10.5 key1 0 timeout
```
从Redis3.0.6开始，可以使用KEYS选项，一次迁移多个key，这样可以提升迁移效率：
```shell
MIGRATE 192.168.10.5 6379 "" 0 timeout KEYS key1 key2 key3
```

Codis的迁移命令：
```shell
codis-admin --dashboard=ADDR -slot-action --create --sid=300 --gid=6
```

## Gossip
Gossip的三板斧分别是：直接邮寄(Direct Mail)、反熵(Anti-entropy)和谣言传播(Rumor mongering)。
- 直接邮寄：就是直接发送更新数据，当数据发送失败时，将数据缓存下来，然后重传。
- 反熵：通过异步修复实现最终一致性的方法。通过推、拉、推拉的方式来修复数据。反熵指的是集群中的节点，每隔一段时间就随机选择某个其他节点，然后通过互联网互相交换自己的所有数据来消除两者之间的差异，实现数据的最终一致性。
- 谣言传播：广泛地散播谣言，它指的是当一个节点有了新数据后，这个节点变成活跃状态，并周期性地联系其他节点向其发送新数据，知道所有节点都存储了该数据。
虽然反熵很实用，但是执行反熵时，相关节点都是已知的，而且节点数量不能太多，如果是一个动态变化或节点数比较多的分布式环境，反熵就不适用了，那么就需要使用谣言传播来保证最终一致性了。
实现反熵时，实现细节可以和最初算法的约定不一样，比如，不是一个节点不断随机选择领一个节点，来修复副本上的熵，而是设计一个闭环的流程，一次修复所有节点的数据副本的不一致。
**反熵需要做一致性对比(可以通过校验和来校验数据是否一致)，很消耗系统性能，所以建议你将是否启用反熵功能、执行一致性检测的时间间隔等，做成可配置的，能在不同场景中按需使用。**
## 通信开销：限制Redis Cluster规模的关键因素
### Redis中Gossip协议的工作原理：
1. 每个实例之间会按照一定的频率，从急群中随机挑选一些实例，把PING消息发送给挑选出来的实例，用来检测这些实例是否在线，并交换彼此的状态信息。PING消息中封装了发送消息的实力自身的状态、部分其他实例的状态信息，以及Slot映射表。
2. 一个实例在接收到PING消息后，会给发送PING消息的实力，发送一个PONG消息。PONS消息包含的内容和PING消息一样。
![[Pasted image 20241015114535.png]]
Gossip协议可以保证在一段时间后，集群中的每一个实例都能获得其它所有实例的状态信息。
### 降低实例间通信开销的方法
GOSSIP消息大小没办法减小，每秒一条的PING消息频率不算高，如果再降低该频率的话，集群中各实例的状态可能就没办法及时传播了。
每个实例100毫秒会做一次检测，给PONG消息接收超过cluster-node-timeout/2的节点发送PING消息。按照每100毫秒进行检测的频率，是Redis实例默认的周期性检查任务的统一频率，我们一般不需要修改它。
那么，就只有cluster-node-timeout这个配置项可以修改了。默认是15秒。
但是如果调的时间更长，有会导致实际故障恢复时间被延长，会影响到集群服务的正常工作。
**建议在调整cluster-node-timeout值的前后，使用tcpdump命令抓取实例发送心跳信息网络包的情况。**
```shell
tcpdump host 192.168.10.3 port 16379 -i 网卡名 -w /tmp/r1.cap
```
通过分析网络包的数量和大小，就可以判断调整cluster-node-timeout值前后，心跳占用的带宽情况了。

## Redis 6.0的新特性
### 从单线程处理网络请求到多线程处理
随着网络硬件的性能提升，Redis的性能瓶颈有时会出现在网络IO的处理上，也就是说，**单个主线程处理网络请求的速度跟不上底层网络硬件的速度。**
一般有两种方法应对该问题：
- 用用户态网络协议栈(如DPDK)取代内核网络协议栈，让网络请求的处理不用在内核里执行，直接在用户态完成处理就行。
- 采用多个IO线程来处理网络请求，提高网络请求处理的并行度。Redis 6.0就是采用的这种方法。

可以把主线程和多IO线程的协作分成四个阶段。
**阶段一：服务端和客户端建立Socket连接，并分配处理线程**
**阶段二：IO线程读取并解析请求**
**阶段三：主线程执行请求操作**
这三个阶段如下图所示：
![[Pasted image 20241018111823.png]]
**阶段四：IO线程回写Socket和主线程清空全局队列**
![[Pasted image 20241018112014.png]]
因为网络请求的处理属于io操作，使用多线程可以加速这两个网路解析和写回的操作。从而解决因为网络处理速度跟不上底层硬件速度的瓶颈问题。

**1.设置io-threads-do-reads配置项为yes，表示启用多线程。**
```shell
io-threads-do-reads yes
```
**2.设置线程个数。一般来说，线程个数要小于Redis实例所在机器的CPU核个数，** 例如对于一个8核的机器来说，Redis官方建议配置6个IO线程。
```shell
io-threads 6
```
如果你在实际应用中，发现Redis实例的CPU开销不大，吞吐量却没有提升，可以考虑使用Redis 6.0的多线程机制，加速网路处理，进而提升实例吞吐量。
### 实现服务端协助的客户端缓存
实现了服务端协助的客户端缓存功能，也称为(Tracking)功能。有了这个功能，业务应用中的Redis客户端就可以把读取的数据缓存在业务应用本地了，应用就可以直接在本地快速读取数据了。但是，**如果数据被修改了或是失效了，如何通知客户端对缓存的数据做失效处理？**

**普通模式下的Tracking的启用与禁用命令**:
```shell
CLIENT TRACKING ON|OFF
```
**广播模式下的Tracking命令**：
```shell
CLIENT TRACKING ON BCAST PREFIX user
```
普通和广播模式都需要客户端使用RESP3协议与Redis实例进行通信。
**如何让RESP2协议的客户端也能接受失效消息：**
```shell
//客户端B执行，客户端B的ID号是303 （客户端B使用RESP2）
SUBSCRIBE _redis_:invalidate

//客户端A执行 (客户端A使用RESP3)
CLIENT TRACKING ON BCAST REDIRECT 303
```
这样设置以后，如果有键值对被修改了，客户端B就可以通过_redis_:invalidate频道获得失效消息了。
### Redis 6.0中创建用户密码
```shell
ACL SETUSER normal on >abc
```
Redis 6.0中为以用户为粒度设置命令操作的访问权限。
![[Pasted image 20241018120152.png]]
```shell
# 设置用户 normaluser 只能调用 Hash 类型的命令操作，而不能调用 String 类型的命令操作
ACL SETUSER normaluser +@hash -@string

# 使用波浪号“~”和 key 的前缀来表示控制访问的 key。例如，我们执行下面命令，就可以设置用户 normaluser 只能对以“user:”为前缀的 key 进行命令操作
ACL SETUSER normaluser ~user:* +@all
```
### 小结
![[Pasted image 20241018115524.png]]
## 基于NVM(Non-Volatile Memory)内存部署Redis
NVM内存的特点：
- 能持久化保存数据；
- 读写速度和DRAM接近；
- 容量大。
业界已经有了实际的NVM内存产品，Intel推出了Optane AEP内存条.
AEP内存给软件提供了两种使用模式：
- 第一种Memory模式，这种模式是把NVM内存作为大容量内存来使用，也就是说，只使用NVM容量大和性能高的特性，没有启用数据持久化的功能。
- 第二种是 App Direct模式，这种模式启用了NVM持久化数据的功能。在这种模式下，应用软件把数据写到AEP内存上时，数据就直接持久化保存下来了。也可以称这种模式为(Persistency Memory, PM)。
### 基于NVM内存的Redis实践
先小结一下现在Redis在涉及持久化操作时的问题：
- RDB文件创建时的fork操作会阻塞主线程；
- AOF文件记录日志时，需要在数据可靠性和写性能之间取得平衡；
- 使用RDB或AOF恢复数据时，恢复效率受RDB和AOF大小的限制。
但是，如果我们使用持久化内存，就可以充分利用PM快速持久化的特点，来避免RDB和AOF的操作。因为PM支持内存访问，而Redis的操作都是内存操作，那么，我们就可以把Redis直接运行在PM上。同时，数据本身就可以在PM上持久化保存了，我们就不需要额外的RDB或AOF日志机制来保证数据可靠性了。
**因为PM的读写速度比DRAM慢，所以，如果使用PM来运行Redis，需要评估下PM提供的访问延迟和访问带宽，是否能满足业务层的需求。**
## Redis和memcached以及与RocksDB的对比
![[Pasted image 20241018165905.png]]
![[Pasted image 20241018165910.png]]
**集群的可扩展性是我们评估集群方案的一个重要维度，** 你一定要关注，集群中元数据是用Slot映射表，还是一致性哈希维护的。如果是Slot映射表，那么，是用中心化的第三方存储系统来保存，还是由各个实例来扩散保存，这也是需要考虑清楚的。Redis Cluster、Codis和Memcached采用的方式各不相同。
- Redis Cluster：使用Slot映射表并由实例扩散保存。
- Codis：使用Slot映射表并由第三方存储系统保存。
- Memcached：使用一致性哈希。

## RESP 2的编码格式规范
以下为RESP2协议的5中编码类型和相应的开头字符：
![[Pasted image 20241021135416.png]]
1. **简单字符串类型 (RESP Simple Strings)**
```shell
+OK\r\n
```
2. **长字符串类型(RESP Bulk String)**
```shell
$9 testvalue\r\n
```
3. **整数类型**
```shell
:3\r\n
```
4. **错误类型**
```shell
-ERR unknown command `PUT`, with args beginning with: `testkey`, `testvalue`
```
5. **数组编码类型(RESP Arrays)**
```shell
*2\r\n$3\r\nGET\r\n$7\r\ntestkey\r\n
```
### RESP3
RESP3增加了对浮点数、布尔类型、有序字段集合、无序集合等多种类型数据的支持。不过，这里有个地方需要注意，Redis 6.0只支持RESP3，对RESP 2协议不兼容，所以，如果你使用Redis 6.0版本，需要确认客户端已经支持了RESP 3协议。
## Redis运维工具
### 最基本的监控命令：INFO命令
![[Pasted image 20241021144443.png]]
**无论你是运行但实例或是集群，我建议你重点关注一下stat、commandstat、cpu和memory这四个参数的返回结果。**
### 面向Prometheus的Redis-exporter监控
Redis-exporter是用来监控Redis的，是Prometheus的插件，它将INFO命令监控到的运行状态和各种统计信息提供给Prometheus，从而进行可视化展示和报警设置。除了获取Redis实例的运行状态，Redis-exporter还可以监控键值对的大小和集合类型数据的元素个数。
此外，我们可以开发一个Lua脚本，定制化采集所需监控的数据。然后，我们使用scripts命令行选项，让Redis-exporter运行这个特定的脚本，从而可以满足业务层的多样化监控需求。
### 数据迁移工具Redis-shake
![[Pasted image 20241021144953.png]]
**Redis-shake的一大优势，就是支持多种类型的迁移。它及支持单个实例间的数据迁移，也支持集群到集群间的数据迁移。包括codis这类通过proxy接收请求的集群。**
### 通过Redis-full-check进行数据一致性比对
### 集群管理工具CacheCloud
## Redis的使用规范
![[Pasted image 20241021152714.png]]
```ad-note
开发人员：
1、key 的长度尽量短，节省内存空间 
2、避免 bigkey，防止阻塞主线程 
3、4.0+版本建议开启 lazy-free 
4、把 Redis 当作缓存使用，设置过期时间 
5、不使用复杂度过高的命令，例如SORT、SINTER、SINTERSTORE、ZUNIONSTORE、ZINTERSTORE 6、查询数据尽量不一次性查询全量，写入大量数据建议分多批写入 
7、批量操作建议 MGET/MSET 替代 GET/SET，HMGET/HMSET 替代 HGET/HSET 
8、禁止使用 KEYS/FLUSHALL/FLUSHDB 命令 
9、避免集中过期 key 
10、根据业务场景选择合适的淘汰策略 
11、使用连接池操作 Redis，并设置合理的参数，避免短连接 
12、只使用 db0，减少 SELECT 命令的消耗 
13、读请求量很大时，建议读写分离，写请求量很大，建议使用切片集群 运维层面主要面向的是 DBA 运维人员： 
1、按业务线部署实例，避免多个业务线混合部署，出问题影响其他业务 
2、保证机器有足够的 CPU、内存、带宽、磁盘资源 
3、建议部署主从集群，并分布在不同机器上，slave 设置为 readonly 
4、主从节点所部署的机器各自独立，尽量避免交叉部署，对从节点做维护时，不会影响到主节点 5、推荐部署哨兵集群实现故障自动切换，哨兵节点分布在不同机器上
6、提前做好容量规划，防止主从全量同步时，实例使用内存突增导致内存不足 
7、做好机器 CPU、内存、带宽、磁盘监控，资源不足时及时报警，任意资源不足都会影响 Redis 性能 
8、实例设置最大连接数，防止过多客户端连接导致实例负载过高，影响性能 
9、单个实例内存建议控制在 10G 以下，大实例在主从全量同步、备份时有阻塞风险 
10、设置合理的 slowlog 阈值，并对其进行监控，slowlog 过多需及时报警 
11、设置合理的 repl-backlog，降低主从全量同步的概率 
12、设置合理的 slave client-output-buffer-limit，避免主从复制中断情况发生 
13、推荐在从节点上备份，不影响主节点性能 
14、不开启 AOF 或开启 AOF 配置为每秒刷盘，避免磁盘 IO 拖慢 Redis 性能 
15、调整 maxmemory 时，注意主从节点的调整顺序，顺序错误会导致主从数据不一致 
16、对实例部署监控，采集 INFO 信息时采用长连接，避免频繁的短连接 
17、做好实例运行时监控，重点关注 expired_keys、evicted_keys、latest_fork_usec，这些指标短时突增可能会有阻塞风险 
18、扫描线上实例时，记得设置休眠时间，避免过高 OPS 产生性能抖动
```
